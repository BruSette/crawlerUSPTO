<html><head>
<base target="_top"/>
<title>United States Patent: 9390727</title></head>
<!---BUF1=9390727
BUF7=2016
BUF8=63603
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=88&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=1&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=88&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=2&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=88&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=3&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=87&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=2&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=89&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=2&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D88%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D2%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209390727"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9390727&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D88%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D2%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209390727">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09390727&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D88%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D2%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>88</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,390,727</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
             Bilobrov, I
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     July 12, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">Detecting distorted audio signals based on audio fingerprinting
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> An audio identification system generates a probe audio fingerprint of an
     audio signal and determines amount of pitch shifting in the audio signal
     based on analysis of correlation between the probe audio fingerprint and
     a reference audio fingerprint. The audio identification system applies a
     time-to-frequency domain transform to frames of the audio signal and
     filters the transformed frames. The audio identification system applies a
     two-dimensional discrete cosine transform (DCT) to the filtered frames
     and generates the probe audio fingerprint from a selected number of DCT
     coefficients. The audio identification system calculates a DCT sign-only
     correlation between the probe audio fingerprint and the reference audio
     fingerprint, and the DCT sign-only correlation closely approximates the
     similarity between the audio characteristics of the probe audio
     fingerprint and those of the reference audio fingerprint. Based on the
     correlation analysis, the audio identification system determines the
     amount of pitch shifting in the audio signal.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Bilobrov, I; Sergiy</b> (Santa Clara, CA)<b>, Khadkevich; Maksim</b> (London, <b>GB</b>) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/><b><i>Facebook,</i></b> Inc.</b> </td><td> <br/>Menlo Park </td><td align="center"> <br/>CA </td><td align="center"> <br/>US </td> <td align="left"> </td>
</tr> </tbody></table>
<!-- AANM>
~AANM <B><I>Facebook,</I></B> Inc.
~AACI Menlo Park
~AAST CA
~AACO US
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b><a href="#h2" name="h3"></a><a href="#h4"></a><b><i>Facebook,</i></b> Inc.</b>
 (Menlo Park, 
CA)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>53521897
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>14/153,404</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>January 13, 2014</b></td></tr>
     </tbody></table>
<hr/> <center><b>Prior Publication Data</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Document Identifier</u></b></th><th align="center" scope="col"><b><u>Publication Date</u></b></th></tr><tr><td align="center"> </td><td align="center"> US 20150199974 A1</td><td align="center">Jul 16, 2015</td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table>
     <hr/>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">G10L 25/51 (20130101); G10L 25/27 (20130101); G10L 19/018 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">G06F 17/00 (20060101); G10L 25/51 (20130101); G10L 25/27 (20130101); G10L 19/018 (20130101)</td></tr>
       <tr><td align="left" valign="top" width="30%"><b>Field of Search: </b></td>
       <td align="right" valign="top" width="70%">
       

 ;700/94 ;704/E19.009
       </td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9390727">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20120209612&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2012/0209612</a></td><td align="left">
August 2012</td><td align="left">
Bilobrov</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
<table width="90%">   <tbody><tr><td><align="left"><br/>Haitsma, J. et al., "Robust Audio Hashing for Content Identification," Content-Based Multimedia Indexing (CBMI), 2001, nine pages. cited by applicant
.<br/>Ito, I. et al., "DCT Sign-Only Correlation With Application To Image Matching And The Relationship With Phase-Only Correlation," Acoustics, Speech and Signal Processing, ICASSP 2007, 2007, pp. I-1237 to I-1240. cited by applicant
.<br/>Wang, A., "An Industrial-Strength Audio Search Algorithm," ISMIR 2003, Proceedings of the Fourth International Conference on Music Information Retrieval, Oct. 26-30, 2013, pp. 7-13, Baltimore, Maryland, USA. cited by applicant. </align="left"></td></tr>
</tbody></table><br/><center><b>Other References</b></center> <br/>
       <i>Primary Examiner:</i> Saunders, Jr.; Joseph
<br/>
       <i>Attorney, Agent or Firm:</i> <coma>Fenwick &amp; West LLP
<br/>
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>What is claimed is: <br/><br/> 1.  A computer-implemented method comprising: receiving an audio signal including a plurality of frames, each frame representing a portion of the audio signal;  generating a
probe audio fingerprint based on one or more of the plurality frames;  selecting a reference audio fingerprint from a plurality of reference audio fingerprints;  determining whether the probe audio fingerprint matches the reference audio fingerprint
based on a correlation between the probe audio fingerprint and the reference audio fingerprint;  obtaining position information of at least one absolute peak value of the correlation between the probe audio fingerprint and the reference fingerprint;  and
determining amount of distortion in the audio signal based on the position of the absolute peak value of the correlation, the amount of distortion indicating how much pitch of the audio signal has shifted from original pitch associated with the audio
signal.
<br/><br/> 2.  The computer-implemented method of claim 1, wherein generating the probe audio fingerprint of the audio signal comprises: applying a time domain to frequency domain transform to one or more of the plurality of frames of the audio signal; 
filtering the transformed one or more of the plurality of frames of the audio signal;  applying a two-dimensional discrete cosine transform (DCT) to the filtered frames of the audio signal;  and generating the probe audio fingerprint from a predetermined
number of DCT coefficients of the audio signal.
<br/><br/> 3.  The computer-implemented method of claim 2, wherein the time domain to frequency domain transform is a Short-Time Fourier Transform (STFT).
<br/><br/> 4.  The computer-implemented method of claim 2, wherein applying a time domain to frequency domain transform comprises: applying a weighting function to one or more of the plurality of frames of the audio signal using a window function;  and
applying a Short-Time Fourier Transform (STFT) to the weighted frames of the audio signals.
<br/><br/> 5.  The computer-implemented method of claim 2, wherein filtering the transformed one or more of the plurality of frames of the audio signal comprises: applying a 16-band filter to the transformed one or more of the plurality of frames.
<br/><br/> 6.  The computer-implemented method of claim 5, wherein the 16-band filter is a 16-band third octave triangular filter, wherein applying the 16-band third octave triangular filter to a transformed frame of the plurality of frames splits the
transformed frame into 16 filter banks.
<br/><br/> 7.  The computer-implemented method of claim 2, wherein applying a two-dimensional DCT transform to the filtered frames of the audio signal comprises: generating a matrix of DCT coefficients, each DCT coefficient having a representation of sign
information;  and selecting a predetermined number of DCT coefficients from the matrix of DCT coefficients.
<br/><br/> 8.  The computer-implemented method of claim 2, wherein generating the probe audio fingerprint from a predetermined number of DCT coefficients of the audio signal comprises: selecting sign information of the predetermined number of DCT
coefficients;  generating the probe audio fingerprint of the audio signal from the sign information of the predetermined number of DCT coefficients;  and representing the probe audio fingerprint as an integer having a predetermined number of bits.
<br/><br/> 9.  The computer-implemented method of claim 1, wherein determining whether the probe audio fingerprint matches the reference audio fingerprint comprises: calculating the correlation between the probe audio fingerprint and the reference audio
fingerprint, the correlation approximating similarity between audio characteristics of the probe audio fingerprint and audio characteristics of the reference audio fingerprint;  and matching the probe audio fingerprint with the reference fingerprint
based on the calculated correlation between the probe audio fingerprint and the reference fingerprint.
<br/><br/> 10.  The computer-implemented method of claim 9, wherein calculating the correlation between the probe audio fingerprint and the reference audio fingerprint comprises: applying a two-dimensional discrete cosine transform to columns of DCT
coefficients representing the probe audio fingerprint;  applying the two-dimensional discrete cosine transform to columns of DCT coefficients representing the reference audio fingerprint;  and calculating DCT sign-only correlation from the transformed
columns of DCT coefficients representing the probe audio fingerprint and the transformed columns of DCT coefficients representing the reference audio fingerprint, the DCT sign-only correlation having at least one absolute peak value and information of
the position of the at least one absolute peak value.
<br/><br/> 11.  The computer-implemented method of claim 9, wherein matching the probe audio fingerprint with the reference fingerprint comprises: obtaining at least one absolute peak value of the calculated correlation between the probe audio fingerprint
and the reference fingerprint;  and responsive to the absolute peak value exceeding a threshold value, determining that the probe audio fingerprint matches the reference fingerprint.
<br/><br/> 12.  The computer-implemented method of claim 1, further comprising: retrieving identifying information associated with the reference audio fingerprint responsive to the probe audio fingerprint matching the reference fingerprint;  and
associating the identifying information with the audio signal.
<br/><br/> 13.  A computer system comprising: an audio fingerprint generation module for: receiving an audio signal including a plurality of frames, each frame representing a portion of the audio signal;  generating a probe audio fingerprint based on one
or more of the plurality frames;  selecting a reference audio fingerprint from a plurality of reference audio fingerprints;  an audio fingerprint matching module for: determining whether the probe audio fingerprint matches the reference audio fingerprint
based on correlation between the probe audio fingerprint and the reference audio fingerprint;  obtaining position information of at least one absolute peak value of the correlation between the probe audio fingerprint and the reference fingerprint;  and
determining amount of distortion in the audio signal based on the position of the absolute peak value of the correlation, the amount of distortion indicating how much pitch of the audio signal has shifted from original pitch associated with the audio
signal;  and a computer processor configured to execute the audio fingerprint generation module and the audio fingerprint matching module.
<br/><br/> 14.  The system of claim 13, wherein the audio fingerprint generation module is further for: applying a time domain to frequency domain transform to one or more of the plurality of frames of the audio signal;  filtering the transformed one or
more of the plurality of frames of the audio signal;  applying a two-dimensional discrete cosine transform (DCT) to the filtered frames of the audio signal;  and generating the probe audio fingerprint from a predetermined number of DCT coefficients of
the audio signal.
<br/><br/> 15.  The system of claim 14, wherein applying a time domain to frequency domain transform comprises: applying a weighting function to one or more of the plurality of frames of the audio signal using a window function;  and applying a Short-Time
Fourier Transform (STFT) to the weighted frames of the audio signals.
<br/><br/> 16.  The system of claim 14, wherein filtering the transformed one or more of the plurality of frames of the audio signal comprises: applying a 16-band filter to the transformed one or more of the plurality of frames.
<br/><br/> 17.  The system of claim 16, wherein the 16-band filter is a 16-band third octave triangular filter, wherein applying the 16-band third octave triangular filter to a transformed frame of the plurality of frames splits the transformed frame into
16 filter banks.
<br/><br/> 18.  The system of claim 14, wherein applying a two-dimensional DCT transform to the filtered frames of the audio signal comprises: generating a matrix of DCT coefficients, each DCT coefficient having a representation of sign information;  and
selecting a predetermined number of DCT coefficients from the matrix of DCT coefficients.
<br/><br/> 19.  The system of claim 14, wherein generating the probe audio fingerprint from a predetermined number of DCT coefficients of the audio signal comprises: selecting sign information of the predetermined number of DCT coefficients;  generating
the prober audio fingerprint of the audio signal from the sign information of the predetermined number of DCT coefficients;  and representing the prober audio fingerprint as an integer having the predetermined number of bits.
<br/><br/> 20.  The system of claim 13, wherein the audio fingerprint matching module is further for: calculating the correlation between the probe audio fingerprint and the reference audio fingerprint, the correlation approximating similarity between
audio characteristics of the probe audio fingerprint and audio characteristics of the reference audio fingerprint;  and matching the probe audio fingerprint with the reference fingerprint based on the calculated correlation between the probe audio
fingerprint and the reference fingerprint.
<br/><br/> 21.  The system of claim 20 wherein calculating the correlation between the probe audio fingerprint and the reference audio fingerprint comprises: applying a two-dimensional discrete cosine transform to columns of DCT coefficients representing
the probe audio fingerprint;  applying the two-dimensional discrete cosine transform to columns of DCT coefficients representing the reference audio fingerprint;  and calculating DCT sign-only correlation from the transformed columns of DCT coefficients
representing the probe audio fingerprint and the transformed columns of DCT coefficients representing the reference audio fingerprint, the DCT sign-only correlation having at least one absolute peak value and information of the position of the at least
one absolute peak value.
<br/><br/> 22.  The system of claim 20, wherein matching the probe audio fingerprint with the reference fingerprint comprises: obtaining at least one absolute peak value of the calculated correlation between the probe audio fingerprint and the reference
fingerprint;  and responsive to the absolute peak value exceeding a threshold value, determining that the probe audio fingerprint matches the reference fingerprint.
<br/><br/> 23.  The system of claim 13, wherein the audio fingerprint matching module is further for: retrieving identifying information associated with the reference audio fingerprint responsive to the probe audio fingerprint matching the reference
fingerprint;  and associating the identifying information with the audio signal. <hr/> <center><b><i>Description</i></b></center> <hr/> <br/><br/>BACKGROUND
<br/><br/> This disclosure generally relates to audio identification, and more specifically to detecting distorted audio signals based on audio fingerprinting.
<br/><br/> An audio fingerprint is a compact summary of an audio signal that can be used to perform content-based identification.  For example, existing audio signal identification systems use various audio signal identification schemes to identify the
name, artist, and/or album of an unknown song.  When presented with an unidentified audio signal, an audio signal identification system is configured to generate an audio fingerprint for the audio signal, where the audio fingerprint includes
characteristic information about the audio signal usable for identifying the audio signal.  The characteristic information about the audio signal may be based on acoustical and perceptual properties of the audio signal.  Using fingerprints and matching
algorithms, the audio fingerprint generated from the audio signal is compared to a database of reference audio fingerprints for identification of the audio signal.
<br/><br/> Audio fingerprinting techniques should be robust to a variety of distortions due to noisy transmission channels or specific sound processing.  Pitch shifting and tempo shifting are two of the most common and problematic types of distortions to
most existing audio identification systems based on analysis of spectral content.  Pitch shifting refers to raising or lowering the original pitch of an audio signal.  When pitch shifting occurs, all the frequencies of the audio signal in the spectrum
are multiplied by a factor.  Tempo shifting or variation refers to a playing an audio signal slower or faster than its original speed.  Since spectral content of an audio signal is either stretched along the time axis (tempo variations or shifting) or
shifted along the frequency axis (pitching shifting), existing audio identification solutions based on the analysis of spectral content are often not robust enough to accurately identify distorted versions of an audio signal.
<br/><br/> Various existing solutions are provided by audio identification systems to detect distorted versions of audio signals, such as solutions involving computing Hamming distance between two sub-fingerprints of audio signals.  Using a lower Hamming
distance as a threshold, a higher matching rate between the sub-fingerprints will be found.  However, a pitch shift can lead to significant changes in spectral content of an audio signal, resulting in a high Hamming distance and consequently a low
matching rate.  One of the possible solutions is to extract several indexes, each corresponding to a given pitch shift, and to then match a sub-fingerprint being evaluated to all the indexes.  However, this approach introduces additional computational
load to the matching process and additional space to store multiple fingerprint versions.
<br/><br/>SUMMARY
<br/><br/> To identify audio signals, an audio identification system generates probe audio fingerprints for the audio signals.  The audio identification system generates a probe audio fingerprint of an audio signal by applying a time-to-frequency domain
transform, e.g., a Short-Time Fourier Transform (STFT) to one or more frames of the audio signal.  The transformed frames are filtered by a band-pass filter, such as a 16-band third-octave filter bank, Mel-frequency filter bank, or any similar filter
banks, by the audio identification system.  The band-pass filtering generates multiple sub-samples corresponding to different frequency bands of the audio signal.
<br/><br/> The audio identification system applies a two-dimensional discrete cosine transform (DCT) to the filtered frames to generate a matrix of DCT coefficients, each of which has sign information.  The audio identification system selects a number of
DCT coefficients, e.g., 64 DCT coefficients from the first 4 even columns of the matrix of DCT coefficients.  To compactly represent the probe audio fingerprint, e.g., representing the probe audio fingerprint as a 64-bit integer, the audio identification
system only keeps the sign information of the selected DCT coefficients to represent the probe audio fingerprint.
<br/><br/> To detect distortion (e.g., pitch shifting) in the audio signal, the audio identification system calculates a DCT sign-only correlation between the probe audio fingerprint and a reference audio fingerprint.  The audio identification system
applies a DCT transform on the columns of DCT sign coefficients of the probe audio fingerprint and corresponding DCT sign coefficients of the reference audio signal to generate the DCT sign-only correlation.  The DCT sign-only correlation closely
approximates the similarity between the audio characteristics of the probe audio fingerprint and those of the reference audio fingerprint.
<br/><br/> The audio identification system analyzes the DCT sign-only correlation between the probe audio fingerprint and the reference audio fingerprint to determine whether the probe audio fingerprint matches the reference audio fingerprint.  For
example, responsive to the absolute peak value of the DCT sign-only correlation function exceeding a threshold value, the audio identification system determines that the probe audio fingerprint matches the reference audio fingerprint.  From the position
of the absolute peak value in the DCT sign-only correlation function, the audio identification system determines the amount of pitch shifting in the audio signal.  Thus, DCT sign-only correlation based audio fingerprint matching can be used to detect
pitch shifted versions of audio signals where distance based, e.g., Hamming distance, matching algorithms fail to the detect such pitch shifted versions of audio signals.
<br/><br/> The features and advantages described in this summary and the following detailed description are not all-inclusive.  Many additional features and advantages will be apparent to one of ordinary skill in the art in view of the drawings,
specification, and claims hereof. <br/><br/>BRIEF DESCRIPTION OF THE DRAWINGS
<br/><br/> FIG. 1 is a block diagram of a process for identifying audio signals in accordance with an embodiment.
<br/><br/> FIG. 2 is a block diagram of an audio identification system in accordance with an embodiment.
<br/><br/> FIG. 3 is a block diagram of an audio fingerprint generation module in accordance with an embodiment.
<br/><br/> FIG. 4 is a flowchart of generating an audio signal fingerprint in accordance with an embodiment.
<br/><br/> FIG. 5 is a block diagram of an audio fingerprint matching module in accordance with an embodiment.
<br/><br/> FIG. 6 is a flowchart of detecting distortion in an audio signal based on the audio fingerprint of the audio signal in accordance with an embodiment.
<br/><br/> FIG. 7 is an example filter bank configuration for audio signal fingerprint generation in accordance with an embodiment.
<br/><br/> FIG. 8A is an example similarity matrix of an audio signal without distortion of pitch shifting.
<br/><br/> FIG. 8B is an illustration of discrete cosine transform (DCT) sign-only correlation corresponding to the similarity matrix illustrated in FIG. 8A.
<br/><br/> FIG. 9A is an example similarity matrix of an audio signal with 20% distortion of pitch shifting.
<br/><br/> FIG. 9B is an illustration of DCT sign-only correlation corresponding to the similarity matrix illustrated in FIG. 9A.
<br/><br/> The figures depict various embodiments for purposes of illustration only.  One skilled in the art will readily recognize from the following discussion that alternative embodiments of the structures and methods illustrated herein may be employed
without departing from the principles described herein.
<br/><br/>DETAILED DESCRIPTION
<br/><br/> Overview
<br/><br/> Embodiments of the invention enable the robust identification of audio signals based on audio fingerprints.  FIG. 1 shows an example embodiment of an audio identification system 100 identifying an audio signal 102.  As shown in FIG. 1, the audio
identification system 100 has an audio fingerprint generation module 110, an audio fingerprint matching module 120 and a fingerprints database 130.  The audio identification system 100 receives an audio signal 102 generated by an audio source 101,
generates an audio fingerprint of the audio signal 102 by the audio fingerprint generation module 110, matches the generated audio fingerprint with one or more reference audio fingerprints stored in the fingerprints database 130 and outputs an verified
audio signal 106.
<br/><br/> As shown in FIG. 1, an audio source 101 generates the audio signal 102.  The audio source 101 may be any entity suitable for generating audio (or a representation of audio), such as a person, an animal, speakers of a mobile device, a desktop
computer transmitting a data representation of a song, or other suitable entity generating audio.  The audio signal 102 comprises one or more discrete audio frames, each of which corresponds to a fragment of the audio signal 102 at a particular time. 
Hence, each audio frame of the audio signal 102 corresponds to a length of time of the audio signal 102, such as 25 ms, 50 ms, 100 ms, 200 ms, etc.
<br/><br/> Upon receiving the one or more audio frames of the audio signal 102, the audio fingerprint generation module 110 generates an audio fingerprint 113 from one or more of the audio frames of the audio signal 102.  For simplicity and clarity, the
audio fingerprint 113 of the audio signal 102 is referred to as a "probe audio fingerprint" throughout the entire description.  The probe audio fingerprint 113 of the audio signal 102 may include characteristic information describing the audio signal
102.  Such characteristic information may indicate acoustical and/or perceptual properties of the audio signal 102.  To generate the probe audio fingerprint 113 of the audio signal 102, the audio fingerprint generation module 110 preprocesses the audio
signal 102, transforms the audio signal 102 from one domain to another domain, filters the transformed audio signal and generates the audio fingerprint from the further transformed audio signal.  One embodiment of the audio fingerprint generation module
110 is further described with reference to FIG. 3 and FIG. 4.
<br/><br/> To detect a distorted version of the audio signal 102, the audio fingerprint matching module 120 matches the probe audio fingerprint 113 of the audio signal 102 against a set of reference audio fingerprints stored in the fingerprints database
130.  To match the probe audio fingerprint 113 to a reference audio fingerprint, the audio fingerprint matching module 120 calculates a correlation between the probe audio fingerprint 113 and the reference audio fingerprint.  The correlation measures the
similarity between the audio characteristics of the probe audio fingerprint 113 of the audio signal 102 and the audio characteristics of the reference audio fingerprint.  The audio fingerprint matching module 120 determines whether the audio signal 102
is distorted based on the similarity.  One embodiment of the audio fingerprint matching module 120 is further described with reference to FIG. 5 and FIG. 6.
<br/><br/> The fingerprints database 130 stores probe audio fingerprints of audio signals and/or one or more reference audio fingerprints, which are audio fingerprints generated from one or more reference audio signals.  Each reference audio fingerprint in
the fingerprints database 130 is also associated with identifying information and/or other information related to the audio signal from which the reference audio fingerprint was generated.  The identifying information may be any data suitable for
identifying an audio signal.  For example, the identifying information associated with a reference audio fingerprint includes title, artist, album, publisher information for the corresponding audio signal.  Identifying information may also include data
indicating the source of an audio signal corresponding to a reference audio fingerprint.  For example, the reference audio signal of an audio-based advertisement may be broadcast from a specific geographic location, so a reference audio fingerprint
corresponding to the reference audio signal is associated with an identifier indicating the geographic location (e.g., a location name, global positioning system (GPS) coordinates, etc.).
<br/><br/> In one embodiment, the fingerprints database 130 stores indices of the reference audio fingerprints.  Each index associated with a reference audio fingerprint may be computed from a portion of the corresponding reference audio fingerprint.  For
example, a set of bits from a reference audio fingerprint corresponding to low frequency coefficients in the reference audio fingerprint may be used as the reference audio fingerprint's index.
<br/><br/> System Architecture
<br/><br/> FIG. 2 is a block diagram illustrating one embodiment of a system environment 200 including an audio identification system 100.  As shown in FIG. 2, the system environment 200 includes one or more client devices 202, one or more external systems
203, the audio identification system 100 and a social networking system 205 connected through a network 204.  While FIG. 2 shows three client devices 202, one social networking system 205, and one external system 203, it should be appreciated that any
number of these entities (including millions) may be included.  In alternative configurations, different and/or additional entities may also be included in the system environment 200.  Furthermore, in some embodiments, the audio identification system 100
can be a system or module running on or otherwise included within one of the other entities shown in FIG. 2.
<br/><br/> A client device 202 is a computing device capable of receiving user input, as well as transmitting and/or receiving data via the network 204.  In one embodiment, a client device 202 sends a request to the audio identification system 100 to
identify an audio signal captured or otherwise obtained by the client device 202.  The client device 202 may additionally provide the audio signal or a digital representation of the audio signal to the audio identification system 100.  Examples of client
devices 202 include desktop computers, laptop computers, tablet computers (pads), mobile phones, personal digital assistants (PDAs), gaming devices, or any other device including computing functionality and data communication capabilities.  Hence, the
client devices 202 enable users to access the audio identification system 100, the social networking system 205, and/or one or more external systems 203.  In one embodiment, the client devices 202 also allow various users to communicate with one another
via the social networking system 205.
<br/><br/> The network 204 may be any wired or wireless local area network (LAN) and/or wide area network (WAN), such as an intranet, an extranet, or the Internet.  The network 204 provides communication capabilities between one or more client devices 202,
the audio identification system 100, the social networking system 205, and/or one or more external systems 203.  In various embodiments the network 204 uses standard communication technologies and/or protocols.  Examples of technologies used by the
network 204 include Ethernet, 802.11, 3G, 4G, 802.16, or any other suitable communication technology.  The network 204 may use wireless, wired, or a combination of wireless and wired communication technologies.  Examples of protocols used by the network
204 include transmission control protocol/Internet protocol (TCP/IP), hypertext transport protocol (HTTP), simple mail transfer protocol (SMTP), file transfer protocol (TCP), or any other suitable communication protocol.
<br/><br/> The external system 203 is coupled to the network 204 to communicate with the audio identification system 100, the social networking system 205, and/or with one or more client devices 202.  The external system 203 provides content and/or other
information to one or more client devices 202, the social networking system 205, and/or to the audio identification system 100.  Examples of content and/or other information provided by the external system 203 include identifying information associated
with reference audio fingerprints, content (e.g., audio, video, etc.) associated with identifying information, or other suitable information.
<br/><br/> The social networking system 205 is coupled to the network 204 to communicate with the audio identification system 100, the external system 203, and/or with one or more client devices 202.  The social networking system 205 is a computing system
allowing its users to communicate, or to otherwise interact, with each other and to access content.  The social networking system 205 additionally permits users to establish connections (e.g., friendship type relationships, follower type relationships,
etc.) between one another.  Though the social networking system 205 is included in the embodiment of FIG. 2, the audio identification system 100 can operate in environments that do not include a social networking system, including within any environment
for which detection of distortion of audio signals is desirable.
<br/><br/> In one embodiment, the social networking system 205 stores user accounts describing its users.  User profiles are associated with the user accounts and include information describing the users, such as demographic data (e.g., gender
information), biographic data (e.g., interest information), etc. Using information in the user profiles, connections between users, and any other suitable information, the social networking system 205 maintains a social graph of nodes interconnected by
edges.  Each node in the social graph represents an object associated with the social networking system 205 that may act on and/or be acted upon by another object associated with the social networking system 205.  An edge between two nodes in the social
graph represents a particular kind of connection between the two nodes.  For example, an edge may indicate that a particular user of the social networking system 205 is currently "listening" to a certain song.  In one embodiment, the social networking
system 205 may use edges to generate stories describing actions performed by users, which are communicated to one or more additional users connected to the users through the social networking system 205.  For example, the social networking system 205 may
present a story about a user listening to a song to additional users connected to the user.
<br/><br/> Discrete Cosine Transform (DCT) Based Audio Fingerprint Generation
<br/><br/> To detect audio signals with pitch shifting, the audio identification system 100 generates audio fingerprints of the audio signals based on DCT transform and filtering of the audio signals.  FIG. 3 is a block diagram of an audio fingerprint
generation module 110 in accordance with an embodiment of the invention.  The audio fingerprint generation module 110 is configured to preprocess an audio signal, transform the audio signal from time domain to frequency domain, filter the transformed
audio signal and generate the audio fingerprint from the further transformed audio signal.  In the embodiment illustrated in FIG. 3, the audio fingerprint generation module 110 has a preprocessing module 112, a transform module 114, a filtering module
116 and a fingerprint generation module 118.  Other embodiments of the audio fingerprint module 110 may have additional and/or different modules.  In addition, the functions may be distributed among the modules in a different manner than described
herein.
<br/><br/> The preprocessing module 112 receives an audio signal and preprocesses the received audio signal for audio fingerprint generation.  In one embodiment, the preprocessing module 112 converts the audio signal into multiple audio features and
selects a subset of the audio features to be used in generating an audio fingerprint for the audio signal.  Other examples of audio signal preprocessing include analog-to-digital conversion if the audio signal is in analog representation, extracting
metadata associated with the audio signal, coding/decoding the audio signal for mobile applications, normalizing the amplitude (e.g., bounding the dynamic range of the audio signal to a predetermined range) and dividing the audio signal into multiple
audio frames corresponding to the variation velocity of the underlying acoustic events of the audio signal.  The preprocessing module 112 may perform other audio signal preprocessing operations known to those of ordinary skills in the art.
<br/><br/> The transform module 114 transforms the audio signal from one domain to another domain for efficient signal compression and noise removal in audio fingerprint generation.  In one embodiment, the transform module 114 transforms the audio signal
from time domain to frequency domain by applying a Short-Time Fourier Transform (STFT).  Other embodiments of the transform module 114 may use other types of time-to-frequency transforms.  Based on the time-to-frequency domain transform of the audio
signal, the transform module 114 obtains power spectrum information for each frame of the audio signal over a range of frequencies, such as 250 to 2250 Hz.
<br/><br/> Let x[n] be a discrete audio signal in the time domain sampled at a sampling frequency F.sub.s.  x[n] is divided into frames with frame step p samples.  For a frame, corresponding to sample t, STFT transform is performed on the audio signal
weighted by a window function w[n] as follows in Equation (1): X[t,k]=.SIGMA..sub.n=0.sup.M-1w[n]x[n+t]e.sup.-2.pi.jnk/M (1) where parameter k and parameter M denote a bin number and the window size, respectively.
<br/><br/> The filtering module 116 receives the transformed audio signal and filters the transformed audio signal.  In one embodiment, the filtering module 116 applies a B-band third octave triangular filter bank to each spectral frame of the transformed
audio signal.  Other embodiments of the filtering module 116 may use other types of filter banks.  In a third-octave filter bank, spacing between centers of adjacent bands is equal to one-third octave.  In one embodiment, the center frequency f.sub.c[k]
of k-th filter is defined as in Equation (2) f.sub.c[k]=2.sup.k/3F.sub.0 (2) where parameter F.sub.0 is set to 500 Hz and the number of filter banks, B, is set to 16.  The upper and lower band edges in the k-th band are equal to the central frequencies
of the next and the previous bands, respectively.  By applying the band-pass filters, multiple sub-band samples corresponding to different frequency bands of the audio signal are generated.  FIG. 7 is an example filter bank configuration for audio signal
fingerprint generation in accordance with an embodiment of the invention.
<br/><br/> Let fb[i] be the output of filter bank after processing i-th frame.  fb[i] consists of B bins, each bin containing spectral power of the corresponding spectral bandwidth.  A sequence of N.sub.fb consecutive frames containing spectral power
starting from fb[i] is used to generate a sub-fingerprint F.sub.sub[i].  In one embodiment, the number of consecutive frames N.sub.fb is set to 32.  Upon filtering the transformed audio signal, the filtering module 116 obtains a B.times.N.sub.fb matrix
and normalizes the B.times.N.sub.fb matrix by row to remove possible equalization effect in the audio signal.
<br/><br/> The fingerprint generation module 118 is for generating an audio fingerprint for an audio signal by further transforming the audio signal.  In one embodiment, the fingerprint generation module 118 receives the normalized matrix B.times.N.sub.fb
from the filtering module 116 and applies a two-dimensional (2D) Discrete Cosine Transform (DCT) to the matrix B.times.N.sub.fb to get a matrix D of DCT coefficients.
<br/><br/> From DCT coefficients in the matrix D, the fingerprint generation module 118 selects a subset of 64 coefficients to represent an audio fingerprint of the audio signal being processed.  In one embodiment, the fingerprint generation module 118
selects first 4 even columns of the DCT coefficients from the DCT coefficients matrix D, which results in a 4.times.16 matrix F sub to represent the audio fingerprint.  To represent the audio fingerprint F sub as a 64-bit integer, the fingerprint module
118 keeps only sign information of the selected DCT coefficients.  The sign information of DCT coefficients is robust against quantization noise (e.g., scalar quantization errors) because positive signs of DCT coefficients do not change to negative signs
and vice versa.  In addition, the concise expression of DCT signs saves memory space to calculate and store them.
<br/><br/> Turning now to FIG. 4, a flowchart is shown illustrating a process for generating an audio signal fingerprint in accordance with an embodiment of the invention.  Initially, the audio fingerprint generation module 110 receives 410 an audio signal
for audio fingerprint generation.  The audio fingerprint generation module 110 preprocesses 420 the received audio signal by applying one or more operations to the audio signal, such as extracting metadata associated with the audio signal, normalizing
the amplitude and dividing the audio signal into multiple audio frames.
<br/><br/> To compactly represent the information contained in the audio signal, the audio fingerprint generation module 110 transforms the audio signal by applying 430 a time-to-frequency domain transform (e.g., STFT transform) to the audio signal.  The
audio fingerprint generation module 110 filters 440 the transformed audio signal by splitting each spectral frame of the transformed audio signal into multiple filter banks.  Example filtering is to apply a 16-band third octave triangular filter bank to
each spectral frame of the transformed audio signal and to obtain a matrix of 16.times.32 bins of spectral power of the corresponding spectral bandwidth.
<br/><br/> The audio fingerprint generation module 110 applies 450 a 2D DCT transform to the filtered audio signal to obtain a matrix of 64 selected DCT coefficients.  To balance efficient representation and computation complexity, the audio fingerprint
generation module 110 only keeps the sign information of the selected DCT coefficients.  The audio fingerprint generation module 110 generates 460 an audio fingerprint of the audio signal from the sign information of the selected DCT coefficients and
represents the audio fingerprint as a 64-bit integer.  In addition, the audio fingerprint generation module 110 stores 470 the generated audio fingerprint in a fingerprints database, e.g., the fingerprints database 130 as illustrated in FIG. 1.
<br/><br/> After generating the probe audio fingerprint for the audio signal, the audio fingerprint generation module 110, in conjunction with the audio fingerprint matching module 120, performs one or more rounds of processing to detect pitch shifting in
the audio signal.  For example, the audio fingerprint generation module 110 generates DCT-based audio fingerprints for one or more reference audio signals by applying the similar steps as described above.  The audio fingerprint matching module 120
selects a set of reference audio fingerprints to be compared with the probe audio fingerprint for detecting pitch shifting in the audio signal.
<br/><br/> Audio Fingerprint Matching Based on DCT Sign-Only Correlation
<br/><br/> FIG. 5 is a block diagram of an audio fingerprint matching module 120 in accordance with an embodiment of the invention.  In the embodiment illustrated in FIG. 5, the audio fingerprint matching module 120 has a correlation module 122 and a
matching module 124.  Upon receiving a probe audio fingerprint of an audio signal generated by the audio fingerprint generation module 110, the audio fingerprint matching module 120 calculates a correlation between the probe audio fingerprint of the
audio signal and a reference audio fingerprint stored in the fingerprints database 130.  Responsive to multiple reference audio fingerprints, the audio fingerprint matching module 120 calculates the correlation between the probe audio fingerprint and
each reference audio fingerprint.  The audio fingerprint matching module 120 determines whether the audio signal is distorted (e.g., pitch shifted) based on the correlation analysis.  In one embodiment, the correlation module 122 calculates a correlation
between the probe audio fingerprint of the audio signal and a short list of reference audio fingerprints stored in the fingerprints database 130.  The short list of reference audio fingerprints can be generated based on one or more features of the
reference audio fingerprints, e.g., tempo, timbral shape and others.
<br/><br/> The correlation module 122 is configured to calculate correlation between the probe audio fingerprint of the audio signal and a reference audio fingerprint.  The correlation measures the similarity between the audio characteristics of the probe
audio fingerprint and the audio characteristics of the reference audio fingerprint.  In one embodiment, the correlation module 122 calculates the correlation between the probe audio fingerprint of the audio signal and the reference audio fingerprint by
applying a DCT transform on the columns of DCT sign coefficients of the probe audio fingerprint and the reference audio fingerprint.  For simplicity and clarity, this correlation is referred to as "DCT sign-only correlation."
<br/><br/> Let F.sub.sub(i) be the i-th column of DCT coefficients of the probe audio fingerprint and G.sub.sub(i) be the i-th column of DCT coefficients of the reference audio fingerprint.  F.sub.sub(i) and G.sub.sub(i) are generated by the audio
fingerprint generation module 110 described above.  Let DCT sign product P.sub.i be defined as follows in Equation (3): P.sub.i=F.sub.sub(i)G.sub.sub(i) (3) The correlation module 122 applies a DCT transform on the columns of DCT sign coefficients of
F.sub.sub(i) and G.sub.sub(i) to calculate the correlation.  In other words, the DCT sign-only correlation C.sub.i(k) of the DCT sign product P.sub.i is defined as follows in Equation (4):
<br/><br/> .function..times..times..times..function..times..function..pi..times..tim- es..times..times..times..times..times..times..times.  ##EQU00001## where N is the length of P.sub.i.  P.sub.i can be zero-padded to increase resolution.  After obtaining
P.sub.i values for all the columns of DCT sign coefficients, the correlation module 122 calculates the DCT sign-only correlation C as follows in Equation (5):
<br/><br/> .function..times..times..function.  ##EQU00002##
<br/><br/> The matching module 124 matches the probe audio fingerprint against a set of reference audio fingerprints.  To match the probe audio fingerprint to a reference audio fingerprint, the matching module 124 measures the similarity between the audio
characteristics of the probe audio fingerprint and the audio characteristics of the reference audio fingerprint based on the DCT sign-only correction between the probe audio fingerprint and the reference audio fingerprint.  It is noted that there is a
close relationship between the DCT sign-only correlation and the similarity based on phase-only correlation for image search.  In other words, the similarity based on phase-only correlation is a special case of the DCT sign-only correlation.  Applying
this close relationship to the audio signal distortion detection, the DCT sign-only correlation between the probe audio fingerprint and the reference audio fingerprint closely approximates the similarity between the audio characteristics of the probe
audio fingerprint and the audio characteristics of the reference audio fingerprint.
<br/><br/> In one embodiment, the degree of the similarity or the degree of match between the audio characteristics of the probe audio fingerprint and the audio characteristics of the reference audio fingerprint is indicated by the absolute peak value of
the DCT sign-only correlation function between the probe audio fingerprint and the reference audio fingerprint.  For example, a high absolute peak value of the DCT sign-only correlation function between the probe audio fingerprint and the reference audio
fingerprint indicates that the probe audio fingerprint matches the reference audio fingerprint.  In other words, a pitch shifted audio signal can be identified as the same audio content as a reference audio signal in response to the DCT sign-only
correlation function between the corresponding audio fingerprints of the audio signal and the reference audio signal having an absolute peak value higher than a predetermined threshold value.
<br/><br/> In addition to measure the degree of match between the audio characteristics of the probe audio fingerprint and the audio characteristics of the reference audio fingerprint, the matching module 124 determines the degree of pitch shift of the
audio signal with respect to the reference audio signal based on the position of the absolute peak value of the DCT sign-only correlation function defined in Equation (5) above.  In one embodiment, a frequency multiplication factor R can be derived from
the position fR of the peak in C(k) as
<br/><br/> ##EQU00003## in case of third-octave filter bank.  In this case, frequency f in the probe fingerprint corresponds to frequency fR in the reference fingerprint.
<br/><br/> FIG. 6 is a flowchart of detecting pitch shifting in an audio signal based on the audio fingerprint of the audio signal in accordance with an embodiment of the invention.  Initially, the audio fingerprint matching module 120 receives 610 a probe
audio fingerprint of an audio signal, where the probe audio fingerprint is generated by the audio fingerprint generation module 110 described above.  The audio fingerprint matching module 120 retrieves 620 a reference audio fingerprint for comparison and
calculates 630 a DCT sign-only correlation between the probe audio fingerprint and the reference audio fingerprint according to the Equations (3)-(5) above.
<br/><br/> The audio fingerprint matching module 120 determines 640 whether the absolute peak value of the DCT sign-only correlation function is higher than a predetermined threshold value.  Responsive to the absolute peak value of the DCT sign-only
correlation function being higher than the predetermined threshold value, the audio fingerprint matching module 120 detects 650 a match between the probe audio fingerprint of the audio signal and the reference audio fingerprint.  On the other hand,
responsive to the absolute peak value of the DCT sign-only correlation function being lower than the predetermined threshold value, the audio fingerprint matching module 120 retrieves another reference audio fingerprint and determines whether there is a
match between the probe audio fingerprint and the newly retrieved reference audio fingerprint by repeating the steps 630-650.
<br/><br/> As described above with reference to FIG. 5, a pitch shifted audio signal can be identified as the same audio content as a reference audio signal responsive to the audio fingerprint of the pitch shifted audio signal matching the audio
fingerprint of the reference audio signal based on the DCT sign-only correlation analysis.  In step 660, the audio fingerprint matching module 120 determines the degree of pitch shifting in the audio signal with respect to the reference audio signal
based on the position of the absolute peak value of the DCT sign-only correlation function.
<br/><br/> The audio fingerprint matching module 120 retrieves 670 identifying information associated with the reference audio fingerprint matching the probe audio fingerprint of the audio signal.  The audio fingerprint matching module 120 may retrieve the
identifying information from the audio fingerprints database 130, one or more external systems 203, and/or any other suitable entity.  The audio fingerprint matching module 120 outputs 680 the matching results.  For example, the audio fingerprint
matching module 120 sends the identifying information to a client device 202 that initially requested identification of the audio signal 102.  The identifying information allows a user of the client device 202 to determine information related to the
audio signal 102.  For example, the identifying information indicates that the audio signal 102 is produced by a particular device or indicates that the audio signal 102 is a song with a particular title, artist, or other information.
<br/><br/> In one embodiment, the audio fingerprint matching module 120 provides the identifying information to the social networking system 205 via the network 204.  The social networking system 205 may update a newsfeed or user's user profile, or may
allow a user to do so, to indicate the user requesting the audio identification is currently listening to a song identified by the identifying information.  In one embodiment, the social networking system 205 may communicate the identifying information
to one or more additional users connected to the user requesting identification of the audio signal 102 over the social networking system 205.
<br/><br/> Compared with conventional distance based similarity measurement for matching an audio signal to a reference audio signal, the DCT sign-only correlation between the audio fingerprint of the audio signal and a reference audio fingerprint can be
used to improve the matching performance especially with robust matching rate for the audio signal with pitch shifting.
<br/><br/> FIG. 8A is an example similarity matrix of an audio signal without pitch shifting.  In the example shown in FIG. 8A, the audio signal is a short musical excerpt and a pitch shifted version of the audio signal is produced for the illustration. 
FIG. 8A illustrates a similarity matrix representing a self-comparison, where the audio signal is compared with itself.  Because there is no distortion from pitch shifting in the audio signal, a high matching rate based on Hamming distance is observed. 
In one embodiment, a similarity matrix U consists of i rows and m columns where l is the number of frames in the probe fingerprint, while m is the number of frames in the reference fingerprint.  Value U.sub.i,j is computed as the Hamming distance between
frame i of the probe fingerprint and frame j of the reference fingerprint.
<br/><br/> FIG. 8B is an illustration of DCT sign-only correlation corresponding to the similarity matrix illustrated in FIG. 8A.  The DCT sign-only correlation function between the audio fingerprints of same audio signal is calculated for matrix point
[50, 50].  It is shown in FIG. 8B, the DCT sign-only correlation function has a high absolute peak value, which indicates that the two audio fingerprints of the audio signal match.  Thus, the DCT sign-only correlation analysis confirms the match observed
based on Hamming distance.
<br/><br/> FIG. 9A is an example similarity matrix of an audio signal with 20% distortion of pitch shifting.  The audio signal illustrated in FIG. 9A is the same short musical excerpt as illustrated in FIG. 8A, and the pitch shifted version of the audio
signal has 20% distortion of pitch shifting.  The similarity matrix between the audio signal and its 20% pitch shifted version is based on Hamming distance.  The high amount of pitch shifting leads to significant changes in spectral content of the audio
signal, resulting in high Hamming distance.  Thus, the high matching rate is no longer observable as illustrated in FIG. 9A.  The distance based matching algorithms would identify the pitch shifted version of the audio signal as different audio content
from the audio signal.
<br/><br/> On the other hand, the DCT sign-only correlation based on the matching algorithm allows an audio identification system to identify certain pitch shifted versions of an audio signal as the same audio content as the audio signal.  FIG. 9B is an
illustration of DCT sign-only correction corresponding to the similarity matrix illustrated in FIG. 9A.  The DCT sign-only correlation function illustrated in FIG. 9B has a strong absolute peak value (e.g., higher than a predetermined threshold value),
which indicates the 20% pitch shifted audio signal still matches the audio signal, i.e., having the same audio content, but its pitch being shifted from its original pitch.  The degree of the pitch shift (e.g., 20%) can be determined by the position of
the peak value in the DCT sign-only correlation function.  Thus, the DCT sign-only correlation based matching can be used by the audio identification system for robust identification of pitch-shifted audio signals.
<br/><br/> Applications of DCT Sign-Only Correlation Based Audio Fingerprint Matching
<br/><br/> The DCT sign-only correlation based audio fingerprint matching has a variety of applications, such as for a user portable device to measure movement of the user.  Existing audio devices taking advantage of the Doppler Effect often require tools
in addition to audio signals to measure motion or movement of an object by detecting frequency and amplitude of waves emitted from the object.  The DCT sign-only correlation based audio fingerprint matching may eliminate or reduce the reliance on the
tools other than the audio signals themselves.  For example, a user may talk on a phone while exercising with fitness equipment.  The user movement can cause some distortion such as the pitch shifting in the audio signal of the phone conversation. 
Instead of using an accelerometer to measure the user movement, the distorted audio signal and a reference audio signal can be analyzed based on the DCT sign-only correlation between the corresponding audio fingerprints of the audio signals as described
above to measure the movement.
<br/><br/> Summary
<br/><br/> The foregoing description of the embodiments of the invention has been presented for the purpose of illustration; it is not intended to be exhaustive or to limit the invention to the precise forms disclosed.  Persons skilled in the relevant art
can appreciate that many modifications and variations are possible in light of the above disclosure.
<br/><br/> Some portions of this description describe the embodiments of the invention in terms of algorithms and symbolic representations of operations on information.  These algorithmic descriptions and representations are commonly used by those skilled
in the data processing arts to convey the substance of their work effectively to others skilled in the art.  These operations, while described functionally, computationally, or logically, are understood to be implemented by computer programs or
equivalent electrical circuits, microcode, or the like.  Furthermore, it has also proven convenient at times, to refer to these arrangements of operations as modules, without loss of generality.  The described operations and their associated modules may
be embodied in software, firmware, hardware, or any combinations thereof.
<br/><br/> Any of the steps, operations, or processes described herein may be performed or implemented with one or more hardware or software modules, alone or in combination with other devices.  In one embodiment, a software module is implemented with a
computer program product comprising a computer-readable medium containing computer program code, which can be executed by a computer processor for performing any or all of the steps, operations, or processes described.
<br/><br/> Embodiments of the invention may also relate to an apparatus for performing the operations herein.  This apparatus may be specially constructed for the required purposes, and/or it may include a general-purpose computing device selectively
activated or reconfigured by a computer program stored in the computer.  Such a computer program may be stored in a tangible computer readable storage medium or any type of media suitable for storing electronic instructions, and coupled to a computer
system bus.  Furthermore, any computing systems referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.
<br/><br/> Embodiments of the invention may also relate to a computer data signal embodied in a carrier wave, where the computer data signal includes any embodiment of a computer program product or other data combination described herein.  The computer
data signal is a product that is presented in a tangible medium or carrier wave and modulated or otherwise encoded in the carrier wave, which is tangible, and transmitted according to any suitable transmission method.
<br/><br/> Finally, the language used in the specification has been principally selected for readability and instructional purposes, and it may not have been selected to delineate or circumscribe the inventive subject matter.  It is therefore intended that
the scope of the invention be limited not by this detailed description, but rather by any claims that issue on an application based hereon.  Accordingly, the disclosure of the embodiments of the invention is intended to be illustrative, but not limiting,
of the scope of the invention, which is set forth in the following claims.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09390727&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D88%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D2%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D88%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D2%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209390727"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9390727&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D88%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D2%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209390727">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=88&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=1&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=88&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=2&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=88&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=3&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=87&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=2&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=89&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=2&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></body></html>