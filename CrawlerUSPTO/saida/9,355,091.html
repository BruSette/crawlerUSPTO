<html><head>
<base target="_top"/>
<title>United States Patent: 9355091</title></head>
<!---BUF1=9355091
BUF7=2016
BUF8=47924
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=432&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=432&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=432&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=10&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=431&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=433&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D432%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D9%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209355091"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9355091&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D432%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D9%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209355091">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09355091&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D432%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D9%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>432</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,355,091</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
         Herdagdelen
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     May 31, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">Systems and methods for language classification
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> Systems and methods are provided for classifying text based on language
     using one or more computer servers and storage devices. In general, the
     systems and methods can include a language classification module for
     classifying text of an input data set using the output of a training
     module. In an exemplary embodiment, a bootstrapping step feeds the output
     of the language classification module back into the training module to
     increase the accuracy of the language classification module. By iterating
     the language classification and training modules with input data having
     certain features, a user can tailor the language classification module
     for use with text having those or similar features.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Herdagdelen; Amac</b> (Mountain View, CA)<b>, Firat; Aykut</b> (Cambridge, MA)<b>, Bingham; Christopher</b> (Cambridge, MA) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/>Crimson Hexagon, Inc.</b> </td><td> <br/>Boston </td><td align="center"> <br/>MA </td><td align="center"> <br/>US </td> <td align="left">
</td> </tr> </tbody></table>
<!-- AANM>
~AANM Crimson Hexagon, Inc.
~AACI Boston
~AAST MA
~AACO US
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b>Crimson Hexagon, Inc.</b>
 (Boston, 
MA)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>51531783
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>13/798,890</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>March 13, 2013</b></td></tr>
     </tbody></table>
<hr/> <center><b>Prior Publication Data</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Document Identifier</u></b></th><th align="center" scope="col"><b><u>Publication Date</u></b></th></tr><tr><td align="center"> </td><td align="center"> US 20140278353 A1</td><td align="center">Sep 18, 2014</td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table>
     <hr/>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">G06F 17/28 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">G06F 17/28 (20060101)</td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9355091">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8606575">8606575</a></td><td align="left">
December 2013</td><td align="left">
Witt-ehsani</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20020022956&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2002/0022956</a></td><td align="left">
February 2002</td><td align="left">
Ukrainczyk et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090198487&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0198487</a></td><td align="left">
August 2009</td><td align="left">
Wong et al.</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
<table width="90%">   <tbody><tr><td><align="left"><br/>McCallum et al., A comparison of event models for naive bayes text classification. Proc. of AAAI-98 Workshop on Learning for Text Categorization. 1998, 8
pages. cited by applicant. </align="left"></td></tr> </tbody></table><br/><center><b>Other References</b></center> <br/>
       <i>Primary Examiner:</i> Bryar; Jeremiah
<br/>
       <i>Attorney, Agent or Firm:</i> <coma>Nutter McClennen &amp; Fish LLP
<br/>
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>What is claimed is: <br/><br/> 1.  A method for classifying text according to language using one or more computer processors, comprising: i) extracting one or more permutations of consecutive characters
from each of a plurality of training text strings included in a training data set, statistically associating the extracted one or more permutations of consecutive characters of each training text string with a language in which the training text string
is written, using a training module that comprises a training computer processor coupled to a memory having instructions causing the training computer processor to store statistical association data in a statistical association database;  ii) receiving
an input data set having one or more input text strings, by a classification module that comprises a classifying computer processor coupled to a memory including instructions causing the classifying computer processor to extract one or more permutations
of consecutive characters from each of the input text strings and classify each input text string according to a language in which the input text string is written by comparing the consecutive character permutations extracted from the input text string
with the statistical association data, tag each classified input text string with a tag that indicates its language, and output the tagged input text string;  and iii) receiving, by the training module, the tagged input text strings, statistically
associating the input tagged text strings with the tagged language, and updating the statistical association data in the statistical association database.
<br/><br/> 2.  The method of claim 1, wherein steps (ii) and (iii) are repeated one or more times.
<br/><br/> 3.  The method of claim 1, wherein the method further comprises accessing by the training module the training data set from a training data set database, each text string being associated with a tag that indicates a language in which the text
string is written.
<br/><br/> 4.  The method of claim 1, wherein the training data set includes one or more text strings from a social media website.
<br/><br/> 5.  The method of claim 1, wherein the input data set includes one or more text strings from a social media website.
<br/><br/> 6.  The method of claim 1, wherein the training data set and the input data set both include one or more text strings from at least one common source.
<br/><br/> 7.  The method of claim 6, wherein the statistical association data describes an incidence of one or more features of the text strings of the training data set in one or more languages.
<br/><br/> 8.  The method of claim 7, wherein the classification module classifies the text strings of the input data set by computing probabilities that each of the text strings was written in each of the one or more languages.
<br/><br/> 9.  The method of claim 1, wherein a user can specify one or more languages for classifying the input data set in step (iii).
<br/><br/> 10.  A method for classifying text according to language using one or more computer processors, comprising: i) accessing, by a training module, a training data set from a training data set database having a plurality of text strings, each text
string being associated with a tag that indicates a language in which the text string is written, the training module comprising a training computer processor coupled to a memory having instructions that cause the training computer processor to access
the training data;  ii) statistically associating, using the training module, one or more permutations of consecutive characters extracted from each of the text strings of the training data set with corresponding tagged language and storing statistical
association data, associating the permutations with their corresponding language, in a statistical association database;  iii) receiving an input data set having one or more input text strings by a classification module that comprises a classifying
computer processor coupled to a memory including instructions that cause the classifying computer processor to extract one or more permutations of consecutive characters from each of the input text strings and classify each input text string according to
a language in which the input text string is written by comparing the consecutive character permutations extracted from the input text string with the statistical association data, tag each classified input text string with a tag that indicates its
language, and output the tagged input text string;  and iv) receiving, by the training module the tagged input text strings, statistically associating the tagged input text strings with the tagged language, and updating the statistical association data
in the statistical association database.
<br/><br/> 11.  The method of claim 10, wherein steps (iii) and (iv) are repeated one or more times.
<br/><br/> 12.  The method of claim 10, wherein the training data set is tagged by one or more human users.
<br/><br/> 13.  A computer system for classifying text according to language, comprising: a training module that comprises a training computer processor coupled to a memory having instructions causing the processor to receive a training data set having one
or more text strings tagged with a tag that indicates a language in which the text string is written, statistically associate one or more permutations of consecutive characters extracted from each of the text strings of the training data set with
corresponding tagged language and storing statistical association data, associating the permutations with their corresponding language, in a statistical association database;  and a classification module that comprises a classifying computer processor
coupled to a memory having instructions causing the processor to receive an input data set having one or more input text strings, extract one or more permutations of consecutive characters from each of the input text strings, classify each input text
string according to a language in which the input text is written by comparing the consecutive character permutations extracted from the input text string with the statistical association data, tag each classified input text string with a tag that
indicates its language, and output the tagged input text string;  wherein the training module can be configured to receive the tagged text strings output by the classifying computer processor as the training data set.
<br/><br/> 14.  The system of claim 13, wherein the training data set includes one or more text strings from a social media website.
<br/><br/> 15.  The system of claim 13, wherein the input data set includes one or more text strings from a social media website.
<br/><br/> 16.  The system of claim 13, wherein the input data set and the training data set both include one or more text strings derived from at least one common source.
<br/><br/> 17.  The system of claim 13, wherein the statistical association data describes an incidence of one or more features of the text strings of the training data in one or more languages.
<br/><br/> 18.  The system of claim 17, wherein the classification module classifies the text strings of the input data set by computing probabilities that each of the text strings was written in each of the one or more languages.
<br/><br/> 19.  A non-transitory computer-readable storage medium containing instructions for classifying text according to language, wherein the instructions cause one or more computer processors to: i) access a training data set from a training data set
database having a plurality of text strings, each text string being associated with a tag that indicates a language in which the text string is written;  ii) statistically associate one or more permutations of consecutive characters extracted from each
of the text strings of the training data set with corresponding tagged language and storing statistical association data, associating the permutations with their corresponding language, in a statistical association database;  iii) receive an input data
set having one or more input text strings by a classification module that comprises a classifying computer processor coupled to a memory including instructions that cause the classifying computer processor to extract one or more permutations of
consecutive characters from each of the input text strings and classify each input text string according to a language in which the input text string is written by comparing the consecutive character permutations extracted from the input text string with
the statistical association data, tag each classified input text string with a tag that indicates its language, and output the tagged input text string;  and iv) receive, by the training module the tagged input text strings, statistically associating the
tagged input text strings with the tagged language, and updating the statistical association data in the statistical association database.
<br/><br/> 20.  The storage medium of claim 19, wherein the instructions cause the one or more processors to repeat steps (iii) and (iv) one or more times. <hr/> <center><b><i>Description</i></b></center> <hr/> <br/><br/>FIELD
<br/><br/> Exemplary embodiments of the present invention relate to classification of text according to language.
<br/><br/>BACKGROUND
<br/><br/> Efforts to extract meaning from source data--including documents and files containing text, audio, video, and other communications media--by classifying them into given categories, have a long history.  Increases in the amount of digital
content, such as web pages, blogs, emails, digitized books and articles, electronic versions of formal government reports and legislative hearings and records, and especially social media such as Twitter, <b><i>Facebook,</i></b> and LinkedIn posts, give rise to
computation challenges for those who desire to mine such voluminous information sources for useful meaning.
<br/><br/> Particularly as the territorial reach of the Internet expands, one obstacle to obtaining value from digital content containing text is language classification.  Categorization of text according to language is a prerequisite to any meaningful
computational analysis of its content.  Moreover, language classification can serve as a filter for information from a particular demographic.
<br/><br/> Existing language classification techniques have several drawbacks.  Some language classifiers adapted for use with digital content simply use information associated with an author profile, for example the author's location or primary language,
as a proxy for the language in which that author has written.  Of course, this approach is subject to error as the author may write in more than one language, none of which may be related to the author's profile information.
<br/><br/> More sophisticated language classification techniques use statistical association algorithms that categorize text based on the probability that certain features of the text, e.g., character combinations, will occur in a given language.  However,
such algorithms require a large amount of human-generated training data for each language supported by the algorithm, particularly where the data to be categorized only includes small amounts of text, e.g., posts on social media websites such as Twitter.
<br/><br/> Many commercially available products using the aforementioned statistical association algorithms suffer from the additional drawback that they are not customizable.  The algorithms are often trained using a standardized set of training data,
such as news articles or Wikipedia pages, which may not include features present in the data to be categorized.  Social media posts, for example, often contain jargon unique to a given social media website that might not be encompassed in the training
data.  In addition, it may be difficult to add or remove languages from the training data based on the user's needs.  Accordingly, there remains a need for improved language classifiers.
<br/><br/>SUMMARY
<br/><br/> The present invention generally provides systems and methods for classifying text according to language.  In one aspect, a method is provided for classifying text according to language using one or more computer processors.  The method can
include accessing by a training module a training data set from a training data set database having a plurality of text strings, each text string being associated with a tag that indicates a language in which the text string is written.  The training
module can include a training computer processor coupled to a memory having instructions that can cause the training computer processor to perform the accessing.  The method can also include using the training module to statistically associate one or
more features of each of a plurality of text strings of the training data set with a language in which the text string is written and store statistical association data in a statistical association database.  The method can also include receiving an
input data set having one or more text strings by a classification module.  The classification module can include a classifying computer processor coupled to a memory including instructions that can cause the classifying computer processor to classify
one or more of the text strings according to a language in which the text string is written, tag the classified text strings with a tag that indicates the language, and output the tagged text strings.  The method can also include receiving by the
training module the tagged text strings, statistically associating the tagged text strings with the tagged language, and updating the statistical association data in the statistical association database.
<br/><br/> The steps of receiving, classifying, and tagging the text strings of the input data set and then receiving and statistically associating the tagged text strings and updating the statistical association data can be repeated one or more times.  A
user can specify one or more languages for classifying the input data set in the classifying step.
<br/><br/> In another aspect, a computer system is provided for classifying text according to language.  The computer system can include a training module that can include a training computer processor coupled to a memory having instructions that can cause
the processor to receive a training data set from a training data set database having one or more text strings tagged with a tag that indicates a language in which the text string is written, statistically associate one or more features of each of the
text strings of the training data set with its tagged language, and store statistical association data in a statistical association database.  The computer system can further include a classification module that can include a classifying computer
processor coupled to a memory having instructions that can cause the processor to receive an input data set having one or more text strings, classify the one or more text strings according to a language in which the text is written, tag the classified
text strings with a tag that indicates the language, and output the tagged text strings.  The training module can be configured to receive the tagged text strings output by the classifying computer processor as the training data set.
<br/><br/> For both the method and the computer system described above, the training data set and/or the input data set can include one or more text strings from a social media website.  The training data set and the input data set can both include one or
more text strings from at least one common source.  The statistical association data can describe an incidence of one or more features of the text strings of the training data set in one or more languages, and the classification module can classify the
text strings of the input data set by computing probabilities that each of the text strings was written in each of the one or more languages.
<br/><br/> In another aspect, a non-transitory computer-readable storage medium can contain instructions for classifying text according to language.  The instructions can cause one or more computer processors to access a training data set database having a
plurality of text strings, each text string being associated with a tag that indicates a language in which the text string is written.  The instructions can also cause the one or more computer processors to statistically associate one or more features of
each of the text strings of a training data set with its tagged language and store statistical association data in a statistical association database.  The instructions can also cause the one or more computer processors to receive an input data set
having one or more text strings, classify one or more of the text strings according to a language in which the text string is written, tag the classified text strings with a tag that indicates the language, and output the tagged text strings.  The
instructions can also cause the one or more computer processors to receive the tagged text strings, statistically associate the tagged text strings with the tagged language, and update the statistical association data in the statistical association
database.  The instructions can cause the one or more computer processors to repeat the steps of receiving, classifying, and tagging text strings of the input data set and then receiving and statistically associating the tagged text strings and updating
the statistical association data one or more times.
<br/><br/> The present invention further provides devices, systems, and methods as claimed. <br/><br/>BRIEF DESCRIPTION OF THE DRAWINGS
<br/><br/> FIG. 1 is a schematic diagram of one exemplary embodiment of a computer system;
<br/><br/> FIG. 2 is a schematic diagram of one exemplary embodiment of a language classifier system;
<br/><br/> FIG. 3 is a flowchart that schematically depicts an exemplary method of a training module for use with the language classifier system of FIG. 2;
<br/><br/> FIG. 4 is a schematic diagram of one exemplary step of the method of FIG. 3;
<br/><br/> FIG. 5 is a schematic diagram of one exemplary embodiment of a statistical association data set generated by the method of FIG. 3;
<br/><br/> FIG. 6 is a flowchart that schematically depicts an exemplary method of a classification module for use with the language classifier system of FIG. 2; and
<br/><br/> FIG. 7 is a flowchart that schematically depicts an exemplary method of a bootstrapping step for use with the language classifier system of FIG. 2.
<br/><br/>DETAILED DESCRIPTION OF THE INVENTION
<br/><br/> Systems and methods are provided for classifying text based on language using one or more computer servers and storage devices.  In general, the systems and methods can include a classification module for classifying text of an input data set
using the output of a training module.  In an exemplary embodiment, a bootstrapping step feeds the output of the classification module back into the training module to increase the accuracy of the language classification module.  By iterating the
classification and training modules with input data having certain features, a user can tailor the classification module for use with text having those or similar features.
<br/><br/> Certain exemplary embodiments will now be described to provide an overall understanding of the principles of the structure, function, manufacture, and use of the methods, systems, and devices disclosed herein.  One or more examples of these
embodiments are illustrated in the accompanying drawings.  Those skilled in the art will understand that the methods, systems, and devices specifically described herein and illustrated in the accompanying drawings are non-limiting exemplary embodiments
and that the scope of the present invention is defined solely by the claims.  The features illustrated or described in connection with one exemplary embodiment may be combined with the features of other embodiments.  Such modifications and variations are
intended to be included within the scope of the present invention.
<br/><br/> Computer Processor
<br/><br/> The systems and methods disclosed herein can be implemented using one or more computer systems, such as the exemplary embodiment of a computer system 100 shown in FIG. 1.  As shown, the computer system 100 can include one or more processors 102
which can control the operation of the computer system 100.  The processor(s) 102 can include any type of microprocessor or central processing unit (CPU), including programmable general-purpose or special-purpose microprocessors and/or any one of a
variety of proprietary or commercially available single or multi-processor systems.  The computer system 100 can also include one or more memories 104, which can provide temporary storage for code to be executed by the processor(s) 102 or for data
acquired from one or more users, storage devices, and/or databases.  The memory 104 can include read-only memory (ROM), flash memory, one or more varieties of random access memory (RAM) (e.g., static RAM (SRAM), dynamic RAM (DRAM), or synchronous DRAM
(SDRAM)), and/or a combination of memory technologies.
<br/><br/> The various elements of the computer system 100 can be coupled to a bus system 112.  The illustrated bus system 112 is an abstraction that represents any one or more separate physical busses, communication lines/interfaces, and/or multi-drop or
point-to-point connections, connected by appropriate bridges, adapters, and/or controllers.  The computer system 100 can also include one or more network interface(s) 106, one or more input/output (IO) interface(s) 108, and one or more storage device(s)
110.
<br/><br/> The network interface(s) 106 can enable the computer system 100 to communicate with remote devices (e.g., other computer systems) over a network, and can be, for example, remote desktop connection interfaces, Ethernet adapters, and/or other
local area network (LAN) adapters.  The IO interface(s) 108 can include one or more interface components to connect the computer system 100 with other electronic equipment.  For example, the IO interface(s) 108 can include high speed data ports, such as
USB ports, 1394 ports, etc. Additionally, the computer system 100 can be accessible to a human user, and thus the IO interface(s) 108 can include displays, speakers, keyboards, pointing devices, and/or various other video, audio, or alphanumeric
interfaces.  The storage device(s) 110 can include any conventional medium for storing data in a non-volatile and/or non-transient manner.  The storage device(s) 110 can thus hold data and/or instructions in a persistent state (i.e., the value is
retained despite interruption of power to the computer system 100).  The storage device(s) 110 can include one or more hard disk drives, flash drives, USB drives, optical drives, various media cards, and/or any combination thereof and can be directly
connected to the computer system 100 or remotely connected thereto, such as over a network.  The elements illustrated in FIG. 1 can be some or all of the elements of a single physical machine.  In addition, not all of the illustrated elements need to be
located on or in the same physical or logical machine.  Rather, the illustrated elements can be distributed in nature, e.g., using a server farm or cloud-based technology.  Exemplary computer systems include conventional desktop computers, workstations,
minicomputers, laptop computers, tablet computers, PDAs, mobile phones, and the like.
<br/><br/> Although an exemplary computer system is depicted and described herein, it will be appreciated that this is for sake of generality and convenience.  In other embodiments, the computer system may differ in architecture and operation from that
shown and described here.
<br/><br/> Modules
<br/><br/> The various functions performed by the computer system 100 can be logically described as being performed by one or more modules.  It will be appreciated that such modules can be implemented in hardware, software, or a combination thereof.  It
will further be appreciated that, when implemented in software, modules can be part of a single program or one or more separate programs, and can be implemented in a variety of contexts (e.g., as part of an operating system, a device driver, a standalone
application, and/or combinations thereof).  In addition, software embodying one or more modules is not a signal and can be stored as an executable program on one or more non-transitory computer-readable storage mediums.  Functions disclosed herein as
being performed by a particular module can also be performed by any other module or combination of modules.
<br/><br/> An exemplary system 10 for carrying out the invention is disclosed in FIG. 2, and can operate as follows: A training data classification module 14 presents one or more human users with a text string from a training data set 12 and receives from
the one or more human users a language classification for the text string.  The text string can then be "tagged" with the language and added to the tagged training data set 16.  Using the tagged training data 16, a training module 18 creates statistical
association data 20 between features of the tagged data 16 and the language with which the data 16 is tagged.  An input data set 22 can then be fed into a classification module 24, which uses the statistical association data 20 to tag each text item of
the input data set 22 according to language, based on the probability that each text item of the input data set 22 was written in one or more languages.  To enhance the accuracy of the classification module 24, the tagged output data 26 is then
"bootstrapped" back into the training module 18 to create additional statistical association data 20 between the tagged output data 26 and the one or more languages.  The training module 18 and classification module 24 can then be repeated any number
times with a new input data set 22, with each iteration increasing the accuracy of the classification module 24, particularly the accuracy of the classification module 24 for the type of input data 22 used in that iteration.
<br/><br/> The system 10 can include fewer or more modules than what is shown and described herein and can be implemented using one or more digital data processing systems of the type described above.  The system 10 can thus be implemented on a single
computer system, or can be distributed across a plurality of computer systems, e.g., across a "cloud." The system 10 also includes a plurality of databases, which can be stored on and accessed by computer systems.  It will be appreciated that any of the
modules or databases disclosed herein can be subdivided or can be combined with other modules or databases.
<br/><br/> Training Module
<br/><br/> One exemplary embodiment of the training module 18 can be configured to generate statistical association data 20 between certain features of the tagged training data set 16 and language.  The training module 18 can include a training computer
processor and a memory coupled to the training computer processor.  The memory can contain instructions causing the training computer processor to perform the steps outlined in FIG. 3, although it will be appreciated that any number of steps can be added
to or removed from the exemplary method performed by the training module 18.  Moreover, it should be noted that any ordering of method steps implied by such flowcharts or the description thereof is not to be construed as limiting the method to performing
the steps in that order.  Rather, the various steps of each of the methods disclosed herein can be performed in a variety of sequences.
<br/><br/> First, the training computer processor can receive 30 the tagged training data 16 from a training data set database.  The tagged training data 16 can include a plurality of text strings of any length that have been obtained from any one or more
sources, e.g., social media websites, blogs, etc. Each of the text strings of the tagged training data set 16 can be associated with a tag that indicates a language in which the text string is written.  In an exemplary embodiment, the tags associated
with each text string of the tagged training data set 16 have been generated by one or more human users.
<br/><br/> Given the tagged training data 16, the training computer processor can generate 32 statistical association data 20 between certain features of the tagged training data 16 and language according to the one or more training algorithms.  One
exemplary training algorithm creates statistical association data 20 between one or more consecutive characters of a text string and the language in which the text string is written.  FIG. 4 illustrates an application of the exemplary training algorithm
to an exemplary text string 15 that is associated with a tag indicating that it is written in English.  First, the text string 15 can be broken down into character permutations 15a, 15b, 15c, 15d, 15e, 15f, 15g of more than one consecutive character. 
The character permutations can include, e.g., letters, numbers, punctuation marks, and may or may not be case sensitive.  The training algorithm can extract all possible character permutations within the text string 15, or only some of the character
permutations within the text string 15.  Although the text string 15 is broken down into triplets in the example of FIG. 4, the character permutations 15a, 15b, 15c, 15d, 15e, 15f, 15g can consist of any number of consecutive characters.  Because the tag
associated with the text string 15 indicates that the text string 15 is written in English, each character permutation 15a, 15b, 15c, 15d, 15e, 15f, 15g can be assigned a "point" value for English.
<br/><br/> Applying this exemplary training algorithm to each text string of the training data set 12, the training module 18 can generate 32 statistical association data 20 between character permutations of the tagged training data 16 and the one or more
languages.  The statistical association data 20 can then be stored 34 in a statistical association database, and can be, e.g., in the form of a matrix 17 of the character permutations 15a, 15b, 15c, 15d, 15e, 15f, 15g and corresponding point values
reflecting an incidence of each of the character permutations 15a, 15b, 15c, 15d, 15e, 15f, 15g in each of the one or more languages.  FIG. 5 depicts the exemplary matrix 17 that could have been generated based on the exemplary text string 15 of FIG. 4
and stored in the statistical association database.  Different types of statistical association data 20 can be generated for different types of training algorithms, however.  By way of non-limiting example, the statistical association data 20 can
associate average word length, grammatical structure, use of certain letters or punctuation marks, etc., with language.
<br/><br/> Classification Module
<br/><br/> Using the statistical association data 20 generated 32 by the training module 18, the exemplary classification module 34 can classify text of the input data set 22 according to language.  The classification module 34 can include a classifying
computer processor and a memory coupled to the classifying computer processor.  The memory can contain instructions that cause the classifying computer processor to perform the various steps outlined in FIG. 6, although it will be appreciated that any
number of steps can be added to or removed from the exemplary method performed by the classification module 34.
<br/><br/> First, the classifying computer processor can receive 36 the input data set 22.  Like the training data set 12, the input data set 22 can include one or more text strings of any length and can be obtained from a variety of sources, either the
same or different from the sources used to generate the training data set 12.  In an exemplary embodiment, however, the input data set 22 is generated from at least one source that was used to generate the training data set 12.  In general, the input
data set 22 is supplied by the end user.  The classifying computer processor can prompt the end user for input regarding the source or sources of the input data 22, and the classifying computer processor can then retrieve the input data 22 from one or
more databases containing data from the requested sources.  The source or sources can be selected by the end user according to various constraints, e.g., website, time range, author profile information, etc. In another aspect, the end user can input the
input data set 22 directly into the classification module 34.
<br/><br/> The classifying computer processor can then classify 38 the input data set 22 according to language using one or more language classification algorithms.  Several probability-based algorithms have been applied to the problem of language
classification, such as the Naive-Bayes method, described in A. McCallum and K. Nigam, "A Comparison of Event Models for Naive Bayes Text Classification," Proc.  of AAAI-98 Workshop on Learning for Text Categorization (1998).  The Naive-Bayes method and
similar methods classify text by calculating a probability that the text was written in a particular language, based on statistical association data between certain features of text and language.  According to an exemplary embodiment of the
classification module 34, a language classification algorithm classifies the input data set 22 according to language based on probabilities computed using the statistical association data 20.
<br/><br/> An exemplary embodiment of a classification algorithm that can be used in step 38 can operate as follows: First, the classification algorithm extracts character permutations from a text string of the input data set 22, similarly to the process
exemplified in FIG. 4 and described above for the training module 18.  The classification algorithm then searches the statistical association data set 20 for each of the character permutations derived from the text string of the input data set 22.  If a
character permutation can be found in the statistical association data set 20, then the text string will be assigned the corresponding point value, or point values, of that character permutation for the language, or languages, with which the character
permutation is associated.  For example, given the statistical association data matrix 17 of FIG. 5, if the character permutation 15c is extracted from the text string of the input data set 22, the text string will be assigned 1 point for English and 0
points for both Spanish and Portuguese.  The search and point assignment process is repeated for each character permutation extracted from the text string, and all of the points for each language are added to yield a total point value for each of the one
or more languages.  Based on the total point values for each of the one or more languages, the classification algorithm computes a probability that the text string was written in each of the one or more languages.  The text string is then classified as
being written in the language for which the text string has the highest computed probability, and the classification algorithm is repeated for the remaining text strings of the input data set 22.
<br/><br/> Having classified 38 the input data set 22 according to language, the classifying computer processor can associate each text string of the input data set 22 with a tag indicating a language in which the text string was written to generate 40 the
tagged output data set 26.  The tagged output data set 26 from the classification module 34 can be output 42 in any format.  In one exemplary embodiment, the tagged output data 26 is organized into an easily readable format, e.g., a chart, a graph, etc.,
and output 42 to a display monitor to be viewable by the end user.  The tagged output data 26 can also be stored in one or more databases for further use.
<br/><br/> The method performed by the classification module 34 can include one or more alternative steps for tagging text strings under certain circumstances, either automatically or at the option of the end user.  By way of non-limiting example, if a
probability that a text string was written in all of the one or more languages is zero, then the text string may simply be discarded before the classification module 34 reaches step 40.  If probabilities that a text string was written in more than one of
the one or more languages are within a pre-specified range of each other, e.g., within 5%, then the text string may similarly be discarded before the classification module 34 reaches step 40.
<br/><br/> The classification module 34 can include various options to allow for customization by the end user.  In one aspect, the end user can select the one or more languages for classification using the classification module 34.  Other non-limiting
examples of options for the end user include selecting the one or more classification algorithms to be used for classifying 38 the input data 22 according to language, selecting a format and/or a destination for outputting 42 the tagged output data 26,
etc. For all of the aforementioned examples, the classification module 34 can have default selections in the absence of input by the end user.
<br/><br/> Bootstrapping Step
<br/><br/> The training module 18 and the classification module 34 described above can produce inaccurate results where features of the input data set 22 are not encompassed in the statistical association data set 20 generated from the training data set
12.  This problem can be alleviated by increasing a size of the training data set 12, but increasing the size of the training data set 12 can require additional input by human users to tag the training data set 12 according to language, which can be
time-consuming and costly.  Moreover, when the training data 12 is generated from one or more sources that are different from one or more sources used to generate the input data set 22, the training data 12 may not contain the same features as the input
data set 22, regardless of the size of the training data set 12.  A likelihood that a feature, e.g., a character permutation, will not be encompassed within the statistical association data set 20 can be particularly high where the one or more sources
for the input data set 22 includes a social media website.  Text strings obtained from social media websites, e.g., posts, "tweets," etc., can often be short and can contain jargon unique to a particular social media website.  For example, an exemplary
Twitter post can contain only two words, both of which may be abbreviations or jargon that are unique to Twitter posts.  The accuracy of the classification module 34 therefore relies on the statistical association data set 20 encompassing the two words
contained in the Twitter post, or close variations thereof.
<br/><br/> To increase a likelihood that features of the input data set 22 are contained within the statistical association data set 20, the tagged output data 26 of the classification module 34 can be "bootstrapped" back into the training module 18
according to an exemplary bootstrapping step 50, illustrated in FIG. 7.  Instructions contained in a memory coupled to a training computer processor can cause the training computer processor to receive 44 the tagged output data 26.  The training computer
processor can then generate 46 statistical association data 20 between certain features of the tagged output data 26 and language using one or more training algorithms, such as the training algorithm described above.  The resulting statistical
association data 20 can then be added to the statistical association database to update 48 the statistical association database, which can already contain statistical association data 20 from previous iterations of the training module 18.  The
bootstrapping step 50 can be repeated any number of times, thus serving the dual purpose of increasing the size of the statistical association database and customizing the classification module 24 for use with input data 22 having certain features.
<br/><br/> One drawback to existing bootstrapping methods is the potential for error propagation.  In the present example, a text string that is incorrectly classified by the classification module 24 can be fed back into the training module 18 to produce
incorrect statistical association data 20.  However, the impact of such errors is unlikely to noticeably reduce an accuracy of the classification module 24 due to the training and classifying computer processors' capacity to process large amounts of
data.  Using as an example the training and classification algorithms described above, improper distribution of one point to certain character permutations will have a minimal impact on the probability calculations of the classification algorithm where
each character permutation has a large number of points.  Because a volume of data within the tagged training data set 16 and the tagged output data set 26 is limited only by the capacity of the computer processors and storage devices employed, the
amount of statistical association data 20 generated by the training module 18 can be increased significantly and efficiently, particularly where cloud-based technology and/or multiple computer processors are used.
<br/><br/> Under certain circumstances, one or more of the text strings of the tagged output data set 26 may not be fed back into the training module 18, particularly where there is a high likelihood that the one or more text strings were tagged
incorrectly.  By way of non-limiting example, where a probability that a text string was written in any of the one or more languages is zero, and/or where probabilities that the text string was written in more than one language are within a pre-specified
range of each other, e.g., within 5%, then the text string may not be fed back into the training module 18.  Such data may not represent good training data, and thus may not undergo the bootstrapping step 50 either automatically or at the option of a
user.  In another aspect, the tagged output data set 26 may not be bootstrapped back into the training module 18 where a size of the statistical association data set 20 has reached a predetermined limit.  It will be understood by a person skilled in the
art that such modifications to the bootstrapping step 50 are not necessary, however.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09355091&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D432%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D9%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D432%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D9%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209355091"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9355091&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D432%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D9%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209355091">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=432&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=432&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=432&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=10&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=431&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=433&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></body></html>