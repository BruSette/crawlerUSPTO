<html><head>
<base target="_top"/>
<title>United States Patent: 9355300</title></head>
<!---BUF1=9355300
BUF7=2016
BUF8=105851
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=425&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=425&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=425&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=10&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=424&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=426&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D425%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D9%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209355300"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9355300&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D425%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D9%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209355300">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09355300&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D425%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D9%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>425</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,355,300</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
         Baluja
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     May 31, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">Inferring the gender of a face in an image
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> The subject matter of this specification can be embodied in, among other
     things, a computer-implemented method that includes receiving a plurality
     of images having human faces. The method further includes generating a
     data structure having representations of the faces and associations that
     link the representations based on similarities in appearance between the
     faces. The method further includes outputting a first gender value for a
     first representation of a first face that indicates a gender of the first
     face based on one or more other gender values of one or more other
     representations of one or more other faces that are linked to the first
     representation.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Baluja; Shumeet</b> (Leesburg, VA)<b>, Jing; Yushi</b> (Mountain View, CA) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/>Google Inc.</b> </td><td> <br/>Mountain View </td><td align="center"> <br/>CA </td><td align="center"> <br/>US </td> <td align="left"> </td>
</tr> </tbody></table>
<!-- AANM>
~AANM Google Inc.
~AACI Mountain View
~AAST CA
~AACO US
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b>Google Inc.</b>
 (Mountain View, 
CA)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>44773374
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>14/056,852</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>October 17, 2013</b></td></tr>
     </tbody></table>
<hr/> <center><b>Related U.S. Patent Documents</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col" width="7%"></th><th scope="col"></th><th scope="col"></th> <th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Application Number</u></b></th><th align="center" scope="col"><b><u>Filing Date</u></b></th><th align="center" scope="col"><b><u>Patent Number</u></b></th><th align="center" scope="col"><b><u>Issue Date</u></b></th></tr><tr><td align="center"> </td><td align="center">13274778</td><td align="center">Oct 17, 2011</td><td align="center">8588482</td><td align="center"></td></tr><tr><td align="center"> </td><td align="center">11934547</td><td align="center">Oct 18, 2011</td><td align="center">8041082</td><td align="center"></td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table><td< td=""></td<><td< td=""></td<><td< td=""></td<>     <hr/>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">G06K 9/00288 (20130101); G06Q 50/01 (20130101); G06K 9/00221 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">G06K 9/62 (20060101); G06K 9/00 (20060101); G06Q 99/00 (20060101)</td></tr>
       <tr><td align="left" valign="top" width="30%"><b>Field of Search: </b></td>
       <td align="right" valign="top" width="70%">
       



















 ;382/118,276,159,190,115,100,224 ;707/E15.042,E15.017,803,713,714,716,731,796,798,E17.009,E17.999 ;705/14.4,319
       </td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9355300">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5724521">5724521</a></td><td align="left">
March 1998</td><td align="left">
Dedrick</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5740549">5740549</a></td><td align="left">
April 1998</td><td align="left">
Reilly et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5848397">5848397</a></td><td align="left">
December 1998</td><td align="left">
Marsh et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5913205">5913205</a></td><td align="left">
June 1999</td><td align="left">
Jain et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5918014">5918014</a></td><td align="left">
June 1999</td><td align="left">
Robinson</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5948061">5948061</a></td><td align="left">
September 1999</td><td align="left">
Merriman</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5991429">5991429</a></td><td align="left">
November 1999</td><td align="left">
Coffin et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6009422">6009422</a></td><td align="left">
December 1999</td><td align="left">
Ciccarelli</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6026368">6026368</a></td><td align="left">
February 2000</td><td align="left">
Brown et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6044376">6044376</a></td><td align="left">
March 2000</td><td align="left">
Kurtzman, II</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6078914">6078914</a></td><td align="left">
June 2000</td><td align="left">
Redfern</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6144944">6144944</a></td><td align="left">
November 2000</td><td align="left">
Kurtzman et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6167382">6167382</a></td><td align="left">
December 2000</td><td align="left">
Sparks et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6256648">6256648</a></td><td align="left">
July 2001</td><td align="left">
Hill et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6269361">6269361</a></td><td align="left">
July 2001</td><td align="left">
Davis et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6356659">6356659</a></td><td align="left">
March 2002</td><td align="left">
Wiskott et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6389372">6389372</a></td><td align="left">
May 2002</td><td align="left">
Glance et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6400853">6400853</a></td><td align="left">
June 2002</td><td align="left">
Shiiyama</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6401075">6401075</a></td><td align="left">
June 2002</td><td align="left">
Mason et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6549896">6549896</a></td><td align="left">
April 2003</td><td align="left">
Candan et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6594673">6594673</a></td><td align="left">
July 2003</td><td align="left">
Smith et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6721733">6721733</a></td><td align="left">
April 2004</td><td align="left">
Lipson et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6792419">6792419</a></td><td align="left">
September 2004</td><td align="left">
Raghavan</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6816836">6816836</a></td><td align="left">
November 2004</td><td align="left">
Basu et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6985882">6985882</a></td><td align="left">
January 2006</td><td align="left">
Del Sesto</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7035467">7035467</a></td><td align="left">
April 2006</td><td align="left">
Nicponski</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7039599">7039599</a></td><td align="left">
May 2006</td><td align="left">
Merriman</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7136875">7136875</a></td><td align="left">
November 2006</td><td align="left">
Anderson et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7324670">7324670</a></td><td align="left">
January 2008</td><td align="left">
Kozakaya et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7492943">7492943</a></td><td align="left">
February 2009</td><td align="left">
Li et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7689682">7689682</a></td><td align="left">
March 2010</td><td align="left">
Eldering et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7701608">7701608</a></td><td align="left">
April 2010</td><td align="left">
Katayama et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7739276">7739276</a></td><td align="left">
June 2010</td><td align="left">
Lee et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7765218">7765218</a></td><td align="left">
July 2010</td><td align="left">
Bates et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7801907">7801907</a></td><td align="left">
September 2010</td><td align="left">
Fischer et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7801956">7801956</a></td><td align="left">
September 2010</td><td align="left">
Cumberbatch et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7809163">7809163</a></td><td align="left">
October 2010</td><td align="left">
Sheu</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7831595">7831595</a></td><td align="left">
November 2010</td><td align="left">
Suresh et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7853622">7853622</a></td><td align="left">
December 2010</td><td align="left">
Baluja et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7860386">7860386</a></td><td align="left">
December 2010</td><td align="left">
Terashima</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7899218">7899218</a></td><td align="left">
March 2011</td><td align="left">
Satoshi</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7904461">7904461</a></td><td align="left">
March 2011</td><td align="left">
Baluja et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7961986">7961986</a></td><td align="left">
June 2011</td><td align="left">
Jing et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8027541">8027541</a></td><td align="left">
September 2011</td><td align="left">
Hua et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8041082">8041082</a></td><td align="left">
October 2011</td><td align="left">
Baluja et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8055664">8055664</a></td><td align="left">
November 2011</td><td align="left">
Baluja et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8140570">8140570</a></td><td align="left">
March 2012</td><td align="left">
Ingrassia et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8588482">8588482</a></td><td align="left">
November 2013</td><td align="left">
Baluja et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20020023230&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2002/0023230</a></td><td align="left">
February 2002</td><td align="left">
Bolnick et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20020116466&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2002/0116466</a></td><td align="left">
August 2002</td><td align="left">
Trevithick et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20020120506&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2002/0120506</a></td><td align="left">
August 2002</td><td align="left">
Hagen</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20020124053&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2002/0124053</a></td><td align="left">
September 2002</td><td align="left">
Adams et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20030013951&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2003/0013951</a></td><td align="left">
January 2003</td><td align="left">
Stefanescu et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20030050977&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2003/0050977</a></td><td align="left">
March 2003</td><td align="left">
Puthenkulam et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040042599&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0042599</a></td><td align="left">
March 2004</td><td align="left">
Zaner et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040088325&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0088325</a></td><td align="left">
May 2004</td><td align="left">
Elder et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040098362&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0098362</a></td><td align="left">
May 2004</td><td align="left">
Gargi</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040122803&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0122803</a></td><td align="left">
June 2004</td><td align="left">
Dom et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040143841&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0143841</a></td><td align="left">
July 2004</td><td align="left">
Wang et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040148275&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0148275</a></td><td align="left">
July 2004</td><td align="left">
Achlioptas</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040202349&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0202349</a></td><td align="left">
October 2004</td><td align="left">
Erol et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040215793&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0215793</a></td><td align="left">
October 2004</td><td align="left">
Ryan et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040267604&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0267604</a></td><td align="left">
December 2004</td><td align="left">
Gross</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050043897&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0043897</a></td><td align="left">
February 2005</td><td align="left">
Meyer</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050091202&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0091202</a></td><td align="left">
April 2005</td><td align="left">
Thomas</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050114325&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0114325</a></td><td align="left">
May 2005</td><td align="left">
Liu et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050125308&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0125308</a></td><td align="left">
June 2005</td><td align="left">
Puentes et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050125408&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0125408</a></td><td align="left">
June 2005</td><td align="left">
Somaroo et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050144069&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0144069</a></td><td align="left">
June 2005</td><td align="left">
Wiseman et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050149395&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0149395</a></td><td align="left">
July 2005</td><td align="left">
Henkin et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050154639&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0154639</a></td><td align="left">
July 2005</td><td align="left">
Zetmeir</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050159998&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0159998</a></td><td align="left">
July 2005</td><td align="left">
Buyukkokten et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050171832&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0171832</a></td><td align="left">
August 2005</td><td align="left">
Hull et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050198031&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0198031</a></td><td align="left">
September 2005</td><td align="left">
Pezaris et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050216300&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0216300</a></td><td align="left">
September 2005</td><td align="left">
Appelman et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050278443&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0278443</a></td><td align="left">
December 2005</td><td align="left">
Winner et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060004704&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0004704</a></td><td align="left">
January 2006</td><td align="left">
Gross</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060031121&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0031121</a></td><td align="left">
February 2006</td><td align="left">
Speicher</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060069584&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0069584</a></td><td align="left">
March 2006</td><td align="left">
Bates et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060085259&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0085259</a></td><td align="left">
April 2006</td><td align="left">
Nicholas et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060136098&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0136098</a></td><td align="left">
June 2006</td><td align="left">
Chitrapura et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060159343&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0159343</a></td><td align="left">
July 2006</td><td align="left">
Grady</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060165040&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0165040</a></td><td align="left">
July 2006</td><td align="left">
Rathod et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060184617&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0184617</a></td><td align="left">
August 2006</td><td align="left">
Nicholas et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060190225&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0190225</a></td><td align="left">
August 2006</td><td align="left">
Grand</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060195442&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0195442</a></td><td align="left">
August 2006</td><td align="left">
Cone et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060200432&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0200432</a></td><td align="left">
September 2006</td><td align="left">
Flinn et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060200434&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0200434</a></td><td align="left">
September 2006</td><td align="left">
Flinn et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060200435&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0200435</a></td><td align="left">
September 2006</td><td align="left">
Flinn et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060204142&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0204142</a></td><td align="left">
September 2006</td><td align="left">
West et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060218577&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0218577</a></td><td align="left">
September 2006</td><td align="left">
Goodman et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060224675&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0224675</a></td><td align="left">
October 2006</td><td align="left">
Fox et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060247940&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0247940</a></td><td align="left">
November 2006</td><td align="left">
Zhu et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060248573&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0248573</a></td><td align="left">
November 2006</td><td align="left">
Pannu et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060271460&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0271460</a></td><td align="left">
November 2006</td><td align="left">
Hanif</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060282328&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0282328</a></td><td align="left">
December 2006</td><td align="left">
Gerace et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060294084&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0294084</a></td><td align="left">
December 2006</td><td align="left">
Patel et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060294134&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0294134</a></td><td align="left">
December 2006</td><td align="left">
Berkhim et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070005341&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0005341</a></td><td align="left">
January 2007</td><td align="left">
Burges et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070043688&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0043688</a></td><td align="left">
February 2007</td><td align="left">
Kountz et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070043766&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0043766</a></td><td align="left">
February 2007</td><td align="left">
Nicholas et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070050446&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0050446</a></td><td align="left">
March 2007</td><td align="left">
Moore</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070078846&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0078846</a></td><td align="left">
April 2007</td><td align="left">
Gulli et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070106551&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0106551</a></td><td align="left">
May 2007</td><td align="left">
McGucken</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070121843&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0121843</a></td><td align="left">
May 2007</td><td align="left">
Atazky et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070124721&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0124721</a></td><td align="left">
May 2007</td><td align="left">
Cowing et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070156614&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0156614</a></td><td align="left">
July 2007</td><td align="left">
Flinn et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070192306&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0192306</a></td><td align="left">
August 2007</td><td align="left">
Papakonstantinou et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070203872&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0203872</a></td><td align="left">
August 2007</td><td align="left">
Flinn et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070203940&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0203940</a></td><td align="left">
August 2007</td><td align="left">
Wang et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070218900&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0218900</a></td><td align="left">
September 2007</td><td align="left">
Abhyanker</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070288462&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0288462</a></td><td align="left">
December 2007</td><td align="left">
Fischer et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080004951&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0004951</a></td><td align="left">
January 2008</td><td align="left">
Huang et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080010275&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0010275</a></td><td align="left">
January 2008</td><td align="left">
Lee et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080091834&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0091834</a></td><td align="left">
April 2008</td><td align="left">
Norton</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080103784&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0103784</a></td><td align="left">
May 2008</td><td align="left">
Wong et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080103877&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0103877</a></td><td align="left">
May 2008</td><td align="left">
Gerken</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080104079&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0104079</a></td><td align="left">
May 2008</td><td align="left">
Craig</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080104225&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0104225</a></td><td align="left">
May 2008</td><td align="left">
Zhang et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080120308&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0120308</a></td><td align="left">
May 2008</td><td align="left">
Martinez et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080120411&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0120411</a></td><td align="left">
May 2008</td><td align="left">
Eberle</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080126476&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0126476</a></td><td align="left">
May 2008</td><td align="left">
Nicholas et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080140650&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0140650</a></td><td align="left">
June 2008</td><td align="left">
Stackpole</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080155080&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0155080</a></td><td align="left">
June 2008</td><td align="left">
Marlow et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080159590&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0159590</a></td><td align="left">
July 2008</td><td align="left">
Yi et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080162431&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0162431</a></td><td align="left">
July 2008</td><td align="left">
Xu et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080162510&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0162510</a></td><td align="left">
July 2008</td><td align="left">
Balo et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080189169&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0189169</a></td><td align="left">
August 2008</td><td align="left">
Turpin et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080195657&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0195657</a></td><td align="left">
August 2008</td><td align="left">
Naaman et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080215416&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0215416</a></td><td align="left">
September 2008</td><td align="left">
Ismalon</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080222295&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0222295</a></td><td align="left">
September 2008</td><td align="left">
Robinson et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080243607&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0243607</a></td><td align="left">
October 2008</td><td align="left">
Rohan et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080249966&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0249966</a></td><td align="left">
October 2008</td><td align="left">
Luege Mateos</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080275899&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0275899</a></td><td align="left">
November 2008</td><td align="left">
Baluja et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090018918&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0018918</a></td><td align="left">
January 2009</td><td align="left">
Moneypenny et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090024548&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0024548</a></td><td align="left">
January 2009</td><td align="left">
Zhu et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090063284&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0063284</a></td><td align="left">
March 2009</td><td align="left">
Turpin et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090076800&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0076800</a></td><td align="left">
March 2009</td><td align="left">
Li et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090112701&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0112701</a></td><td align="left">
April 2009</td><td align="left">
Turpin et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090144075&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0144075</a></td><td align="left">
June 2009</td><td align="left">
Flinn et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090192967&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0192967</a></td><td align="left">
July 2009</td><td align="left">
Luo et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090248661&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0248661</a></td><td align="left">
October 2009</td><td align="left">
Bilenko et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110112916&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0112916</a></td><td align="left">
May 2011</td><td align="left">
Baluja et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110219073&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0219073</a></td><td align="left">
September 2011</td><td align="left">
Lawler et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110268369&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0268369</a></td><td align="left">
November 2011</td><td align="left">
Richards et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20120054205&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2012/0054205</a></td><td align="left">
March 2012</td><td align="left">
Baluja et al.</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
       <center><b>Foreign Patent Documents</b></center>
<table width="100%"> <tbody><tr><td></td><th scope="col"></th> <td></td><th scope="col"></th> <td></td><th scope="col"></th></tr> <tr> <td align="left">
</td><td align="left">1 544 729</td><td></td><td align="left">
Jun 2005</td><td></td><td align="left">
EP</td></tr><tr><td align="left">
</td><td align="left">11-265369</td><td></td><td align="left">
Sep 1999</td><td></td><td align="left">
JP</td></tr><tr><td align="left">
</td><td align="left">2002-132604</td><td></td><td align="left">
May 2002</td><td></td><td align="left">
JP</td></tr><tr><td align="left">
</td><td align="left">WO 97/21183</td><td></td><td align="left">
Jun 1997</td><td></td><td align="left">
WO</td></tr><tr><td align="left">
</td><td align="left">WO 00/68860</td><td></td><td align="left">
Nov 2000</td><td></td><td align="left">
WO</td></tr><tr><td align="left">
</td><td align="left">WO 2004/111771</td><td></td><td align="left">
Dec 2004</td><td></td><td align="left">
WO</td></tr><tr><td align="left">
</td><td align="left">WO 2006/121575</td><td></td><td align="left">
Nov 2006</td><td></td><td align="left">
WO</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
<table width="90%">   <tbody><tr><td><align="left"><br/>Accounts, at http://www.cs.rice.edu/.about.ssiyer/accounts/, as available via the Internet and printed on Jul. 29, 2004. cited by applicant
.<br/>Adamic et al., "A Social Network Caught in the Web," at http://firstmonday.org/issues/issue8.sub.--6/adamic/, as available via the Internet and printed on Jul. 28, 2004. cited by applicant
.<br/>AdForce, Inc., A Complete Guide to AdForce, Version 2.6, 1998. cited by applicant
.<br/>U.S. Appl. No. 95/001,061, Reexam of Stone. cited by applicant
.<br/>U.S. Appl. No. 95/001,068, Reexam of Stone. cited by applicant
.<br/>U.S. Appl. No. 95/001,069, Reexam of Stone. cited by applicant
.<br/>U.S. Appl. No. 95/001,073, Reexam of Stone. cited by applicant
.<br/>AdForce, Inc., S-1/A SEC Filing, May 6, 1999. cited by applicant
.<br/>AdKnowledge Campaign Manager: Reviewer's Guide, AdKnowledge, Aug. 1998. cited by applicant
.<br/>AdKnowledge Market Match Planner: Reviewer's Guide, AdKnowledge, May 1998. cited by applicant
.<br/>AdStar.com website archive from www.Archive.org, Apr. 12, 1997, and Feb. 1, 1997. cited by applicant
.<br/>Amazon.com, "Selling at Amazon Marketplace," at http://pages.amazon.com/exec/obidos/tg/browse/-/1161234/ref=hp.sub.--hp.s- ub.--is.sub.--4.sub.--2/002-283572 as available via the Internet and printed on Jul. 29, 2004. cited by applicant
.<br/>Amazon.com, "New Seller FAQ," at http://pages.amazon.com/exec/obidos/tg/browse/-/1161274/002-2835726-55136- 22 as available via the Internet and printed on Jul. 29, 2004. cited by applicant
.<br/>Azran, "The Rendezvous Algorithm: Multiclass Semi-Supervised Learning with Markov Random Walks," ICML, 2007, 8 pages. cited by applicant
.<br/>Baluja and Rowley, Intl J Computer Vision, 2007, 71(1): at http://portal.acm.org/toc.cfm?id=J325&amp;type=periodical&amp;coll=&amp;dl=ACM&amp;CFID=1- 5151515&amp;CFTOKEN=6184618 (617001 invention disclosure). cited by applicant
.<br/>Baluja et al., "Video Suggestion and Discovery for YouTube: Taking Random Walks Through the View Graph," Proc. 17.sup.th International World Wide Web Conference (WWW), 2008. cited by applicant
.<br/>Baseview Products, Inc., AdManager Pro Administrator's Manual v. 2.0, Jun. 1998. cited by applicant
.<br/>Baseview Products, Inc., ClassManagerPro Administration and Receivables Manual v. 1.0.5, Feb. 1, 1997. cited by applicant
.<br/>Bay et al., "SURF: Speeded up robust features," Proc. 9.sup.th International European Conference on Computer Vision (ECCV), pp. 404-417, 2006. cited by applicant
.<br/>Belongie et al., "Shape matching and object recognition using shape contexts," IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 24, No. 24, pp. 509-522, 2002. cited by applicant
.<br/>Boccaletti et al., "Complex Networks: Structure and Dynamics," Physics Reports, 2006, 424:175-308. cited by applicant
.<br/>Brin and Page, "The Anatomy of a Large-Scale Hypertextual Web Search Engine," Computer Networks, 1998, pp. 1-26. cited by applicant
.<br/>Business Wire, "Global Network, Inc. Enters Into agreement in Principle with Major Advertising Agency," Oct. 4, 1999. cited by applicant
.<br/>Carson et al., "Blobworld: Image Segmentation Using Expectation-Maximization and Its Application to Image Querying," IEEE transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2002, 24(8):1026-1038. cited by applicant
.<br/>Chakrabarti et al., "Mining the Web's Link Structure," IEEE Computer Magazine, 1999, pp. 60-67. cited by applicant
.<br/>Dalal and Triggs, "Histograms of Oriented Gradients for Human Detection," Proc IEEE Conf Computer Vision and Pattern Recognition, Jun. 2005, pp. 886-893 (found in 474001 invention disclosure). cited by applicant
.<br/>Datar et al., "Locality-sensitive hashing scheme based on p-stable distributions," Proc 20.sup.th Symposium on Computational Geometry (SCG), pp. 253-262, 2004. cited by applicant
.<br/>Datta et al., "Image retrieval: Ideas, influences, and trends of the new age," ACM Computing Surveys, vol. 40, No. 2, Articale 5, Publication date: Apr. 2008. cited by applicant
.<br/>Dedrick, Interactive Electronic Advertising, IEEE, 1994, pp. 55-66. cited by applicant
.<br/>Dedrick, A Consumption Model for Targeted Electronic Advertising, Intel Architecture Labs, IEEE, 1995, pp. 41-49. cited by applicant
.<br/>Dellaert et al., "Mixture Trees for Modeling and Fast Conditional Sampling with Applications in Vision and Graphics," Proc IEEE Conf Computer Vision and Pattern Recognition, 2005 at http://www.cs.unc.edu/.about.kwatra/publications/cvpr05-mixtree.pdf
(1084 invention disclosure). cited by applicant
.<br/>Doctorow, "Running Notes from Revenge of the User: Lessons from Creator/User Battles," at http://craphound.com/danahetcon04.txt, as available via the Internet and printed Jun. 28, 2004. cited by applicant
.<br/>Ebay.com, "What is eBay?" at http://pages.ebay.com/help/welcome/questions/about-ebay.html as available via the Internet and printed on Jul. 29, 2004. cited by applicant
.<br/>Ebay.com, "How to Bid," at http://pages.ebay.com/help/welcome/bid.html as available via the Internet and printed on Jul. 29, 2004. cited by applicant
.<br/>Ebay.com, "How to Sell," at http://pages.ebay.com/help/welcome/sell.html as available via the Internet and printed on Jul. 29, 2004. cited by applicant
.<br/><a href="#h0" name="h1"></a><a href="#h2"></a><b><i>Facebook</i></b> Press Release "Leading Websites Offer <a href="#h1" name="h2"></a><a href="#h3"></a><b><i>Facebook</i></b> Beacon for Social Distribution, Users Gain Ability to Share their Action from 44 Participating Sites with their Friends on <a href="#h2" name="h3"></a><a href="#h4"></a><b><i>Facebook</i></b>" [online] [retrieved on Jan. 24, 2008] [retrieved from the
internet: http://www.facebook.com/press/releases.php?p9166] 1 page. (Same ref as "Press Releases <a href="#h3" name="h4"></a><a href="#h5"></a><b><i>Facebook</i></b>". cited by applicant
.<br/>Jessica Zhang , `<a href="#h4" name="h5"></a><a href="#h6"></a><b><i>Facebook</i></b> unveils school-specific advertisements` [online]. The Stanford Daily, 2005, [retrieved on Aug. 16, 2010]. Retrieved from the Internet: http://www.stanforddaily.com/2005/01/06<a href="#h5" name="h6"></a><a href="#h7"></a><b><i>/facebook</i></b>-unveils-school-specific--
advertisements/, (Jan. 6, 2005) 1 page. cited by applicant
.<br/>Fergus et al., "A visual category filter for Google images," Proc. 8.sup.th European Conference on Computer Vision (ECCV) , 2004, pp. 242-256. cited by applicant
.<br/>Fergus et al., "Object class recognition by unsupervised scale-invariant learning," Proc. Conference on Computer Vision and Pattern Recognition (CVPR), 2003, 2:264-271. cited by applicant
.<br/>Frey and Dueck, "Clustering by Passing Messages Between Data Points," Science, 2007, 315:972-976. cited by applicant
.<br/>Friedman et al., "Bayesian network classifiers," Machine Learning, 1997, 29:131-163. cited by applicant
.<br/>Frome et al., "Learning globally-consistent local distance functions for shape-based image retrieval and classification," Proc. 11.sup.th IEEE International Conference on Computer Vision (ICCV), pp. 1-8, 2007. cited by applicant
.<br/>Gibson et al., "Inferring Web Communities from Link Topology," Proc 9.sup.th ACM Conference on Hypertex and Hypermedia, 1998, 10 pages (found in 473001 invention disclosure). cited by applicant
.<br/>Gionis et al., "Similarity Search in High Dimensions via Hashing," Proc 25.sup.th Very Large Database Conf, 1999 at people.csail.mit.edu/indyk/vldb99.ps (1084 invention disclosure). cited by applicant
.<br/>Glance et al., "Knowledge Pump: Supporting the Flow and Use of Knowledge," Information Technology for Knowledge Management, 1997, Borghoff and Pareschi (eds.), Springer Verlag, 22 pages. cited by applicant
.<br/>Grauman and Darrell, "The Pyramid Match Kernel: Discriminative Classification with Sets of Image Features," ICCV 2005 at http://people.csail.mit.edu/kgrauman/jobapp/kgrauman.sub.--sample.sub.--p- apers.pdf (found in 474001 invention disclosure).
cited by applicant
.<br/>Harris and Stephens, "A combined corner and edge detector," Proc. 4.sup.th Alvey Vision Conference, pp. 147-151, 1988. cited by applicant
.<br/>Haveliwala, "Topic-Sensitive PageRank," IEEE Transactions on Knowledge and Data Engineering, 2003, 10 pages. cited by applicant
.<br/>He et al., "Imagerank: spectral techniques for structural analysis of image database," Proc. International Conference on Multimedia and Expo, 2002, 1:25-28. cited by applicant
.<br/>Herlocker et al., "Evaluating Collaborative Filtering Recommender Systems," ACM Transactions on Information Systems, 2004, 22(1):5-53. cited by applicant
.<br/>Hsu et al., "Video search reranking through random walk over document-level context graph," Proc. 15.sup.th International Conference on Multimedia, 2007 pp. 971-980. cited by applicant
.<br/>Indyk and Motwani, "Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality," Proc. 30.sup.th ACM Symp. On Computational Theory, 1998, pp. 604-613. cited by applicant
.<br/>Indyk, "Stable Distributions, Pseudorandom Generators, Embeddings, and Data Stream Computation," Proc. 41.sup.st IEEE Symposium on Foundations of Computer Science (FOCS), pp. 189-197, 2000. cited by applicant
.<br/>Information Access Technologies, Inc., Aaddzz brochure, "The best Way to Buy and Sell Web Advertising Space," 1997. cited by applicant
.<br/>Information Access Technologies, Inc., Aaddzz.com website archive from www.Archive.org, archived on Jan. 30, 1998. cited by applicant
.<br/>Jeh and Widom, "SimRank: A Measure of Structural-Context Similarity," Proc 8.sup.th ACM SIGKDD international conference on knowledge discovery and data mining, Jul. 2002, pp. 538-543 (found in 473001 invention disclosure). cited by applicant
.<br/>Jing et al., "Canonical image selection from the web," Proc. 6.sup.th International Conference on Image and Video Retrieval (CIVR), pp. 280-287, 2007. cited by applicant
.<br/>Jing and Baluja, "VisualRank: Applying PageRank to Large-Scale Image Search," IEEE Transaction on Pattern Analysis and Machine Intelligence, 2008, 30(11):1877-1890. cited by applicant
.<br/>Joachims, "Text Categorization with Support Vector Machines: Learning with Many Relevant Features," Proc 10.sup.th European Conf on Machine Learning, 1998, pp. 137-142 (533 invention disclosure). cited by applicant
.<br/>Joshi et al., "The story picturing engine--a system for automatic text illustration," ACM Transactions on Multimedia, Computing, Communications and Applications, 2006, 2(1):68-89. cited by applicant
.<br/>Kautz et al., "ReferralWeb: Combining Social Networks and Collaborative Filtering," Communications of the ACM, 1997, 40(3):1-4. cited by applicant
.<br/>Ke et al., "Efficient near-duplicate detection and sub-image retrieval," Proc. ACM International Conference on Multimedia (ACM MM), pp. 869-876, 2004. cited by applicant
.<br/>Ke and Sukthankar, "PCA-SIFT: A More Distinctive Representation for Local Image Descriptors," Proc. Conference on Computer Vision and Pattern Recognition (DVPR), 2004, 2:506-516. cited by applicant
.<br/>Kleinberg et al., "The Web as a graph: measurements, models, and methods," Proc International Conference on Combinatronics, 1999, 18 pages. cited by applicant
.<br/>Kleinberg, "Authoritative Sources in a Hyperlinked Environment," Journal of the ACM, 1999, 46(5):604-632. cited by applicant
.<br/>Kondor and Lafferty, "Diffusion kernels on graphs and other discrete structures," Proc. 19.sup.th International Conference on Machine Learning (ICML), pp. 315-322, 2002. cited by applicant
.<br/>Konstan et al., "GroupLens: Applying Collaborative Filtering to Usenet News," Communications of the ACM, Mar. 1997, 40(3):77-87. cited by applicant
.<br/>Lazebnik et al., "A sparse texture representation using affine-invariant regions," Proc. Conference on Computer Vision and Pattern Recognition (CVPR), vol. 2, pp. 319-324, 2003. cited by applicant
.<br/>Leigh et al., "Transformation, Ranking, and Clustering for Face Recognition Algorithm Comparison," at http://www.itl.nist.gov/div898/itperf/renorm.pdf (617001 invention disclosure). cited by applicant
.<br/>Liew et al., "Social Networks," U.S. Appl. No. 60/552,718, filed Mar. 15, 2004, 9 pages. cited by applicant
.<br/>Liu et al., "A Comparative Study on Feature selection and Classification methods Using Gene Expression Profiles and Proteomic Patterns," Genome Informatics 13:, pp. 51-60, 2002. cited by applicant
.<br/>Liu et al., "An Investigation of Practical Approximate Nearest Neighbor Algorithms," Proc Neural Information Processing Systems, 2004 at http://www.cs.cmu.edu/.about.tingliu/research.htm (1084 invention disclosure). cited by applicant
.<br/>Lowe, "Distinctive Image Features from Scale-Invariant Keypoints," International Journal of Computer Vision (IJCV), 2004, 60(2):91-110. cited by applicant
.<br/>Lowe, "Local Feature View Clustering for 3D Object Recognition," CVPR 2001, at http:/www.cs.ubc.ca/.about.lowe/papers/cvpr01.pdf (found in 473001 and 474001 invention disclosures). cited by applicant
.<br/>Ma and Manjunath, "NeTra: AToolbox for Navigating Large Image Databases," Multimedia System, 1999, 3(7):184-198. cited by applicant
.<br/>Microsoft Corporation, "Is Friendster the `Next Big Thing`?" at http://mobilemomentum.msn.com/article.aspx?aid=4, as available via the Internet and printed on Jul. 29, 2004. cited by applicant
.<br/>Mikolajczyk and Schmid, "A performance evaluation of local descriptors," IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2005, 27(10):1615-1630. cited by applicant
.<br/>Monay and Gatica-Perez, "On Image Auto-Annotation with Latent Space Models," MM'03, Nov. 2-8, 2003. cited by applicant
.<br/>Multiply, "About Multiply," at http://multiply.com/info/about, as available via the Internet and printed on May 3, 2004. cited by applicant
.<br/>Multiply, "Help," at http://multiply.com/info/help, as available via the Internet and printed on May 3, 2004. cited by applicant
.<br/>Multiply, "Multiply Privacy Policy," at http://multiply.com/info/privacy, as available via the Internet and printed on May 3, 2004. cited by applicant
.<br/>Multiply, "Multiply Terms of Service," at http://multiply.com/info/tos, as available via the Internet and printed on May 3, 2004. cited by applicant
.<br/>Nister and Stewenius, "Scalable recognition with a vocabulary tree," Proc. Conference on Computer vision and Pattern Recognition (CVPR), vol. 2, pp. 2161-2168, 2006. cited by applicant
.<br/>Nowak and Jurie, "Learning visual similarity measures for comparing never seen objects," Proc. Conference on Computer Vision and Pattern Recognition (CVPR), 2007. cited by applicant
.<br/>Park et al., "Majority based ranking approach in web image retrieval," Lecture Notes in Computer Science, vol. 27-28, pp. 499-504, 2003. cited by applicant
.<br/>Pentland et al., "Photobook: Content-based manipulation of image databases," International Journal of Computer Vision (IJCV), 1996, 18(3):233-254. cited by applicant
.<br/>Philbin et al., "Object retrieval with large vocabularies and fast spatial matching," Proc. Conference on Computer Vision and Pattern Recognition (CVPR), 2007. cited by applicant
.<br/>Pilaszy, "Text Categorization and Support Vector Machines," Computer Science, 1998, vol. 1398, 10 pages. cited by applicant
.<br/>Press Releases <a href="#h6" name="h7"></a><a href="#h8"></a><b><i>Facebook</i></b> "Leading Websites Offer <a href="#h7" name="h8"></a><a href="#h9"></a><b><i>Facebook</i></b> Beacon for Social Distribution," [online] [retrieved from the internet: http://www.facebook.com/press/releases.php?p9166] [retrieved on Jan. 24, 2008], 1 page (same ref as "<a href="#h8" name="h9"></a><a href="#h10"></a><b><i>Facebook</i></b> Press
Release") Cite "<a href="#h9" name="h10"></a><a href="#h11"></a><b><i>Facebook</i></b> Press Release". cited by applicant
.<br/>Roach et al., "Video Genre Classification Using Dynamics," 2001, Proc Acoustics, Speech, and Signal Processing on IEEE Intl Conference, pp. 1557-1560 (955001 invention disclosure). cited by applicant
.<br/>Rothganger et al., "3D Object Modeling and Recognition Using Affine-Invariant Patches and Multi-View Spatial Constraints," CVPR 2003, at http://vasc.ri.cmu.edu/.about.hebert/04AP/fred.sub.--cvpr03.pdf (found in 473001 and 474001 invention
disclosures). cited by applicant
.<br/>Rowley et al., "Neural Network-Based Face Detection," IEEE Transactions on Pattern Analysis and Machine Intelligence, 1998, 20(1): (617001 invention disclosure). cited by applicant
.<br/>Schindler et al., "City-Scale Location Recognition," Proc. Conference on Computer Vision and Pattern Recognition (CVPR), 2007, 7 pages. cited by applicant
.<br/>Sebastiani, "Machine Learning in Automated Text Categorization," ACM Computing Surveys, 2002, 34(1):1-47. cited by applicant
.<br/>Simon et al., "Scene summarization for online image collections," Proc. 12.sup.th International Conference on Computer Vision (ICCV), 2007. cited by applicant
.<br/>Smeulders et al., "Content based image retrieval at the end of the early years," IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2000, 22(12):1349-1380. cited by applicant
.<br/>Sullivan, Danny, "Is It Really Personalized Search?" http://searchenginewatch.com printed on May 13, 2004. cited by applicant
.<br/>Szummer and Jaakkola, "Partially labeled classification with Markov random walks," NIPS 2001 (473001 invention disclosure). cited by applicant
.<br/>Tribe.net, "Listings Directory," at http://www.tribe.net/tribe/servlet/template/pub.Listings.vm, as available via the Internet and printed on Jun. 28, 2004. cited by applicant
.<br/>Uchihashi and Kanade, "Content-free image retrieval by combinations of keywords and user feedbacks," Proc. 5.sup.th International Conference on Image and Video Retrieval (CIVR), pp. 650-659, 2005. cited by applicant
.<br/>Viola and Jones, "Robust Real Time Face Detection," Int J Computer Vision, 2004 at http://scholar.google.com/scholar?h1=en&amp;lr=&amp;cluster=1009836281419- 2689387 (617001 invention disclosure). cited by applicant
.<br/>Wang et al., "AnnoSearch: Image Auto-Annotation by Search," CVPR 2006 (in 473001 invention disclosure). cited by applicant
.<br/>Weinberger et al., "Distance metric learning for large margin nearest neighbor classification," Proc. 18.sup.th Conference on Advances in Neural Information Processing Systems (NIPS), vol. 18, pp. 1437-1480, 2006. cited by applicant
.<br/>Winder and Brown, "Learning local image descriptors," Prof. Conference on Computer Vision and Pattern Recognition (CVPR), 2007. cited by applicant
.<br/>Xing et al., "Distance metric learning, with applications to clustering with side-information," Proc. 15.sup.th Conference on Advances in Neural Information Processing Systems (NIPS), 2002, 15:450-459. cited by applicant
.<br/>Yang et al., "Mining Social Networks for Targeted Advertising," Proceedings of the 39.sup.th Hawaii International Conference on System Sciences, 2006. cited by applicant
.<br/>Yang and Pedersen, "A Comparative Study on Feature Selection in Text Categorization Source," Proc 14.sup.th Intl Conf Machine Learning, 1997, pp. 412-420. cited by applicant
.<br/>Zeff et al., Advertising on the Internet, 2.sup.nd ed., John Wiley &amp; Sons, 1999. cited by applicant
.<br/>Zhou and Scholkopf, "Learning from Labeled and Unlabeled Data Using Random Walks," Lecture notes in computer science, 2004, Springer, 8 pages. cited by applicant
.<br/>Zhu et al., "Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions," Proc. 20.sup.th Intl Conf on Machine Learning, 2003, 8 pages. cited by applicant
.<br/>Zhu, "Semi-Supervised Learning with Graphs," 2005, Doctoral Thesis, Carnegie Mellon University, 174 pages. cited by applicant
.<br/>International Search Report/Written Opinion in PCT/US2008/062263 mailed Jan. 1, 2008, 13 pages. cited by applicant
.<br/>International Preliminary Report on Patentability in PCT/US2008/062263 mailed Nov. 12, 2009, 7 pages. cited by applicant
.<br/>International Search Report/Written Opinion in PCT/US2008/62285 mailed Dec. 5, 2008, 12 pages. cited by applicant
.<br/>International Preliminary Report on Patentability in PCT/US2008/62285 mailed Nov. 12, 2009, 8 pages. cited by applicant. </align="left"></td></tr> </tbody></table><br/><center><b>Other References</b></center> <br/>
       <i>Primary Examiner:</i> Chawan; Sheela C
<br/>
       <i>Attorney, Agent or Firm:</i> <coma>Fish &amp; Richardson P.C.
<br/>
       <hr/>
       <center><b><i>Parent Case Text</i></b></center>
       <hr/>
       <br/><br/>CROSS-REFERENCE TO RELATED APPLICATION
<br/><br/> This application is a continuation application of and claims priority to
     U.S. application Ser. No. 13/274,778, filed on Oct. 17, 2011, which
     claims priority to U.S. application Ser. No. 11/934,547, filed on Nov. 2,
     2007, the entire contents of which are hereby incorporated by reference.
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>What is claimed is: <br/><br/> 1.  A computer-implemented method, comprising: receiving, by a computing system, images that display a particular visible physical feature for multiple people displayed in
the images;  determining, by the computing system, similarities in appearance among the particular visible physical features for at least some of the people displayed in the images;  for each person in at least a subset of the people displayed in the
images: identifying one or more people displayed in the images whose particular visible physical feature is determined to be similar to the particular visible physical feature of the person, identifying a respective classification for the particular
visible physical feature of each of the one or more people, and determining a classification for the particular visible physical feature of the person based on the respective classifications for the particular visible physical features of the one or more
people whose particular visible physical feature is determined to be similar to the particular visible physical feature of the person;  and outputting, by the computing system, information that characterizes the classification of the particular visible
physical feature determined for at least one of the people displayed in the images.
<br/><br/> 2.  The method of claim 1, further comprising generating a data structure that includes representations of the particular visible physical features of at least a second subset of the people displayed in the images and that specifies links among
at least some of the representations of the particular visible physical features.
<br/><br/> 3.  The method of claim 2, wherein the data structure comprises a plurality of nodes and edges, each node in the data structure corresponding to a particular representation of the particular visible physical feature of a person displayed in the
images, and each edge in the data structure linking a pair of nodes among the plurality of nodes and indicating a respective similarity score for the representations of the particular visible physical feature corresponding to the pair of nodes.
<br/><br/> 4.  The method of claim 1, wherein the particular visible physical feature comprises a face.
<br/><br/> 5.  The method of claim 1, wherein, for each person in the at least subset of people displayed in the images, determining the classification for the particular visible physical feature of the person comprises determining a likelihood that the
person is male or female based on the respective classifications for the particular physical features of the one or more people indicating likelihoods that each of the one or more people are male or female, respectively.
<br/><br/> 6.  The method of claim 1, further comprising identifying the one or more people whose particular visible physical feature is determined to be similar to the particular visible physical feature of the person by performing a random walk algorithm
on a data structure that links representations of the particular visible physical feature of at least some of the people displayed in the images.
<br/><br/> 7.  The method of claim 6, further comprising performing multiple iterations of the random walk algorithm on the data structure by performing the random walk algorithm a predetermined number of times or until respective classification values for
at least some representations of the particular visible physical feature from among the multiple representations of the particular visible physical feature linked in the data structure converge such that the respective classification values change less
than a threshold amount over corresponding classification values determined from a previous iteration of the random walk algorithm.
<br/><br/> 8.  The method of claim 1, wherein the classification for the particular visible physical feature of at least one of the one or more people is seeded from user input.
<br/><br/> 9.  The method of claim 1, wherein the classification for the particular visible physical feature of at least one person from among the at least subset of the people displayed in the images is determined further based on respective
classifications for the particular visible physical feature of one or more additional people whose particular visible physical feature is determined to be similar to the particular visible physical feature of at least one of the one or more people.
<br/><br/> 10.  The method of claim 1, wherein determining the classification for the particular visible physical feature of the person comprises performing an adsorption algorithm.
<br/><br/> 11.  A computer program product tangibly embodied in one or more non-transitory computer-readable storage devices that, when executed by one or more processors, causes performance of operations comprising: receiving, by a computing system,
images that display a particular visible physical feature for multiple people displayed in the images;  determining, by the computing system, similarities in appearance among the particular visible physical features for at least some of the people
displayed in the images;  for each person in at least a subset of the people displayed in the images: identifying one or more people displayed in the images whose particular visible physical feature is determined to be similar to the particular visible
physical feature of the person, identifying a respective classification for the particular visible physical feature of each of the one or more people, and determining a classification for the particular visible physical feature of the person based on the
respective classifications for the particular visible physical features of the one or more people whose particular visible physical feature is determined to be similar to the particular visible physical feature of the person;  and outputting, by the
computing system, information that characterizes the classification of the particular visible physical feature determined for at least one of the people displayed in the images.
<br/><br/> 12.  The computer program product of claim 11, wherein the operations further comprise generating a data structure that includes representations of the particular visible physical features of at least a second subset of the people displayed in
the images and that specifies links among at least some of the representations of the particular visible physical features.
<br/><br/> 13.  The computer program product of claim 12, wherein the data structure comprises a plurality of nodes and edges, each node in the data structure corresponding to a particular representation of the particular visible physical feature of a
person displayed in the images, and each edge in the data structure linking a pair of nodes among the plurality of nodes and indicating a respective similarity score for the representations of the particular visible physical feature for the pair of
nodes.
<br/><br/> 14.  The computer program product of claim 11, wherein the particular visible physical feature comprises a face.
<br/><br/> 15.  The computer program product of claim 11, wherein, for each person in the at least subset of people displayed in the images, determining the classification for the particular visible physical feature of the person comprises determining a
likelihood that the person is male or female based on the respective classifications for the particular physical features of the one or more people indicating likelihoods that each of the one or more people are male or female, respectively.
<br/><br/> 16.  The computer program product of claim 11, wherein the operations further comprise identifying the one or more people whose particular visible physical feature is determined to be similar to the particular visible physical feature of the
person by performing a random walk algorithm on a data structure that links multiple representations of the particular visible physical feature of at least some of the people displayed in the images.
<br/><br/> 17.  The computer program product of claim 16, wherein the operations further comprise performing multiple iterations of the random walk algorithm on the data structure by performing the random walk algorithm a predetermined number of times or
until respective classification values for at least some representations of the particular visible physical feature from among the multiple representations of the particular visible physical feature linked in the data structure converge such that the
respective classification values change less than a threshold amount over corresponding classification values determined from a previous iteration of the random walk algorithm.
<br/><br/> 18.  The computer program product of claim 11, wherein the classification for the particular visible physical feature of at least one of the one or more people is seeded from user input.
<br/><br/> 19.  The computer program product of claim 11, wherein the classification for the particular visible physical feature of at least one person from among the at least subset of the people displayed in the images is determined further based on
respective classifications for the particular visible physical feature of one or more additional people whose particular visible physical feature is determined to be similar to the particular visible physical feature of at least one of the one or more
people.
<br/><br/> 20.  A system comprising: one or more computers configured to provide: an image repository to store a plurality of images;  a visible physical feature detector to identify respective visible physical features among people displayed in the
plurality of images;  a similarity calculator to determine similarities in appearance among representations of the visible physical features of the people displayed in the images;  a graph generator to link the representations of the visible physical
features based on the determined similarities in appearance among the representations of the visible physical features;  and a classification generator to determine classification values for particular ones of the representations of the visible physical
features based on the determined similarities in appearance among the particular representations of the visible physical features and other ones of the representations of the visible physical features that are linked to the particular representations,
wherein the classification values indicate a likelihood that respective ones of the people displayed in the plurality of images corresponding to the representations of the visible physical features have a particular visually discernible characteristic.
<hr/> <center><b><i>Description</i></b></center> <hr/> <br/><br/>TECHNICAL FIELD
<br/><br/> This instant specification relates to inferring the gender of a face in an image.
<br/><br/>BACKGROUND
<br/><br/> In many application domains, such as photo collection, social networking, surveillance, adult-content detection, etc, it is desirable to use an automated method to determine the gender of a person in an image.  Some systems use computer vision
algorithms in an attempt to determine gender by directly analyzing a face within an image to determine if the face is associated with typically male or female characteristics.
<br/><br/>SUMMARY
<br/><br/> In general, this document describes inferring the gender of a person or animal from a set of images.  The faces are represented in a data structure and includes links between faces having similarities.
<br/><br/> In a first aspect, a computer-implemented method includes receiving a plurality of images having human faces.  The method further includes generating a data structure having representations of the faces and associations that link the
representations based on similarities in appearance between the faces.  The method further includes outputting a first gender value for a first representation of a first face that indicates a gender of the first face based on one or more other gender
values of one or more other representations of one or more other faces that are linked to the first representation.
<br/><br/> In a second aspect, a computer-implemented method includes receiving a plurality of images having animal faces.  The method further includes generating a graph having nodes representing the faces and edges that link the nodes based on
similarities in appearance between the faces represented by the nodes.  The method further includes outputting a first gender value for a first node that indicates a gender of a first face associated with the first node based on one or more second gender
values of one or more neighboring nodes in the graph.
<br/><br/> In a third aspect, a computer-implemented method includes receiving a plurality of images having animal faces.  The method further includes generating a data structure that associates a first face with one or more second faces based on
similarities in appearance between the first face and the one or more second faces.  The method further includes outputting a first gender value for the first face based on one or more gender values previously associated with the one or more second
faces.
<br/><br/> In a fourth aspect, a system includes a face detector for identifying faces within images.  The system further includes means for generating a data structure configured to associate a first face with one or more second faces based on
similarities in appearance between the first face and the one or more second faces.  The system further includes an interface to output a first gender value for the first face based on one or more gender values previously associated with the one or more
second faces.
<br/><br/> The systems and techniques described here may provide one or more of the following advantages.  First, an automated method is provided for identifying the gender of a person (or persons) in a collection of images.  Second, a gender detection
method is provided that does not require large data sets for training a statistical gender classifier.  Third, a method for gender detection is provided that does not rely on computer vision techniques to directly identify a gender of a face based on an
image of the face.  Fourth, genders associated with androgynous faces within images can be more accurately determined.
<br/><br/> The details of one or more embodiments are set forth in the accompanying drawings and the description below.  Other features and advantages will be apparent from the description and drawings, and from the claims. <br/><br/>DESCRIPTION OF DRAWINGS
<br/><br/> FIG. 1 is a schematic diagram showing an example of a system for inferring the gender of a face in an image.
<br/><br/> FIG. 2 is a block diagram showing another example of a system for inferring the gender of a face in an image.
<br/><br/> FIG. 3A shows examples of images used for inferring a gender of a face in an image.
<br/><br/> FIG. 3B shows examples of comparisons and similarities between faces in images.
<br/><br/> FIG. 3C shows an example of a weighted undirected graph of the similarities between the faces in the images.
<br/><br/> FIG. 3D shows an example of manually identified gender values for a portion of the weighted undirected graph.
<br/><br/> FIG. 4 is a flow chart showing an example of a method for constructing a weighted undirected graph of similarities between faces in images.
<br/><br/> FIG. 5 is a flow chart showing an example of a method for inferring the gender of a face in an image using a weighted undirected graph of similarities between faces in images.
<br/><br/> FIGS. 6A-D show examples of gender values in a weighted undirected graph before performing a gender inferring process and after one iteration, two iterations, and four iterations of the gender inferring process, respectively.
<br/><br/> FIG. 7A shows an example of a table that stores gender values for faces.
<br/><br/> FIG. 7B shows an example of a table that stores similarity scores between a first face and other faces.
<br/><br/> FIG. 8 shows an example of a weighted undirected graph including dummy labels.
<br/><br/> FIG. 9 is a schematic diagram showing an example of a generic computing system that can be used in connection with computer-implemented methods described in this document.
<br/><br/> Like reference symbols in the various drawings indicate like elements.
<br/><br/>DETAILED DESCRIPTION
<br/><br/> This document describes systems and techniques for inferring the gender of a face in an image.  A network, such as the Internet or a wide area network, can include servers hosting images of people and/or other animals.  In general, an image of a
female face may have similarities with images of other female faces and an image of a male face may have similarities with images of other male faces.  In certain implementations, a gender determination system analyzes the similarities between the faces,
including faces having a known gender and faces having an unknown gender.  The gender determination system can use the similarities and the known genders to infer a gender of one or more faces having an unknown gender.
<br/><br/> In certain implementations, a face having an unknown gender may have little or no direct similarities with the faces having a known gender.  The gender determination system may determine that the face having the unknown gender has similarities
with one or more other unknown gender faces.  The gender determination system may also determine that one or more of the other unknown gender faces have similarities with one or more faces having known genders.  The gender determination system can use
the chain of intermediate faces to infer the gender of the face having an unknown gender by indirectly relating the unknown gender face to a face having a known gender.
<br/><br/> FIG. 1 is a schematic diagram showing an example of a system 100 for inferring the gender of a face in an image.  The system 100 includes a gender determination system 102 and multiple publisher systems 104a-c, such as a web logging (blogging)
server, an image hosting server, and a company web server, respectively.  The gender determination system 102 communicates with the publisher systems 104a-c using one or more networks, such as the Internet, and one or more network protocols, such as
Hypertext Transport Protocol (HTTP).
<br/><br/> In the case of HTTP communication, the publisher systems 104a-c can include multiple web pages 106a-c, respectively.  For example, the web page 106a published by the blogging system is a celebrity gossip blog, the web page 106b published by the
image hosting system is an image gallery, and the web page 106c published by the web server system is a homepage for the ACME Corporation.  The web pages 106a-c each include one or more images 108a-e. The images 108a-e include faces of people and/or
other animals.  The gender determination system 102 retrieves the images 108a-e from the publisher systems 104a-c and stores the images 108a-e in a data storage 110.
<br/><br/> The gender determination system 102 includes an inferred gender label generator 112 that performs processing to infer the gender of the faces in the images.  The inferred gender label generator 112 includes a face detector 114 and a similarity
calculator 116.  The face detector 114 uses face detection algorithms known to those skilled in the art to detect faces in the images 108a-e. In certain implementations, the face detector 114 stores the portions of the images 108a-e that include faces in
the data storage 110.  Alternatively, the face detector 114 may store information that describes the location of the face portions within the images 108a-e. The face detector 114 passes the face information to the similarity calculator 116 or otherwise
notifies the similarity calculator 116 of the face information.
<br/><br/> The similarity calculator 116 calculates similarity scores between pairs of faces.  The similarity calculator 116 uses facial analysis algorithms known to those skilled in the art.  For example, one such facial analysis algorithm is discussed in
a paper entitled "Transformation, Ranking, and Clustering for Face Recognition Algorithm Comparison," by Stefan Leigh, Jonathan Phillips, Patrick Grother, Alan Heckert, Andrew Ruhkin, Elaine Newton, Mariama Moody, Kimball Kniskern, Susan Heath, published
in association with the Third Workshop on Automatic Identification Advanced Technologies, Tarrytown, March 2002.
<br/><br/> The inferred gender label generator 112 includes or has access to predetermined gender information for one or more of the faces.  For example, a user may make an input indicating that a face in the image 108a has a gender of female and a face in
the image 108b has a gender of male.  Alternatively, the inferred gender label generator 112 may generate predetermined gender information based on an algorithm, such as associating a distinguishing feature within a face as male or female.  The inferred
gender label generator 112 uses the predetermined gender information and the similarity scores between the faces to link each of the faces directly or indirectly to the faces having predetermined or known genders.  The inferred gender label generator 112
uses the links to infer a gender for the faces.  The inferred gender label generator 112 stores the similarity scores and inferred genders in the data storage 110 as a gender label value table 118.
<br/><br/> The gender determination system 102 can output genders of the faces as gender label information 120a-e for the images 108a-e, respectively, to the publisher systems 104a-c. In another implementation, the gender determination system 102 can
include, or provide gender information to, an image search engine.  For example, a user may make an input indicating a search for images of people or animals having a particular gender.  The gender determination system 102, or another image search
system, uses the gender label information 120a-e to provide images having the requested gender.
<br/><br/> FIG. 2 is a block diagram showing an example of a system 200 for inferring the gender of a face in an image.  The system 200 includes the gender determination system 102 as previously described.  The inferred gender label generator 112 receives
the images 108a-e and may store the images 108a-e in the data storage 110.  The inferred gender label generator 112 uses the face detector 114 to locate faces within the images 108a-e. Subsequently or concurrently, the inferred gender label generator 112
uses the similarity calculator 116 to calculate similarity scores between faces.
<br/><br/> In the system 200, the similarity calculator 116 is included in a graph generator 202.  The graph generator 202 generates a graph of the faces in the images 108a-e. Each face represents a node in the graph.  The graph generator 202 represents
the similarity scores between faces as edges that link the nodes.  Each edge is associated with its similarity score.  The edges linking the nodes may be undirected, bi-directional, or uni-directional.  For the purposes of clarity, the edges in the
following examples are undirected unless otherwise specified.
<br/><br/> The inferred gender label generator 112 associates each face with one or more gender labels and each gender label may have a gender label value.  In some implementations, the gender label values are numeric values, such as decimal numbers.  For
example, the face in the image 108a may have gender labels of "Male" and "Female" with associated gender label values of "0.00" and "1.00," respectively.  The inferred gender label generator 112 stores the gender label values in the gender label value
table 118.
<br/><br/> In some implementations, the gender determination system 102 includes a data storage 204 that stores one or more predetermined gender label values 206.  The gender determination system 102 may receive the predetermined gender label values, for
example, from a user input.  The graph generator 202 associates the predetermined gender label values 206 with corresponding nodes in the graph.  For example, a user may input the gender label values of "0.00" Male and "1.00" Female for the face in the
image 108a.  The graph generator 202 associates the gender label values with the node in the graph that represents the face from the image 108a.  The inferred gender label generator 112 subsequently uses the predetermined gender label values 206 as a
starting point, or seed, for the algorithm to infer other gender label values of faces included in other images.
<br/><br/> FIG. 3A shows examples of the images 108a-e used for inferring a gender of a face in an image.  The gender determination system 102 may use images such as the images 108a-e or other images.  The gender determination system 102 may also use fewer
or more images than are shown here.  The images 108a-e all include a person having a face.  In some implementations, an image may include multiple people or animals.  In this example, the image 108a is an adult female, the image 108b is an adult male,
the image 108c is a female child, the image 108d is a male child, and the image 108e is a picture of a somewhat androgynous adult.
<br/><br/> FIG. 3B shows examples of comparisons and similarities between multiple faces 302a-e in the images 108a-e. The face detector 114 identifies regions within the images 108a-e that represent the faces 302a-e. The similarity calculator 116 compares
features between faces to determine similarity scores between faces.  In some implementations, the similarity calculator 116 compares pairs of faces and determines a similarity score for each pair of faces.  In some implementations, the similarity score
is a decimal number in the range from zero to one, where zero indicates no similarity and one indicates that the faces are identical or indistinguishable.
<br/><br/> For example, the similarity calculator 116 may determine similarity scores of "0.7" between the faces 302b and 302d, "0.5" between the faces 302b and 302e, and "0.7" between the faces 302a and 302c.  The similarity scores of "0.7" indicate that
the faces 302b and 302d are significantly similar and the faces 302a and 302c are significantly similar.  The similarity score of "0.5" indicates that the faces 302b and 302e are only somewhat similar.
<br/><br/> The similarity calculator 116 may perform the comparison and calculate a similarity score for each pair of images.  The graph generator 202 uses the similarity scores to generate the graph representing similarities between the faces 302a-e.
<br/><br/> FIG. 3C shows an example of a weighted undirected graph 310 of the similarities between the faces 302a-e in the images 108a-e. The graph generator 202 represents the faces 302a-e with multiple nodes 312a-e, respectively.  The graph generator 202
links pairs of nodes with edges that represent a similarity score between the linked pair of nodes.  For example, the graph generator 202 links the nodes 312b and 312d with an edge having an associated similarity score of "0.7." In some implementations,
the graph generator 202 generates an edge between each pair of nodes in the weighted undirected graph 310.  In some implementations, the graph generator 202 links a portion of the pairs of nodes with edges.  For example, the graph generator 202 can link
pairs of nodes having a threshold similarity score.  In another example, the graph generator 202 can link a threshold number of nodes to a particular node, such as the five nodes having the highest similarity scores with the particular node.  For the
purposes of clarity in explanation, the weighted undirected graph 310 only includes edges between neighboring nodes.
<br/><br/> FIG. 3D shows an example of manually identified gender values for a portion of the weighted undirected graph 310.  As previously described, the gender determination system 102 includes the predetermined gender label values 206.  The graph
generator 202 associates the predetermined gender label values 206 with corresponding nodes.  For example, the graph generator 202 associates gender label values of "1.0" Male and "0.0" Female with the node 312b and "0.0" Male and "1.0" Female with the
node 312a.
<br/><br/> In this example of a weighted undirected graph, each node eventually has a Male gender label and a Female gender label (e.g., either predetermined or inferred).  The nodes 312a-b have predetermined Male and Female gender labels and label values. As described below, the inferred gender label generator 112 uses the weighted undirected graph 310 to infer Male and Female gender label values for the nodes 312c-e. Although this example describes two gender labels, in other implementations, a node may
have zero, three, or more gender labels as well.
<br/><br/> In some implementations, the inferred gender label generator 112 also infers gender label values for nodes having predetermined gender label values.  Inferred gender label values for nodes having predetermined gender label values can be used to
check the accuracy of the inferred gender label generator 112 and/or to check for possible errors in the predetermined gender label values 206.
<br/><br/> FIG. 4 is a flow chart showing an example of a method 400 for constructing a weighted undirected graph of similarities between faces in images.  In some implementations, the face detector 114, the similarity calculator 116, and the graph
generator 202 can include instructions that are executed by a processor of the gender determination system 102.
<br/><br/> The method 400 can start with step 402, which identifies face regions using a face detector.  For example, the inferred gender label generator 112 can use the face detector 114 to locate the regions of the images 108b and 108d that include the
faces 302b and 302d, respectively.
<br/><br/> In step 404, the method 400 compares two faces and generates a similarity score representing the similarity between the two faces.  For example, the similarity calculator 116 compares the faces 302b and 302d and calculates the similarity score
of "0.7" between the faces 302b and 302d.
<br/><br/> If at step 406 there are more faces to compare, then the method 400 returns to step 404 and compares another pair of faces.  Otherwise, the method 400 constructs a weighted undirected graph at step 408, where the faces are nodes and the
similarity scores are edges that link the nodes.  For example, the graph generator 202 constructs the nodes 312b and 312d representing the faces 302b and 302d.  The graph generator 202 links the nodes 312b and 312d with an edge associated with the
similarity score of "0.7" between the nodes 312b and 312d.
<br/><br/> At step 410, the method 400 receives a user input labeling a subset of the face nodes as either male or female.  For example, the gender determination system 102 may receive a user input including the predetermined gender label values 206.  The
predetermined gender label values 206 indicate that the node 312b has gender label values of "1.0" Male and "0.0" Female.  The inferred gender label generator 112 uses the weighted undirected graph 310 and the predetermined gender label values 206 to
infer the gender of one or more faces represented by nodes in the weighted undirected graph 310.
<br/><br/> FIG. 5 is a flow chart showing an example of a method 500, referred to here as an "adsorption" algorithm, for inferring the gender of a face in an image using a weighted undirected graph of similarities between faces in images.  In some
implementations, the inferred gender label generator 112 can include instructions that are executed by a processor of the gender determination system 102.
<br/><br/> The adsorption algorithm 500 can be executed using information from the graphs shown in FIGS. 3C-D and can be executed for every node in the graphs.  The adsorption algorithm 500 can start with step 502, which determines if a specified number of
iterations have run for a graph.  If the number is not complete, step 504 is performed.  In step 504, a node representing a face is selected.  For example, the inferred gender label generator 112 can select a node that has gender label values that may be
modified by the algorithm, such as "Male" or "Female."
<br/><br/> In step 506, a gender label is selected from the selected node.  For example, the inferred gender label generator 112 can select the gender label "Male" if present in the selected node.  In step 508, a gender label value for the selected gender
label is initialized to zero.  For example, the inferred gender label generator 112 can set the gender label value for "Male" to zero.
<br/><br/> In step 510, a neighboring node of the selected node can be selected.  For example, the selected node may specify that a neighboring node has a similarity score of "0.7" with the selected node.  The inferred gender label generator 112 can select
the neighboring node.
<br/><br/> In step 512, a corresponding weighted gender label value of a selected neighbor is added to a gender label value of the selected gender label.  For example, if the selected neighboring node has a gender label value for the gender label "Male,"
the inferred gender label generator 112 can add this value to the selected node's gender label value for "Male." In certain implementations, the gender label value retrieved from a neighboring node can be weighted to affect the contribution of the gender
label value based on the degree of distance from the selected node (e.g., based on whether the neighboring node is linked directly to the selected node, linked by two edges to the selected node, etc.) In some implementations, the gender label value can
also be based on a weight or similarity score associated with the edge.
<br/><br/> In step 514, it is determined whether there is another neighbor node to select.  For example, the inferred gender label generator 112 can determine if the selected node is linked by a single edge to any additional neighbors that have not been
selected.  In another example, a user may specify how many degrees out (e.g., linked by two edges, three edges, etc.) the inferred gender label generator 112 should search for neighbors.  If there is another neighbor that has not been selected, steps 510
and 512 can be repeated, as indicated by step 514.  If there is not another neighbor, step 516 can be performed.
<br/><br/> In step 516, it is determined whether there is another gender label in the selected node.  For example, the selected node can have multiple gender labels, such as "Male" and "Female." If these additional gender labels have not been selected, the
inferred gender label generator 112 can select one of the previously unselected gender labels and repeat steps 506-514.  If all the gender labels in the node have been selected, the gender label values of the selected node can be normalized, as shown in
step 518.  For example, the inferred gender label generator 112 can normalize each gender label value so that it has a value between 0 and 1, where the gender label value's magnitude is proportional to its contribution relative to all the gender label
values associated with that node.
<br/><br/> In step 520, it can be determined whether there are additional nodes in the graph to select.  If there are additional nodes, the method can return to step 504.  If all the nodes in the graph have been selected, the method can return to step 502
to determine whether the specified number of iterations has been performed on the graph.  If so, the adsorption algorithm 500 can end.
<br/><br/> In certain implementations, the adsorption algorithm 500 can include the following pseudo code:
<br/><br/> Set t=0
<br/><br/> For each node, n, in the similarity graph, G: For each gender label, I: Initialize the gender label: n.sub.i,t=0.0;
<br/><br/> For t=1 .  . . x iterations: For each node used to label other nodes, n, in the similarity graph, G: For each gender label, I: Initialize the gender label value: n.sub.i,t+1=n.sub.i,t For each node to be labeled, n, in the similarity graph, G:
For each gender label, I: Initialize the gender label value: n.sub.i,t+1=n.sub.t,injection;
<br/><br/> //where n.sub.t,injection is a node having a static assigned value For each node, n, in the similarity graph, G: For each node, m, that has an edge with similarity s.sub.mn, to n: For each gender label:
n.sub.i,t+1=n.sub.i,t+1+(s.sub.mn*m.sub.i,t) Normalize the weight of the gender labels at each n, so that the sum of the labels at each node=1.0
<br/><br/> In certain implementations, after "x" iterations, the inferred gender label generator 112 can examine one or more of the nodes of the graph and probabilistically assign a gender label to each node based on the weights of the gender labels (e.g.,
a label with the maximum label value can be assigned to the node).
<br/><br/> In some implementations, the number of the iterations is specified in advance.  In other implementations, the algorithm terminates when the gender label values for the gender labels at each node reach a steady state (e.g., a state where the
difference in the label value change between iterations is smaller than a specified epsilon).
<br/><br/> In another alternative method, gender label values for nodes can be inferred by executing a random walk algorithm on the graphs.  More specifically, in some implementations, given a graph, G, the inferred gender label generator 112 can calculate
gender label values, or label weights, for every node by starting a random walk from each node.  The random walk algorithm can include reversing the direction of each edge in the graph if the edge is directed.  If the edge is bi-directional or
undirected, the edge can be left unchanged.
<br/><br/> The inferred gender label generator 112 can select a node of interest and start a random walk from that node to linked nodes.  At each node where there are multiple-out nodes (e.g., nodes with links to multiple other nodes), the inferred gender
label generator 112 can randomly select an edge to follow.  If the edges are weighted, the inferred gender label generator 112 can select an edge based on the edge's weight (e.g., the greatest weighted edge can be selected first).
<br/><br/> If during the random walk, a node is selected that is a labeling node (e.g., used as a label for other nodes), the classification for this walk is the label associated with the labeling node.  The inferred gender label generator 112 can maintain
a tally of the labels associated with each classification.
<br/><br/> If during the random walk, a node is selected that is not a labeling node, the inferred gender label generator 112 selects the next random path, or edge, to follow.
<br/><br/> The inferred gender label generator 112 can repeat the random walk multiple times (e.g., 1000s to 100,000s of times) for each node of interest.  After completion, the inferred gender label generator 112 can derive the gender label values based
on the tally of labels.  This process can be repeated for each node of interest.
<br/><br/> Additionally, in some implementations, the inferred gender label generator 112 generates a second data structure, such as a second graph.  The second graph can include nodes substantially similar to the weighted undirected graph 310.  In one
example, the weighted undirected graph 310 includes Male gender label values and a second graph includes Female gender label values.
<br/><br/> In some implementations, the adsorption algorithm 500 can also be performed on the second graph.  The inferred gender label generator 112 can select and compare the resulting label value magnitudes for a corresponding node from both the first
and second graphs.  In some implementations, the inferred gender label generator 112 can combine the gender label value magnitudes from each graph through linear weighting to determine a final gender label value magnitude (e.g., the first graph's gender
label value contributes 0.7 and the second graph's gender label value contributes 0.3).  In other implementations, the gender label values can be weighed equally to determine the final gender label value (e.g., 0.5, 0.5), or the inferred gender label
generator 112 can give one gender label value its full contribution while ignoring the gender label value from the other graph (e.g., [1.0, 0.0] or [0.0, 1.0]).
<br/><br/> In other implementations, the inferred gender label generator 112 can use a cross-validation method to set the contribution weights for gender label value magnitudes from each graph.  For example, the inferred gender label generator 112 can
access nodes in a graph, where the nodes have gender label value magnitudes that are known.  The inferred gender label generator 112 can compare the actual gender label value magnitudes with the known gender label value magnitudes.  The inferred gender
label generator 112 can then weight each graph based on how closely its gender label values match the known gender label values.
<br/><br/> In certain implementations, the inferred gender label generator 112 can compare the gender label value magnitudes to an expected a priori distribution, instead of or in addition to examining the final gender label magnitudes.  For example, if a
summed value of a first gender label across all nodes is 8.0 and the summed value of a second gender label across all of the nodes is 4.0, the a priori distribution suggests that first gender label may be twice as likely to occur as the second gender
label.  The inferred gender label generator 112 can use this expectation to calibrate the gender label value magnitudes for each node.  If in a particular node, the first gender label value is 1.5 and the second gender label value is 1.0, then the
evidence for the first gender label, although higher than the second gender label, is not as high as expected because the ratio is not as high as the a priori distribution.  This decreases the confidence that the difference in magnitudes is meaningful. 
A confidence factor can be translated back into the rankings.
<br/><br/> In some implementations, if the difference in magnitudes is below a confidence threshold, the inferred gender label generator 112 can rank a gender label with a lower value above a gender label with a higher value (e.g., manipulate the lower
value so that it is increased to a gender label value greater than the higher value).  For example, if the first gender label's value is expected to be three times the value of the second gender label, but was only 1.1 times greater than the second
gender label, the inferred gender label generator 112 can rank the second gender label above the first gender label.  In some implementations, the confidence factor can be kept as a confidence measure, which can be used, for example, by machine learning
algorithms to weight the resultant label value magnitudes.
<br/><br/> In yet other implementations, instead of or in addition to comparing the gender label value magnitudes based on the a priori distribution of the nodes, the inferred gender label generator 112 can compare the gender label value magnitudes based
on an end distribution of magnitudes across all nodes.  For example, the inferred gender label generator 112 can measure how different a particular node's distribution is from an average calculated across all nodes in a graph.
<br/><br/> FIGS. 6A-D show examples of gender values in a weighted undirected graph 600 before performing a gender inferring process and after one iteration, two iterations, and four iterations of the gender inferring process, respectively.  Referring to
FIG. 6A, the weighted undirected graph 600 includes predetermined and initial gender values for the nodes 312a-e. The weighted undirected graph 600 only shows Male gender labels for the sake of clarity.  Other gender labels may be included in the
weighted undirected graph 600 or another graph having nodes corresponding to nodes in the weighted undirected graph 600.
<br/><br/> The weighted undirected graph 600 includes predetermined gender label values for the nodes 312a-b of "0.00" Male and "1.00" Male, respectively.  The nodes 312c-e have initial gender label values of "0.50" Male.  In some implementations, an
initial gender label value may be a predetermined value, such as "0.50." Alternatively, the initial gender label value may be based on the average of the predetermined gender label values 206 or another set of gender label values.
<br/><br/> Referring to FIG. 6B, the weighted undirected graph 600 now shows the Male gender label values of the nodes 312a-e after one iteration of the inferred gender label generator 112.  In this example, the Male gender label values of the nodes 312a-b
are fixed at "0.00" and "1.00," respectively.  The Male gender label values of the nodes 312c-e are now "0.25," "0.73," and "0.53," respectively.  The relatively high similarity between the nodes 312a and 312c as compared to other nodes gives the node
312c a Male gender label value ("0.25") close to the Male gender label value of the node 312a ("0.00").  Similarly, the relatively high similarity between the nodes 312b and 312d as compared to other nodes gives the node 312d a Male gender label value
("0.73") close to the Male gender label value of the node 312b ("1.00").  The node 312e has slightly higher similarity with the nodes 312b and 312d leading to a Male gender value ("0.53") that leans toward Male.
<br/><br/> Referring to FIG. 6C, the weighted undirected graph 600 now shows the gender label values for the nodes 312a-e after two iterations of the inferred gender label generator 112.  The Male gender label value of the node 312c has increased by "0.01"
to "0.26." The Male gender label value of the node 312d has increased by "0.01" to "0.74." The Male gender label value of the node 312e has increased by "0.01" to "0.54." In some implementations, the inferred gender label generator 112 may stop
processing a graph after the change in each gender label value is below a particular threshold, such as "0.01."
<br/><br/> Referring to FIG. 6D, the weighted undirected graph 600 now shows the gender label values for the nodes 312a-e after four iterations of the inferred gender label generator 112.  The Male gender label value of the node 312d has increased by
"0.01" to "0.75." The Male gender label values of the nodes 312a-e have now converged within "0.01." The inferred gender label generator 112 stops processing the weighted undirected graph 600 and outputs the Male gender label values.  Alternatively, the
inferred gender label generator 112 may stop processing after a predetermined number of iterations, such as five iterations.
<br/><br/> In some implementations, the inferred gender label generator 112 infers Female gender label values for the nodes 312a-e in addition to the Male gender label values previously described.  The inferred gender label generator 112 can compare and/or
combine the Male and Female gender label values to determine a gender for the faces 302a-e associated with the nodes 312a-e. For example, if the Male gender label value of a node is larger than its Female gender label value than the corresponding face is
determined to be Male and vice versa for Female.
<br/><br/> FIG. 7A shows an example of a table 700 that stores gender values for the faces 302a-e. The weighted undirected graphs 310 and 600 may stored in the data storage 110 or structured in a memory module as tables, such as the table 700.  In
particular, the table 700 includes an identifier 702 for each of the faces 302a-e, an indication 704 of where similarity scores to other faces can be found, a Male gender label value 706, and a Female gender label value 708.  The entry in the table 700
for Face A indicates that the similarity scores associated with face can be found in Table A.
<br/><br/> FIG. 7B shows an example of a table 710 that stores similarity scores between a first face and other faces.  In particular, the table 710 represents Table A, which includes the similarity scores associated with Face A from the table 700.  The
table 710 includes an identifier 712 for each of the associated faces and a corresponding similarity score 714.
<br/><br/> FIG. 8 shows an example of a weighted undirected graph 800 including dummy labels.  In certain implementations, dummy labels can be used in the weighted undirected graph 800 to reduce effects of distant neighboring nodes.  When the label value
of a node is determined based on, for example, the label with the greatest label value, the weight assigned to the dummy label can be ignored and the remaining weights used in the determination.  For example, the dummy labels' contribution can be removed
from the calculation of the label values at the end of the algorithm (e.g., after the label values have reached a steady state or a specified number of iterations have occurred).
<br/><br/> In certain implementations, dummy labels can be used in all of the graphs generated by the inferred label generator.  In other implementations, a user may specify for which graph(s) the dummy labels may be used.
<br/><br/> The example of dummy labels shown here associates a dummy node with each of the nodes 312a-e in the weighted undirected graph 800.  In other implementations, dummy labels are assigned to a small number of nodes, such as nodes that are not
associated with initial gender label values.
<br/><br/> FIG. 9 is a schematic diagram of a generic computing system 900.  The generic computing system 900 can be used for the operations described in association with any of the computer-implement methods described previously, according to one
implementation.  The generic computing system 900 includes a processor 902, a memory 904, a storage device 906, and an input/output device 908.  Each of the processor 902, the memory 904, the storage device 906, and the input/output device 908 are
interconnected using a system bus 910.  The processor 902 is capable of processing instructions for execution within the generic computing system 900.  In one implementation, the processor 902 is a single-threaded processor.  In another implementation,
the processor 902 is a multi-threaded processor.  The processor 902 is capable of processing instructions stored in the memory 904 or on the storage device 906 to display graphical information for a user interface on the input/output device 908.
<br/><br/> The memory 904 stores information within the generic computing system 900.  In one implementation, the memory 904 is a computer-readable medium.  In one implementation, the memory 904 is a volatile memory unit.  In another implementation, the
memory 904 is a non-volatile memory unit.
<br/><br/> The storage device 906 is capable of providing mass storage for the generic computing system 900.  In one implementation, the storage device 906 is a computer-readable medium.  In various different implementations, the storage device 906 may be
a floppy disk device, a hard disk device, an optical disk device, or a tape device.
<br/><br/> The input/output device 908 provides input/output operations for the generic computing system 900.  In one implementation, the input/output device 908 includes a keyboard and/or pointing device.  In another implementation, the input/output
device 908 includes a display unit for displaying graphical user interfaces.
<br/><br/> The features described can be implemented in digital electronic circuitry, or in computer hardware, firmware, software, or in combinations of them.  The apparatus can be implemented in a computer program product tangibly embodied in an
information carrier, e.g., in a machine-readable storage device or in a propagated signal, for execution by a programmable processor; and method steps can be performed by a programmable processor executing a program of instructions to perform functions
of the described implementations by operating on input data and generating output.  The described features can be implemented advantageously in one or more computer programs that are executable on a programmable system including at least one programmable
processor coupled to receive data and instructions from, and to transmit data and instructions to, a data storage system, at least one input device, and at least one output device.  A computer program is a set of instructions that can be used, directly
or indirectly, in a computer to perform a certain activity or bring about a certain result.  A computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including
as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment.
<br/><br/> Suitable processors for the execution of a program of instructions include, by way of example, both general and special purpose microprocessors, and the sole processor or one of multiple processors of any kind of computer.  Generally, a
processor will receive instructions and data from a read-only memory or a random access memory or both.  The essential elements of a computer are a processor for executing instructions and one or more memories for storing instructions and data. 
Generally, a computer will also include, or be operatively coupled to communicate with, one or more mass storage devices for storing data files; such devices include magnetic disks, such as internal hard disks and removable disks; magneto-optical disks;
and optical disks.  Storage devices suitable for tangibly embodying computer program instructions and data include all forms of non-volatile memory, including by way of example semiconductor memory devices, such as EPROM, EEPROM, and flash memory
devices; magnetic disks such as internal hard disks and removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.  The processor and the memory can be supplemented by, or incorporated in, ASICs (application-specific integrated circuits).
<br/><br/> To provide for interaction with a user, the features can be implemented on a computer having a display device such as a CRT (cathode ray tube) or LCD (liquid crystal display) monitor for displaying information to the user and a keyboard and a
pointing device such as a mouse or a trackball by which the user can provide input to the computer.
<br/><br/> The features can be implemented in a computer system that includes a back-end component, such as a data server, or that includes a middleware component, such as an application server or an Internet server, or that includes a front-end component,
such as a client computer having a graphical user interface or an Internet browser, or any combination of them.  The components of the system can be connected by any form or medium of digital data communication such as a communication network.  Examples
of communication networks include, e.g., a LAN, a WAN, and the computers and networks forming the Internet.
<br/><br/> The computer system can include clients and servers.  A client and server are generally remote from each other and typically interact through a network, such as the described one.  The relationship of client and server arises by virtue of
computer programs running on the respective computers and having a client-server relationship to each other
<br/><br/> Although a few implementations have been described in detail above, other modifications are possible.  For example, image or facial labels other than gender may be used, such as ethnicity or complexion.
<br/><br/> In addition, the logic flows depicted in the figures do not require the particular order shown, or sequential order, to achieve desirable results.  In addition, other steps may be provided, or steps may be eliminated, from the described flows,
and other components may be added to, or removed from, the described systems.  Accordingly, other implementations are within the scope of the following claims.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09355300&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D425%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D9%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D425%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D9%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209355300"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9355300&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D425%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D9%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209355300">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=425&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=425&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=425&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=10&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=424&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=426&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></body></html>