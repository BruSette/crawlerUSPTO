<html><head>
<base target="_top"/>
<title>United States Patent: 9298970</title></head>
<!---BUF1=9298970
BUF7=2016
BUF8=67758
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=936&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=936&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=936&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=20&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=935&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=937&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D936%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D19%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209298970"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9298970&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D936%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D19%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209298970">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09298970&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D936%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D19%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>936</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,298,970</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
         Wang
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     March 29, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">Method and apparatus for facilitating interaction with an object viewable
     via a display
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> A method, apparatus and computer program product are provided to
     facilitate interaction with a computing device, such as interaction with
     an object viewable via a display of a computing device. In the context of
     a method, an object viewable via a display is identified, such as by
     facial recognition. The method also includes determining that the object
     has become aligned with an operation indicator as a result of movement of
     the display relative to the object. In response to determining that the
     object is aligned with the operation indicator, the method also performs
     an operation associated with the operation indicator. In this regard, a
     method may perform the operation upon at least one aspect associated with
     the object. Corresponding apparatus and computer program products are
     also provided.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Wang; Kongqiao</b> (Helsinki, <b>FI</b>)<b>, Karkkainen; Leo</b> (Helsinki, <b>FI</b>) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/>Nokia Technologies Oy</b> </td><td> <br/>Espoo </td><td align="center"> <br/>N/A </td><td align="center"> <br/>FI </td> <td align="left">
</td> </tr> </tbody></table>
<!-- AANM>
~AANM Nokia Technologies Oy
~AACI Espoo
~AAST N/A
~AACO FI
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b>Nokia Technologies Oy</b>
 (Espoo, 
<b>FI</b>)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>49759348
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>13/686,092</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>November 27, 2012</b></td></tr>
     </tbody></table>
<hr/> <center><b>Prior Publication Data</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Document Identifier</u></b></th><th align="center" scope="col"><b><u>Publication Date</u></b></th></tr><tr><td align="center"> </td><td align="center"> US 20140147021 A1</td><td align="center">May 29, 2014</td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table>
     <hr/>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">G06F 3/011 (20130101); G06K 9/00228 (20130101); G02B 27/017 (20130101); G06K 9/00624 (20130101); G06K 9/00 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">G06K 9/00 (20060101); G06F 3/01 (20060101)</td></tr>
       <tr><td align="left" valign="top" width="30%"><b>Field of Search: </b></td>
       <td align="right" valign="top" width="70%">
       











 ;382/103,118,286 ;359/407,630 ;345/420,629,173,158,660,156,633
       </td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9298970">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5559942">5559942</a></td><td align="left">
September 1996</td><td align="left">
Gough et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F5844547">5844547</a></td><td align="left">
December 1998</td><td align="left">
Minakuchi et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6184859">6184859</a></td><td align="left">
February 2001</td><td align="left">
Kojima</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6295069">6295069</a></td><td align="left">
September 2001</td><td align="left">
Shirur</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6980363">6980363</a></td><td align="left">
December 2005</td><td align="left">
Takagi et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7120279">7120279</a></td><td align="left">
October 2006</td><td align="left">
Chen et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7230583">7230583</a></td><td align="left">
June 2007</td><td align="left">
Tidwell et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7626578">7626578</a></td><td align="left">
December 2009</td><td align="left">
Wilson et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7924506">7924506</a></td><td align="left">
April 2011</td><td align="left">
Rieger</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8199974">8199974</a></td><td align="left">
June 2012</td><td align="left">
Gomez et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8223088">8223088</a></td><td align="left">
July 2012</td><td align="left">
Gomez et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8473862">8473862</a></td><td align="left">
June 2013</td><td align="left">
Davidson</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8531539">8531539</a></td><td align="left">
September 2013</td><td align="left">
Sasaki</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8564693">8564693</a></td><td align="left">
October 2013</td><td align="left">
Makii</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20020140745&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2002/0140745</a></td><td align="left">
October 2002</td><td align="left">
Ellenby et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040070675&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0070675</a></td><td align="left">
April 2004</td><td align="left">
Fredlund et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060061551&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0061551</a></td><td align="left">
March 2006</td><td align="left">
Fateh</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060204135&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0204135</a></td><td align="left">
September 2006</td><td align="left">
Funakura</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080048044&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0048044</a></td><td align="left">
February 2008</td><td align="left">
Zhao et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080159591&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0159591</a></td><td align="left">
July 2008</td><td align="left">
Ruedin</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080262910&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0262910</a></td><td align="left">
October 2008</td><td align="left">
Altberg et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090091553&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0091553</a></td><td align="left">
April 2009</td><td align="left">
Keam et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20100103311&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2010/0103311</a></td><td align="left">
April 2010</td><td align="left">
Makii</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20100125812&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2010/0125812</a></td><td align="left">
May 2010</td><td align="left">
Hartman</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20100238286&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2010/0238286</a></td><td align="left">
September 2010</td><td align="left">
Boghossian</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110034247&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0034247</a></td><td align="left">
February 2011</td><td align="left">
Masuda et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110078620&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0078620</a></td><td align="left">
March 2011</td><td align="left">
Chiou</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110221669&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0221669</a></td><td align="left">
September 2011</td><td align="left">
Shams et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110254861&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0254861</a></td><td align="left">
October 2011</td><td align="left">
Emura et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20120243739&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2012/0243739</a></td><td align="left">
September 2012</td><td align="left">
Fukuchi</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20120293548&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2012/0293548</a></td><td align="left">
November 2012</td><td align="left">
Perez et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20130093788&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2013/0093788</a></td><td align="left">
April 2013</td><td align="left">
Liu</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
       <center><b>Foreign Patent Documents</b></center>
<table width="100%"> <tbody><tr><td></td><th scope="col"></th> <td></td><th scope="col"></th> <td></td><th scope="col"></th></tr> <tr> <td align="left">
</td><td align="left">1 271 293</td><td></td><td align="left">
Jan 2003</td><td></td><td align="left">
EP</td></tr><tr><td align="left">
</td><td align="left">WO-2012/040107</td><td></td><td align="left">
Mar 2012</td><td></td><td align="left">
WO</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
<table width="90%">   <tbody><tr><td><align="left"><br/>Internatinal Search Report and Written Opinion for Application No. PCT/FI2013/051064 dated Feb. 13, 2014. cited by applicant
.<br/>"Facial Recognition, Augmented Reality and Gesture Control to Promote `The Hunger Games`;" Digital AV Magazine; dated Apr. 18, 2012; retrieved on Apr. 5, 2013 from &lt;http://www.digitalavmagazine.com/en/2012/04/18/reconocimiento-facial--
realidad-aumentada-y-control-de-gestos-para-promocionar-los-juegos-del-ham- bre/&gt;. cited by applicant
.<br/>Hendrickx, G.; "Augmented Reality Face Recognition for Mobile Devices;" Katholieke Universiteit Leuven. cited by applicant
.<br/>Storring, M., et al.; "Computer Vision-Based Gesture Recognition for an Augmented Reality Interface;" 4.sup.th IASTED International Conference on Visualization, Imaging, and Image Processing; pp. 766-771; dated Sep. 2004. cited by applicant.
</align="left"></td></tr> </tbody></table><br/><center><b>Other References</b></center> <br/>
       <i>Primary Examiner:</i> Le; Vu
<br/>
       <i>Assistant Examiner:</i> Woldemariam; Aklilu
<br/>
       <i>Attorney, Agent or Firm:</i> <coma>Alston &amp; Bird LLP
<br/>
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>What is claimed is: <br/><br/> 1.  A method for identifying an object viewable via a display of a computing device, the method comprising: identifying the object viewable via the display, wherein
identification of the object viewable via the display is performed by a processor;  causing an operation indicator to be presented upon the display concurrent with the object, wherein the operation indicator is associated with an operation that may be
performed upon at least one aspect associated with the object;  determining, with the processor, that the object has become aligned with the operation indicator as presented upon the display as a result of movement of the display relative to the object,
wherein a relative position of the operation indicator upon the display is independent of the movement of the display relative to the object;  and in response to determining that the object is aligned with the operation indicator, performing the
operation associated with the operation indicator, wherein performing the operation comprises performing the operation upon at least one aspect associated with the object.
<br/><br/> 2.  A method according to claim 1 further comprising providing for selection of the object.
<br/><br/> 3.  A method according to claim 2 wherein performing the operation comprises performing the operation only in an instance in which the object has been selected.
<br/><br/> 4.  A method according to claim 1 wherein identifying an object comprises employing facial recognition to identify a person.
<br/><br/> 5.  A method according to claim 1 wherein the display comprises a pass-through display, and wherein identifying an object comprises identifying an object viewable through the pass-through display.
<br/><br/> 6.  A method according to claim 1 further comprising: receiving an image that includes the object;  and causing the image to be presented upon the display prior to identifying the object.
<br/><br/> 7.  An apparatus for identifying an object viewable via a display of a computing device, the apparatus comprising at least one processor and at least one memory including computer program code, the at least one memory and the computer program
code configured to, with the processor, cause the apparatus to at least: identify the object viewable via the display, wherein identification of the object viewable via the display is performed by the apparatus including the at least one processor; 
causing an operation indicator to be presented upon the display concurrent with the object, wherein the operation indicator is associated with an operation that may be performed upon at least one aspect associated with the object;  determine that the
object has become aligned with the operation indicator as presented upon the display as a result of movement of the display relative to the object, wherein a relative position of the operation indicator upon the display is independent of the movement of
the display relative to the object;  and in response to determining that the object is aligned with the operation indicator, perform the operation associated with the operation indicator, wherein the at least one memory and the computer program code are
configured to, with the processor, cause the apparatus to perform the operation by performing the operation upon at least one aspect associated with the object.
<br/><br/> 8.  An apparatus according to claim 7 wherein the at least one memory and the computer program code are further configured to, with the processor, cause the apparatus to provide for selection of the object.
<br/><br/> 9.  An apparatus according to claim 8 wherein the at least one memory and the computer program code are configured to, with the processor, cause the apparatus to perform the operation by performing the operation only in an instance in which the
object has been selected.
<br/><br/> 10.  An apparatus according to claim 7 wherein the at least one memory and the computer program code are configured to, with the processor, cause the apparatus to identify an object by employing facial recognition to identify a person.
<br/><br/> 11.  An apparatus according to claim 7 wherein the display comprises a pass-through display, and wherein the at least one memory and the computer program code are configured to, with the processor, cause the apparatus to identify an object by
identifying an object viewable through the pass-through display.
<br/><br/> 12.  An apparatus according to claim 7 wherein the at least one memory and the computer program code are further configured to, with the processor, cause the apparatus to: receive an image that includes the object;  and cause the image to be
presented upon the display prior to identifying the object.
<br/><br/> 13.  A computer program product for identifying an object viewable via a display of a computing device, the computer program product comprising at least one non-transitory computer-readable storage medium having computer-executable program code
portions stored therein, the computer-executable program code portions comprising program code instructions for: identifying the object viewable via the display, wherein identification of the object viewable via the display is performed by the computing
device during execution of the computer-executable program code portions;  causing an operation indicator to be presented upon the display concurrent with the object, wherein the operation indicator is associated with an operation that may be performed
upon at least one aspect associated with the object;  determining that the object has become aligned with the operation indicator as presented upon the display as a result of movement of the display relative to the object, wherein a relative position of
the operation indicator upon the display is independent of the movement of the display relative to the object;  and in response to determining that the object is aligned with the operation indicator, performing the operation associated with the operation
indicator, wherein performing the operation comprises performing the operation upon at least one aspect associated with the object.
<br/><br/> 14.  A computer program product according to claim 13 wherein the computer-executable program code portions further comprise program code instructions for providing for selection of the object, and wherein the program code instructions for
performing the operation comprise program code instructions for performing the operation only in an instance in which the object has been selected.
<br/><br/> 15.  A computer program product according to claim 13 wherein the program code instructions for identifying an object comprise program code instructions for employing facial recognition to identify a person.
<br/><br/> 16.  A computer program product according to claim 13 wherein the display comprises a pass-through display, and wherein the program code instructions for identifying an object comprise program code instructions for identifying an object viewable
through the pass-through display.
<br/><br/> 17.  A computer program product according to claim 13 wherein the computer-executable program code portions further comprise program code instructions for: receiving an image that includes the object;  and causing the image to be presented upon
the display prior to identifying the object. <hr/> <center><b><i>Description</i></b></center> <hr/> <br/><br/>TECHNOLOGICAL FIELD
<br/><br/> An example embodiment of the present invention relates generally to a method and apparatus for facilitating interaction with a computing device and, more particularly, to a method and apparatus for interacting with an object viewable via a
display of the computing device.
<br/><br/>BACKGROUND
<br/><br/> Users of computing devices interact with those devices on a frequent and repeated basis.  Accordingly, a number of different techniques have been developed to facilitate interaction with the computing devices.  These techniques may include the
receipt of user input via a keypad, via a touchscreen or via audible commands, to name but a few.  Although various techniques for facilitating user interaction have been developed, the user experience may be improved by further improvements related to
user interaction with computing devices.
<br/><br/> By way of example, pass-through displays may permit a user to view one or more objects through the display.  Similarly, other types of computing devices may present an image upon the display, such as an image captured by a camera carried by the
computing device, with the image including one or more objects.  In either instance, a user may wish to interact with an object that is viewable via the display, such as by performing an operation upon the object.  The user experience may therefore be
improved by correspondingly providing further improvements in regards to the user's interaction with objects viewable via the display of a computing device.
<br/><br/>BRIEF SUMMARY
<br/><br/> A method, apparatus and computer program product are therefore provided according to an example embodiment of the present invention in order to facilitate interaction with a computing device, such as interaction with an object viewable via a
display of a computing device.  In this regard, the method, apparatus and computer program product of an example embodiment may permit an operation to be performed upon at least one aspect associated with an object viewable via the display.  As such, the
user experience with a computing device, such as a pass-through display, may be improved as a result of the interaction provided in accordance with a method, apparatus and computer program product of an example embodiment.
<br/><br/> In one embodiment, a method is provided that includes identifying an object viewable via a display.  For example, the method of one embodiment may employ facial recognition to identify a person.  The method also includes determining, with a
processor, that the object has become aligned with an operation indicator as a result of movement of the display relative to the object.  In response to determining that the object is aligned with the operation indicator, the method also performs an
operation associated with the operation indicator.  In this regard, a method may perform the operation upon at least one aspect associated with the object.
<br/><br/> The method of one embodiment may also include providing for selection of the object.  In this embodiment, the performance of the operation may not only be dependent upon the determination that the object is aligned with the operation indicator,
but also upon the selection of the object.  In one embodiment, the method also causes the operation indicator to be presented upon the display.  In an embodiment in which the display comprises a pass-through display, the method may identify the object by
identifying an object viewable through the pass-through display.  In another embodiment, the method may receive an image that includes the object and may cause the image to be presented upon the display prior to identifying the object.
<br/><br/> In another embodiment, an apparatus is provided that includes at least one processor and at least one memory including computer program code with at least one memory and computer program code configured to, with the processor, cause the
apparatus to at least identify an object viewable via a display, such as by employing facial recognition to identify a person.  The at least one memory and the computer program code also configured to, with the processor, cause the apparatus to determine
that the object has become aligned with an operation indicator as a result of movement of the display relative to the object.  The at least one memory and the computer program code are also configured to, with the processor, cause the apparatus of this
embodiment to perform an operation associated with the operation indicator in response to determining that the object is aligned with the operation indicator.  The at least one memory and the computer program code are also configured to, with the
processor, cause the apparatus of this embodiment to perform the operation upon at least one aspect associated with the object.
<br/><br/> The at least one memory and the computer program code may be further configured to, with the processor, cause the apparatus of an example embodiment to provide for selection of the object.  In this embodiment, the performance of the operation
may not only be dependent upon the determination that the object is aligned with the operation indicator, but also upon the selection of the object.  In one embodiment, the at least one memory and the computer program code may also be configured to, with
the processor, cause the apparatus to cause the operation indicator to be presented upon the display.  In an embodiment in which the display includes a pass-through display, the at least one memory and the computer program code may be configured to, with
the processor, cause the apparatus to identify an object by identifying an object viewable through the pass-through display.  The at least one memory and the computer program code may be further configured to, with the processor, cause the apparatus of
another embodiment to receive an image that includes the object and to cause the image to be presented upon the display prior to identifying the object.
<br/><br/> In a further embodiment, a computer program product is provided that includes at least one non-transitory computer-readable storage medium having computer-executable program code portions stored therein with the computer-executable program code
portions including program code instructions for identifying objects viewable via a display, such as by employing facial recognition to identify a person.  The computer-executable program code versions of this embodiment also include program code
instructions for determining that the object has become aligned with an operation indicator as a result of movement of the display relative to the object.  The computer-executable program code portions of this embodiment also include program code
instructions for performing an operation associated with the operation indicator in response to determining that the object is aligned with the operation indicator.  In this regard, the program code instructions for performing the operation may include
program code instructions for performing the operation based upon at least one aspect associated with the object.
<br/><br/> The computer-executable program code portions of one embodiment may also include program code instructions for providing for selection of the object.  In this regard, the program code instructions for performing the operation may not only be
dependent upon a determination that the object is aligned with the operation indicator, but may also require the object to have been selected prior to performing the operation.  In one embodiment, the computer-executable program code portion may also
include program code instructions for causing the operation indicator to be presented upon the display.  In an embodiment in which the display includes a pass-through display, the program code instructions for identifying an object may include program
code instructions for identifying an object viewable through the pass-through display.  In another embodiment, the computer-executable program code portions may also include program code instructions for receiving the image that includes the object and
causing the image to be presented upon the display prior to identifying the object.
<br/><br/> In yet another embodiment, an apparatus is provided that includes means for identifying an object viewable via a display.  The apparatus of this embodiment may also include means for determining that the object has become aligned with an
operation indicator as a result of movement of the display relative to the object.  The apparatus of this embodiment may also include means for performing an operation associated with the operation indicator in response to determining that the object is
aligned with the operation indicator.  In this regard, the means for performing the operation may be configured to perform the operation upon at least one aspect associated with the object. <br/><br/>BRIEF DESCRIPTION OF THE DRAWINGS
<br/><br/> Having thus described certain example embodiments of the present invention in general terms, reference will hereinafter be made to the accompanying drawings, which are not necessarily drawn to scale, and wherein:
<br/><br/> FIG. 1 is block diagram of an apparatus that may be specifically configured in accordance with an example embodiment of the present invention;
<br/><br/> FIG. 2 is perspective view of a pass-through display that may be specifically configured in accordance with an example embodiment of the present invention;
<br/><br/> FIG. 3 is a plan view of a mobile terminal that may be specifically configured in accordance with an example embodiment to the present invention;
<br/><br/> FIG. 4 is a flowchart illustrating operations performed, such as by the apparatus of FIG. 1, in accordance with an example embodiment to the present invention;
<br/><br/> FIG. 5 is an image that includes an object, a plurality of operation indicators and a representation of a cursor;
<br/><br/> FIG. 6 is the image of FIG. 5 with one of the operation indicators having become aligned with the object as a result of movement of the display relative to the object and in accordance with an example embodiment of the present invention; and
<br/><br/> FIG. 7 is the image of FIGS. 5 and 6 which depicts the performance of an operation upon at least one aspect associated with the object in response to determining that the object is aligned with the operation indicator in accordance with an
example embodiment to the present invention.
<br/><br/>DETAILED DESCRIPTION
<br/><br/> Some embodiments of the present invention will now be described more fully hereinafter with reference to the accompanying drawings, in which some, but not all, embodiments of the invention are shown.  Indeed, various embodiments of the invention
may be embodied in many different forms and should not be construed as limited to the embodiments set forth herein; rather, these embodiments are provided so that this disclosure will satisfy applicable legal requirements.  Like reference numerals refer
to like elements throughout.  As used herein, the terms "data," "content," "information," and similar terms may be used interchangeably to refer to data capable of being transmitted, received and/or stored in accordance with embodiments of the present
invention.  Thus, use of any such terms should not be taken to limit the spirit and scope of embodiments of the present invention.
<br/><br/> Additionally, as used herein, the term `circuitry` refers to (a) hardware-only circuit implementations (e.g., implementations in analog circuitry and/or digital circuitry); (b) combinations of circuits and computer program product(s) comprising
software and/or firmware instructions stored on one or more computer readable memories that work together to cause an apparatus to perform one or more functions described herein; and (c) circuits, such as, for example, a microprocessor(s) or a portion of
a microprocessor(s), that require software or firmware for operation even if the software or firmware is not physically present.  This definition of `circuitry` applies to all uses of this term herein, including in any claims.  As a further example, as
used herein, the term `circuitry` also includes an implementation comprising one or more processors and/or portion(s) thereof and accompanying software and/or firmware.  As another example, the term `circuitry` as used herein also includes, for example,
a baseband integrated circuit or applications processor integrated circuit for a mobile phone or a similar integrated circuit in a server, a cellular network device, other network device, and/or other computing device.
<br/><br/> As defined herein, a "computer-readable storage medium," which refers to a non-transitory physical storage medium (e.g., volatile or non-volatile memory device), can be differentiated from a "computer-readable transmission medium," which refers
to an electromagnetic signal.
<br/><br/> A method, apparatus and computer program product are provided according to an example embodiment of the present invention in order to facilitate user interaction with objects viewable via a display.  In this regard, the method, apparatus and
computer program product of an example embodiment may permit an operation to be performed upon at least one aspect associated with the object.  Although the method, apparatus and computer program product may be utilized in conjunction with a variety of
computing devices having associated displays upon which an object is viewable, the method, apparatus and computer program product of one example embodiment may facilitate user interaction with an object viewable via a pass-through display such that an
operation may be performed upon at least one aspect associated with the object viewable through the pass-through display.  The method, apparatus and computer program product of this embodiment may therefore permit the user to interact more readily with
objects in their vicinity.
<br/><br/> An example embodiment of the invention will now be described with reference to FIG. 1, in which certain elements of an apparatus 10 for facilitating interaction with an object viewable via a display are depicted.  The apparatus of FIG. 1 may be
employed, for example, in conjunction with, such as by being incorporated into or embodied by, various computing devices, such as a pass-through display or a mobile terminal.  However, it should be noted that the apparatus of FIG. 1 may also be employed
in connection with a variety of other computing devices and therefore, embodiments of the present invention should not be limited to application with the various examples of the computing devices that are described below.
<br/><br/> It should also be noted that while FIG. 1 illustrates one example of a configuration of an apparatus 10 for facilitating interaction with an object viewable via a display, numerous other configurations may also be used to implement embodiments
of the present invention.  As such, in some embodiments, although devices or elements are shown as being in communication with each other, hereinafter such devices or elements should be considered to be capable of being embodied within the same device or
element and thus, devices or elements shown in communication should be understood to alternatively be portions of the same device or element.
<br/><br/> Referring now to FIG. 1, the apparatus 10 for facilitating interaction with an object viewable via a display may include or otherwise be in communication with a processor 12, a memory device 14, a communication interface 16 and optionally a user
interface 18 and a camera 20.  In this regard, reference to a camera includes an embodiment that includes a single camera as well as an embodiment that includes a plurality of cameras, such as to facilitate construction of a wide angle image.  In some
embodiments, the processor (and/or co-processors or any other processing circuitry assisting or otherwise associated with the processor) may be in communication with the memory device via a bus for passing information among components of the apparatus. 
The memory device may be non-transitory and may include, for example, one or more volatile and/or non-volatile memories.  In other words, for example, the memory device may be an electronic storage device (e.g., a computer readable storage medium)
comprising gates configured to store data (e.g., bits) that may be retrievable by a machine (e.g., a computing device like the processor).  The memory device may be configured to store information, data, content, applications, instructions, or the like
for enabling the apparatus to carry out various functions in accordance with an example embodiment of the present invention.  For example, the memory device could be configured to buffer input data for processing by the processor.  Additionally or
alternatively, the memory device could be configured to store instructions for execution by the processor.
<br/><br/> The apparatus 10 may be embodied by a computing device, such as a pass-through display or a mobile terminal.  However, in some embodiments, the apparatus may be embodied as a chip or chip set.  In other words, the apparatus may comprise one or
more physical packages (e.g., chips) including materials, components and/or wires on a structural assembly (e.g., a baseboard).  The structural assembly may provide physical strength, conservation of size, and/or limitation of electrical interaction for
component circuitry included thereon.  The apparatus may therefore, in some cases, be configured to implement an embodiment of the present invention on a single chip or as a single "system on a chip." As such, in some cases, a chip or chipset may
constitute means for performing one or more operations for providing the functionalities described herein.
<br/><br/> The processor 12 may be embodied in a number of different ways.  For example, the processor may be embodied as one or more of various hardware processing means such as a coprocessor, a microprocessor, a controller, a digital signal processor
(DSP), a processing element with or without an accompanying DSP, or various other processing circuitry including integrated circuits such as, for example, an ASIC (application specific integrated circuit), an FPGA (field programmable gate array), a
microcontroller unit (MCU), a hardware accelerator, a special-purpose computer chip, or the like.  As such, in some embodiments, the processor may include one or more processing cores configured to perform independently.  A multi-core processor may
enable multiprocessing within a single physical package.  Additionally or alternatively, the processor may include one or more processors configured in tandem via the bus to enable independent execution of instructions, pipelining and/or multithreading.
<br/><br/> In an example embodiment, the processor 12 may be configured to execute instructions stored in the memory device 14 or otherwise accessible to the processor.  Alternatively or additionally, the processor may be configured to execute hard coded
functionality.  As such, whether configured by hardware or software methods, or by a combination thereof, the processor may represent an entity (e.g., physically embodied in circuitry) capable of performing operations according to an embodiment of the
present invention while configured accordingly.  Thus, for example, when the processor is embodied as an ASIC, FPGA or the like, the processor may be specifically configured hardware for conducting the operations described herein.  Alternatively, as
another example, when the processor is embodied as an executor of software instructions, the instructions may specifically configure the processor to perform the algorithms and/or operations described herein when the instructions are executed.  However,
in some cases, the processor may be a processor of a specific device (e.g., a pass-through display or a mobile terminal) configured to employ an embodiment of the present invention by further configuration of the processor by instructions for performing
the algorithms and/or operations described herein.  The processor may include, among other things, a clock, an arithmetic logic unit (ALU) and logic gates configured to support operation of the processor.
<br/><br/> Meanwhile, the communication interface 16 may be any means such as a device or circuitry embodied in either hardware or a combination of hardware and software that is configured to receive and/or transmit data from/to a network and/or any other
device or module in communication with the apparatus 10.  In this regard, the communication interface may include, for example, an antenna (or multiple antennas) and supporting hardware and/or software for enabling communications with a wireless
communication network.  Additionally or alternatively, the communication interface may include the circuitry for interacting with the antenna(s) to cause transmission of signals via the antenna(s) or to handle receipt of signals received via the
antenna(s).  In some environments, the communication interface may alternatively or also support wired communication.  As such, for example, the communication interface may include a communication modem and/or other hardware/software for supporting
communication via cable, digital subscriber line (DSL), universal serial bus (USB) or other mechanisms.
<br/><br/> In some embodiments, the apparatus 10 may include a user interface 18 that may, in turn, be in communication with the processor 12 to provide output to the user and, in some embodiments, to receive an indication of a user input.  As such, the
user interface may include a display and, in some embodiments, may also include a keyboard, a mouse, a joystick, a touch screen, touch areas, soft keys, a microphone, a speaker, or other input/output mechanisms.  Alternatively or additionally, the
processor may comprise user interface circuitry configured to control at least some functions of one or more user interface elements such as a display and, in some embodiments, a speaker, ringer, microphone and/or the like.  The processor and/or user
interface circuitry comprising the processor may be configured to control one or more functions of one or more user interface elements through computer program instructions (e.g., software and/or firmware) stored on a memory accessible to the processor
(e.g., memory device 14, and/or the like).
<br/><br/> In some example embodiments, the apparatus 10 may include an image capturing element, such as a camera 20, video and/or audio module, in communication with the processor 12.  The image capturing element may be any means for capturing an image,
video and/or audio for storage, display or transmission.  As used herein, an image includes a still image as well as an image from a video recording.  For example, in an example embodiment in which the image capturing element is a camera, the camera may
include a digital camera capable of forming a digital image file from a captured image.  As such, the camera may include all hardware (for example, a lens or other optical component(s), image sensor, image signal processor, and/or the like) and software
necessary for creating a digital image file from a captured image.  Alternatively, the camera may include only the hardware needed to view an image, while the memory device 14 of the apparatus stores instructions for execution by the processor in the
form of software necessary to create a digital image file from a captured image.  In an example embodiment, the camera may further include a processing element such as a co-processor which assists the processor in processing image data and an encoder
and/or decoder for compressing and/or decompressing image data.  The encoder and/or decoder may encode and/or decode according to, for example, a joint photographic experts group (JPEG) standard, a moving picture experts group (MPEG) standard, or other
format.
<br/><br/> The apparatus 10 of FIG. 1 may be embodied by or otherwise associated with a variety of different computing devices, each of which has an associated display.  For example, the apparatus may be embodied by or associated with a pass-through
display.  A pass-through display may be embodied in various manners.  For example, the pass-through display may be a near-eye display, such as a head worn display, through which the user may optically view a scene external to the near-eye display.  By
way of example, a near-eye display of one embodiment is shown in FIG. 2 in the form of a pair of eyeglasses 30.  The eyeglasses may be worn by a user such that the user may view a scene, e.g., a field of view, through the lenses of the eyeglasses. 
However, the eyeglasses of this embodiment may also be configured to present a visual representation of information upon the lenses so as to augment or supplement the user's view of the scene through the lenses of the eyeglasses.  The information
presented by the eyeglasses may augment the objects in the scene viewed through the eyeglasses, such as by identifying or otherwise providing more information regarding one or more of the objects viewed through the eyeglasses.  Alternatively, the
information presented by the eyeglasses may be unrelated to the objects in the scene viewed through the eyeglasses, but may otherwise provide information that may be of interest to the user.  Regardless of the type of information presented by the
eyeglasses, a pass-through display as exemplified by the eyeglasses of FIG. 2 may support augmented reality and other applications.  As another example, the pass-through display may be embodied by a windshield, a visor, a helmet or other type of display
through which a user optically views an image or a scene external to the display.  While examples of a pass-through display have been provided, a pass-through display may be embodied in a number of different manners with a variety of form factors, each
of which may permit a user to optically see through the display so as to view the user's surroundings and each of which may benefit from the method, apparatus and computer program product of an example embodiment of the present invention as described
below.
<br/><br/> By way of another example, the apparatus 10 of FIG. 1 may be embodied by or associated with a mobile terminal 40.  The mobile terminal may be embodied by any of a variety of computing devices including, but not limited to, a portable digital
assistant (PDA), mobile telephone, smartphone, pager, mobile television, gaming device, laptop computer, camera, tablet computer, touch surface, video recorder, audio/video player, radio, electronic book, positioning device (e.g., global positioning
system (GPS) device), or any combination of the aforementioned, and other types of voice and text communications systems.  Regardless of the type of computing device, the mobile terminal may include a display 42 and a user interface.  In the illustrated
embodiment, the user interface includes such as one or more keys 48 for receiving user input.  However, the mobile terminal need not include the keys and may, instead, include another type of user interface, such as a touch screen display.  The apparatus
embodied by or associated with the mobile terminal, such as shown in FIG. 3, may include a camera 20 so as to capture an image, such as an image of the surroundings proximate the mobile terminal.  The image may, in turn, be presented upon the display,
such as the image of a woman holding a pair of books that is presented upon the display of the embodiment of the mobile terminal in FIG. 3.
<br/><br/> In one embodiment, the apparatus 10 may be embodied by the computing device, such as the pass-through display or the mobile terminal 30.  Alternatively, the apparatus may be embodied by a computing device that is remote from the display upon
which the object is viewable, but that is in communication therewith, such as via wireless communication, e.g., via Bluetooth communication, Wi-Fi or another wireless network, or via wired communication, utilizing the communication interface 16.
<br/><br/> Regardless of the embodiment, an object is viewable via a display of a computing device, such as the computing device that embodies or is otherwise associated with the apparatus 10 of FIG. 1.  With respect to a pass-through display, an object
may be viewable through the display, such as an object proximate the user at which the user is looking.  Alternatively, in an embodiment in which a mobile terminal 30 includes or is otherwise associated with a camera that captures an image, the image
captured by the camera, such as an image of the user's surroundings, may be presented upon the display such that a user may view an object within the image presented upon the display.  One example of such an image is presented in FIG. 3.  As such, an
object viewed by a user may be viewable through the display or upon the display.
<br/><br/> The method, apparatus 10 and computer program product of an example embodiment will now be described in conjunction with the operations illustrated in FIG. 4.  In this regard, the apparatus 10 may include means, such as the processor 12, the
user interface 18 or the like, for identifying an object viewable via the display.  See block 52 of FIG. 4.  The object may be identified in various manners.  In one embodiment in which the object is a person, the apparatus, such as the processor, may be
configured to employ facial recognition in order to identify the person.  In this regard, FIG. 5 depicts an image that is viewable via a display, such as an image viewable through a pass-through display or an image presented upon the display of a mobile
terminal.  In the embodiment of FIG. 5, the object is a person, who has been recognized by facial recognition as indicated by the box that bounds the person's face.
<br/><br/> Alternatively, in an instance in which the object is a building, landmark or other structure, the apparatus 10, such as the processor 12, may be configured to identify the object based upon the geographic location of the object, such as may be
estimated by the geographic location of the computing device as provided, for example, by a positioning sensor, and with reference to a database, such as may be maintained by memory 14, which identifies various buildings, landmarks or other structures by
geographic location.  In this embodiment, the positioning sensor may include, for example, a global positioning system (GPS) sensor, an assisted global positioning system (A-GPS) sensor, a Bluetooth (BT)-GPS mouse, other GPS or positioning receivers or
the like.  The positioning sensor may be capable of determining a location of the computing device that embodies the apparatus, such as, for example, longitudinal and latitudinal directions of the computing device, or a position relative to a reference
point such as a destination or start point.
<br/><br/> Alternatively, the apparatus 10, such as the processor 12, may be configured to identify a building, landmark or other structure by comparing the appearance of the building, landmark or other structure that is viewable via the display with a
database, such as may be maintained by memory 14, of images of various buildings, landmarks and other structures in order to identify a building, landmark or other structure that has a matching appearance.  The apparatus, such as the processor, the user
interface 18 or the like, may be configured to identify the object viewable via the display in other manners.  For example, the apparatus, such as the processor, the user interface or the like, may be configured to identify the object by permitting the
user to provide an identification of the object, such as by entering the name of the object or other indicia identifying the object.
<br/><br/> Regardless of the manner in which the object is identified, the apparatus 10, such as the processor 12, the user interface 18 or the like, may cause a representation of the identification of the object, such as the name of the object or other
indicia identifying the object, to be presented upon the display in proximity to the object.  For example, the person that was identified in the embodiment of FIG. 5 is identified as Lily Wang.
<br/><br/> As shown in FIGS. 2, 3 and 5, the apparatus 10 of one embodiment may include means, such as the processor 12, the user interface 18 or the like, for causing one or more operation indicators to be presented upon the display.  See block 50 of FIG.
4.  In this regard, the circles 32, 44, 60 that are presented upon the displays of FIGS. 2, 3 and 5, respectively, are representative of operation indicators.  Operation indicators may take various forms, such as icons, alpha-numeric designations or the
like.  Regardless of the manner in which an operation indicator is presented, an operation indicator is representative of an operation that may be performed upon selection of the operation indicator.  By way of example, a short message service (SMS)
operation indicator, as shown in FIG. 5, may cause an SMS message to be generated upon selection of the SMS operation indicator.  As another example, a <b><i>Facebook</i></b> operation indicator may cause a <b><i>Facebook</i></b> post to be generated upon actuation of the <b><i>Facebook</i></b>
operation indicator.  As a further example, a camera operation indicator may cause the image viewable via the display to be captured upon actuation of the camera operation indicator.  Additionally, a gallery operation indicator may cause all relevant
pictures and/or videos that include a particular person to be listed upon actuation of the gallery operation indicator.  An outlook operation indicator may cause an electronic mail message to be generated upon selection of the outlook operation indicator
and, in some instances may also cause one or more history dialogues with a particular person to be shown.  Further, a social networking operation indicator may cause to a listing of the common friends between the user and a particular person to be shown
upon selection of the social networking operation indicator.  While certain examples of an operation indicator are provided above, there may be many other types of operation indicators with the foregoing provided by way of example, but not of limitation.
<br/><br/> As shown in FIGS. 3 and 5, the operation indicators 44, 60 may be offset from the object that is viewable via the display.  In this regard, the object indicators may not be aligned with or overlap the object, but, instead, may be spaced apart
from the object.  However, the apparatus of one embodiment includes means, such as the processor 12 or the like, for thereafter determining that the object has become aligned with the operation indicator.  See block 54 of FIG. 4.  In this regard, the
object is aligned with the operation indicator in an instance in which the operation indicator overlies, either fully or at least partially, the object or a designated region of the object.  In this regard, FIG. 5 depicts a scene or an image that is
viewable via the display, e.g., a scene viewable through a pass-through display or an image presented upon the display of a mobile terminal.  As described above, the object has been identified, such as by facial recognition, as Lily Wang.  In addition,
the two operation indicators 60 are offset from and therefore are not aligned with the object.  In FIG. 6, however, the SMS operation indicator is in alignment with the object and, more particularly, with a designated portion of the object, that is, the
face of Lily Wang that was the subject of facial recognition as designated by the box bounding her face.  Thus, in an instance in which a portion of the object viewable via the display is designated, such as the face of a person that was identified via
facial recognition as designated by a bounding box, the alignment of an operation indicator with the object may be accomplished by aligning the operation indicator with the designated portion of the object.  Alternatively, in an instance in which no
portion of the object viewable via the display is specifically designated, the alignment of the operation indicator with the object may be accomplished by aligning the operation indicator with any portion of the object.
<br/><br/> In accordance with an example embodiment to the present invention, the object may be aligned with the operation indicator as a result of movement of the display relative to the object as opposed to, for example, movement of the operation
indicator, such as the result of a drag operation, relative to the display.  For example, in an instance in which the display is a pass-through display, the user wearing the pass-through display, such as the user wearing the eyeglasses 30 of FIG. 2, may
move their head in such a manner that the object viewable via the display becomes aligned with the operation indicator 32.  Alternatively, in an instance in which the display is the display of a mobile terminal 40 as shown in FIG. 3, the mobile terminal
may be moved or repositioned such that the image captured by the camera 20 following the repositioning or the mobile terminal includes the object in a position that is now aligned with the operation indicator.  As will be noted by the foregoing examples,
the alignment of the object with respect to the operation indicator is brought about as a result of movement of the display, such as the eyeglasses of FIG. 2 or the mobile terminal of FIG. 3, relative to the object and the relative position of the
operation indicators upon the display remain the same, that is, the relative position of the operation indicators upon the display are independent of movement of the display relative to the object.  As such, the relative position of object with respect
with respect to the display is changed as a result of the relative movement of the display with respect to the object as exemplified by a comparison of FIGS. 5 and 6, without repositioning the operation indicators upon the display.
<br/><br/> The apparatus 10 may also include means, such as the processor 12, the user interface 18 or the like, for performing an operation associated with the operation indicator in response to determining that the object is aligned with the operation
indicator.  See block 58 of FIG. 4.  In this regard, the apparatus, such as the processor, may be configured to perform the operation upon at least one aspect associated with the object.  The operations that are performed in response to determining that
the object is aligned with the operation indicator depends upon the operation indicator with which the object has become aligned.  In regards to the SMS operation indicator, a SMS message may be generated as shown in FIG. 7.  Alternatively, the operation
performed in response to the object being aligned with a <b><i>Facebook</i></b> operation indicator may be the generation of a <b><i>Facebook</i></b> post.  As another example, in an instance in which the object is aligned with the gallery operation indicator, all the pictures
and/or videos including the object may be listed on the display so that the user can easily browse them.  Additionally, upon an object becoming aligned with an outlook operation indicator, an electronic mail message may be generated and, in some
instances one or more history dialogues may be shown.  Furthermore, in an instance in which the object becomes aligned with the social networking operation indicator, a listing of friends may be presented.  As such, various operations may be performed
depending upon the operation associated with or represented by the operation indicator with which the object becomes aligned.
<br/><br/> Regardless of the operation that is associated with operation indicator with which the object becomes aligned, the operation that is performed is performed upon at least one aspect associated with the object, such as based upon the name of the
object, an appearance of the object, contacts associated with the object, metadata associated with the object or other aspects associated with the object.  With respect to the SMS and <b><i>Facebook</i></b> operation indicators, a message or a post may be generated
that is directed to or otherwise associated with the person whose image has become aligned with the operation indicator.  By way of other examples, in an instance in which an image of a person is aligned with the gallery operation indicator, all the
pictures and/or videos including the person may be listed to facilitate browsing.  Additionally, upon an image of a person becoming aligned with an outlook operation indicator, an electronic mail message may be generated that is addressed to the person
and, optionally, one or more history dialogues of message exchanges between the person and the user may be shown.  Furthermore, in an instance in which an image of a person becomes aligned with the social networking operation indicator, a listing of
friends that are common to both the person and the user may be presented.  Although several examples are provided above, other types of operations may be performed based upon at least one aspect associated with the object in other embodiments.
<br/><br/> In one embodiment, the performance of an operation associated with the operation indicator is not only based upon a determination that the object is aligned with the operation indicator, but also upon a selection of the object so as to avoid
inadvertently performing operations as the computing device is repositioned and the object viewable via the display becomes aligned with various operation indicators.  As such, the apparatus 10 may include means, such as the processor 12, the user
interface 18 or the like, for providing for selection of the object.  See block 56 of FIG. 4.  The object may be selected in various manners.  As shown in FIGS. 2 and 3, a cursor 34, 46 may be caused to be presented upon the display.  As such, the cursor
may be positioned upon the object and then selected by the user in order to correspondingly select the object.  The cursor of this embodiment may be repositioned in various manners including by dragging and dropping the cursor via input provided via a
touch screen display in the embodiment of FIG. 3 or via gestures, such as hand gestures that are captured by a camera 20 and interpreted by the processor 12 in conjunction with a pass-through display of the type shown in FIG. 2.  However, the object may
be selected in still other manners including, for example, by audible command, such as by saying the name of the person who has been identified as the object through facial recognition.  In this embodiment, only once the object has been selected and a
determination has been made that the object is aligned with the operation indicator, the apparatus, such as the processor, may cause the operation to be performed upon at least one aspect associated with the object.
<br/><br/> The method, apparatus 10 and computer program product of an example embodiment of the present invention therefore facilitate the interaction by a user with an object that is viewable via a display, such as viewable through a pass-through display
or viewable within an image presented upon a display of a mobile terminal 30.  As such, a user may cause an operation to be performed that is based upon at least one aspect associated with the object.  The method, apparatus and computer program product
of an example embodiment may therefore provide an intuitive mechanism for facilitating user interaction with an object viewable via a display.
<br/><br/> As described above, FIG. 4 illustrates a flowchart of an apparatus 10, method, and computer program product according to example embodiments of the invention.  It will be understood that each block of the flowchart, and combinations of blocks in
the flowchart, may be implemented by various means, such as hardware, firmware, processor, circuitry, and/or other devices associated with execution of software including one or more computer program instructions.  For example, one or more of the
procedures described above may be embodied by computer program instructions.  In this regard, the computer program instructions which embody the procedures described above may be stored by a memory device 14 of an apparatus employing an embodiment of the
present invention and executed by a processor 12 of the apparatus.  As will be appreciated, any such computer program instructions may be loaded onto a computer or other programmable apparatus (e.g., hardware) to produce a machine, such that the
resulting computer or other programmable apparatus implements the functions specified in the flowchart blocks.  These computer program instructions may also be stored in a computer-readable memory that may direct a computer or other programmable
apparatus to function in a particular manner, such that the instructions stored in the computer-readable memory produce an article of manufacture the execution of which implements the function specified in the flowchart blocks.  The computer program
instructions may also be loaded onto a computer or other programmable apparatus to cause a series of operations to be performed on the computer or other programmable apparatus to produce a computer-implemented process such that the instructions which
execute on the computer or other programmable apparatus provide operations for implementing the functions specified in the flowchart blocks.
<br/><br/> Accordingly, blocks of the flowchart support combinations of means for performing the specified functions and combinations of operations for performing the specified functions for performing the specified functions.  It will also be understood
that one or more blocks of the flowchart, and combinations of blocks in the flowchart, can be implemented by special purpose hardware-based computer systems which perform the specified functions, or combinations of special purpose hardware and computer
instructions.
<br/><br/> In some embodiments, certain ones of the operations above may be modified or further amplified.  Furthermore, in some embodiments, additional optional operations may be included, such as illustrated by the blocks having a dashed outline in FIG.
4.  Modifications, additions, or amplifications to the operations above may be performed in any order and in any combination.
<br/><br/> Many modifications and other embodiments of the inventions set forth herein will come to mind to one skilled in the art to which these inventions pertain having the benefit of the teachings presented in the foregoing descriptions and the
associated drawings.  Therefore, it is to be understood that the inventions are not to be limited to the specific embodiments disclosed and that modifications and other embodiments are intended to be included within the scope of the appended claims. 
Moreover, although the foregoing descriptions and the associated drawings describe example embodiments in the context of certain example combinations of elements and/or functions, it should be appreciated that different combinations of elements and/or
functions may be provided by alternative embodiments without departing from the scope of the appended claims.  In this regard, for example, different combinations of elements and/or functions than those explicitly described above are also contemplated as
may be set forth in some of the appended claims.  Although specific terms are employed herein, they are used in a generic and descriptive sense only and not for purposes of limitation.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09298970&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D936%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D19%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D936%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D19%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209298970"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9298970&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D936%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D19%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209298970">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=936&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=936&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=936&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=20&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=935&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=937&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></body></html>