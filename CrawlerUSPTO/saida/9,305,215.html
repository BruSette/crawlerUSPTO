<html><head>
<base target="_top"/>
<title>United States Patent: 9305215</title></head>
<!---BUF1=9305215
BUF7=2016
BUF8=61485
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=890&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=17&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=890&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=890&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=889&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=891&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D890%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D18%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209305215"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9305215&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D890%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D18%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209305215">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09305215&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D890%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D18%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>890</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,305,215</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
         Park
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     April 5, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">Apparatus, method and computer readable recording medium for analyzing
     video using image captured from video
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> An apparatus for analyzing a video capture screen includes: a video frame
     extracting unit extracting at least one frame from a video having a
     plurality of frames; an extracted frame digitizing unit digitizing
     features of each of the at least one frame extracted by the video frame
     extracting unit; an image digitizing unit digitizing features of at least
     one collected search target image; an image comparing and searching unit
     comparing the search target image with the at least one frame extracted
     from the plurality of frames by digitized values of the collected search
     target image and the at least one frame; and a search result processing
     unit mapping related information of the collected search target image to
     a frame coinciding with the search target image and storing the related
     information in a database, when the extracted at least one frame
     coinciding with the search target image is present in a comparison
     result.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Park; Gunhan</b> (Seongnam-si, <b>KR</b>)<b>, Jung; Jeanie</b> (Seongnam-si, <b>KR</b>) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/>NHN Corporation</b> </td><td> <br/>Seongnam-si </td><td align="center"> <br/>N/A </td><td align="center"> <br/>KR </td> <td align="left">
</td> </tr> </tbody></table>
<!-- AANM>
~AANM NHN Corporation
~AACI Seongnam-si
~AAST N/A
~AACO KR
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b>NHN Corporation</b>
 (Seongnam-si, 
<b>KR</b>)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>50100077
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>13/940,984</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>July 12, 2013</b></td></tr>
     </tbody></table>
<hr/> <center><b>Prior Publication Data</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Document Identifier</u></b></th><th align="center" scope="col"><b><u>Publication Date</u></b></th></tr><tr><td align="center"> </td><td align="center"> US 20140050400 A1</td><td align="center">Feb 20, 2014</td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table>
     <hr/>
<center><b>Foreign Application Priority Data</b></center> <hr align="center" width="30%"/> <table width="100%"> <tbody><tr><th scope="col"></th><td></td><td></td><th scope="col"></th><td></td></tr> <tr><td align="center">
Aug 17, 2012
[KR]</td><td></td><td></td><td align="left">
10-2012-0089860</td></tr><tr><td align="center">

</td>
</tr> </tbody></table>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">G06K 9/00536 (20130101); G06F 17/30244 (20130101); G06K 9/00711 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">G06K 9/00 (20060101); G06F 17/30 (20060101)</td></tr>
       <tr><td align="left" valign="top" width="30%"><b>Field of Search: </b></td>
       <td align="right" valign="top" width="70%">
       
 ;382/305
       </td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9305215">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6751776">6751776</a></td><td align="left">
June 2004</td><td align="left">
Gong</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6760042">6760042</a></td><td align="left">
July 2004</td><td align="left">
Zetts</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7477780">7477780</a></td><td align="left">
January 2009</td><td align="left">
Boncyk</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7590309">7590309</a></td><td align="left">
September 2009</td><td align="left">
Steffan</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8196045">8196045</a></td><td align="left">
June 2012</td><td align="left">
Chandratillake</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F9037979">9037979</a></td><td align="left">
May 2015</td><td align="left">
Park</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20020033842&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2002/0033842</a></td><td align="left">
March 2002</td><td align="left">
Zetts</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20120177292&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2012/0177292</a></td><td align="left">
July 2012</td><td align="left">
Cheon</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20150279427&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2015/0279427</a></td><td align="left">
October 2015</td><td align="left">
Godfrey</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
       <i>Primary Examiner:</i> Sherali; Ishrat I
<br/>
       <i>Attorney, Agent or Firm:</i> <coma>Greer Burns &amp; Crain Ltd.
<br/>
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>What is claimed is: <br/><br/> 1.  An apparatus for analyzing a video capture screen, the apparatus comprising: at least one storage device;  and a processor configured to execute the functions of: a video
frame extracting unit configured to extract at least one frame from a video having a plurality of frames;  an extracted frame digitizing unit configured to digitize features of each of the at least one frame extracted by the video frame extracting unit; 
an image digitizing unit configured to digitize features of at least one collected search target image;  an image comparing and searching unit configured to compare the collected search target image with the at least one frame extracted from the
plurality of frames by digitized values of the collected search target image and the at least one frame;  and a search result processing unit configured to map related information of the collected search target image to a frame coinciding with the
collected search target image and storing the related information in a database, when the extracted at least one frame coinciding with the collected search target image is present in a comparison result.
<br/><br/> 2.  The apparatus of claim 1, further comprising a representative frame extracting unit configured to determine one frame selected among the at least one frame from the video to be a representative frame in a case in which an image difference
among a plurality of continuous frames extracted by the video frame extracting unit is in a preset range, wherein the extracted frame digitizing unit is configured to digitize the representative frame determined by the representative frame extracting
unit and store the digitized representative frame in the database.
<br/><br/> 3.  The apparatus of claim 2, wherein the representative frame extracting unit is configured to determine the one frame selected among the plurality of continuous frames to be the representative frame in a case in which a difference value in a
gray histogram among the plurality of continuous frames is a predetermined value or less.
<br/><br/> 4.  The apparatus of claim 2, wherein the representative frame extracting unit is configured to determine at least one frame to be the representative frame per a preset time.
<br/><br/> 5.  The apparatus of claim 1, wherein the extracted frame digitizing unit is configured to digitize invariant points of the at least one extracted frame and then represent the invariant points by a frequency of visual words converted into
predefined cluster values.
<br/><br/> 6.  The apparatus of claim 5, wherein the image digitizing unit is configured to digitize invariant points of the at least one collected search target image and then represent the invariant points by a frequency of visual words converted into
predefined cluster values.
<br/><br/> 7.  The apparatus of claim 6, wherein the image comparing and searching unit is configured to compare the visual words of the at least one extracted frame with the visual words of the at least one collected search target image and determine that
corresponding frames and images coincide with each other when a number of same visual words is a preset number or more.
<br/><br/> 8.  The apparatus of claim 1, wherein the related information of the collected search target image is any one or more selected among a source in which the collected search target image is posted, a number of retweets of the collected search
target image on a social network service (SNS), a number of replies, a number of recommendations, and evaluation information.
<br/><br/> 9.  The apparatus of claim 1, further comprising: a result analysis processing unit configured to automatically calculate statistical values for the extracted at least one frame for a specific video from the comparison result stored by the
search result processing unit to generate an analysis result.
<br/><br/> 10.  The apparatus of claim 9, wherein the result analysis processing unit is configured to provide information selected from a number of citations in a news, a blog, an SNS site referencing a corresponding image in the video and corresponding
link information with respect to the extracted at least one frame of the video according to a result analysis request.
<br/><br/> 11.  The apparatus of claim 9, wherein the result analysis processing unit is configured to calculate and provide rankings of the extracted at least one frame from a number of citations in a news, a blog, an SNS site referencing a corresponding
image in the video with respect to the respective frames of the specific video according to a result analysis request.
<br/><br/> 12.  A method that uses a processor to analyze a video capture screen, the method comprising: extracting at least one frame from a video having a plurality of frames;  digitizing features of the extracted at least one frame;  digitizing features
of at least one collected search target image;  comparing, by the processor, the collected search target image with the at least one frame extracted from the plurality of frames by digitized values of the collected search target image and the at least
one frame;  and mapping related information of the collected search target image to a frame coinciding with the collected search target image and storing the related information in a database, when the extracted at least one frame coinciding with the
collected search target image is present in a comparison result.
<br/><br/> 13.  The method of claim 12, further comprising, after the at least one extracting of the frame, determining one frame selected among the plurality of frames to be a representative frame in a case in which an image difference among a plurality
of extracted continuous frames is in a preset range, wherein in the digitizing of the features of the extracted frames, the determined representative frame is digitized.
<br/><br/> 14.  The method of claim 13, wherein in the determining, one frame selected among the plurality of continuous frames is determined to be the representative frame in a case in which a difference value in a gray histogram among the plurality of
frames is a predetermined value or less.
<br/><br/> 15.  The method of claim 13, wherein in the determining, at least one frame is determined to be the representative frame per a preset time.
<br/><br/> 16.  The method of claim 12, wherein in the digitizing of the features of the extracted frames, invariant points of the at least one extracted frame are digitized and are then represented by a frequency of visual words converted into predefined
cluster values.
<br/><br/> 17.  The method of claim 16, wherein in the digitizing of the feature of the at least one collected search target image, invariant points of the at least one collected search target image are digitized and are then represented by a frequency of
visual words converted into predefined cluster values.
<br/><br/> 18.  The method of claim 17, wherein in the comparing, the visual words of the at least one extracted frame are compared with the visual words of the at least one collected search target image and the corresponding frames and images are
determined to coincide with each other when the number of same visual words is a preset number or more.
<br/><br/> 19.  The method of claim 12, further comprising, after the storing, automatically calculating statistical values for the extracted at least one frame for a specific video from the stored comparison result to generate an analysis result.
<br/><br/> 20.  A non-transitory computer readable medium comprising an executable program which, when executed in a computer, performs a method for analyzing a video using the image captured from the video, the method comprising: extracting at least one
frame from a video having a plurality of frames;  digitizing features of the extracted at least one frame;  digitizing features of at least one collected search target image;  comparing the collected search target image with the at least one frame
extracted from the plurality of frames by digitized values of the collected search target image and the at least one frame;  and mapping related information of the collected search target image to a frame coinciding with the collected search target image
and storing the related information in a database, when the extracted at least one frame coinciding with the collected search target image is present in a comparison result. <hr/> <center><b><i>Description</i></b></center> <hr/> <br/><br/>CROSS-REFERENCE TO
RELATED APPLICATIONS
<br/><br/> This application claims priority from and the benefit of Korean Patent Application No. 10-2012-0089860, filed on Aug.  17, 2012, which is hereby incorporated by reference for all purposes as if fully set forth herein.
<br/><br/>BACKGROUND
<br/><br/> The present invention relates to an apparatus for analyzing an image captured from a video, and more particularly, to an apparatus, a method, and a computer readable recording medium for analyzing a video using a captured image, for collecting
screen images from a video through a network and analyzing information related to the respective collected images.
<br/><br/> Generally, the Internet is an open network configured to apply a common protocol called a transfer control protocol/Internet protocol (TCP/IP) to opponent computers that enables anybody anywhere in the world to freely access and use the
computers.  Users may use various services such as transmission of multimedia information in accordance with the development of a compression technology, transmission of an e-mail, transmission of a file, the World Wide Web (WWW), and the like, as well
as transmission of basic character information, through the Internet.
<br/><br/> As described above, in accordance with the rapid increase in the use of the Internet domestically and globally, the importance of the Internet as a strategic tool for increasing efficiency and productivity over all of the existing industries has
rapidly increased.  New business opportunities through the Internet have also been continuously created, fields of the Internet have expanded, and operators using the Internet have also gradually increased.
<br/><br/> Meanwhile, recently, in accordance with the diffusion of digital apparatuses such as smart phones, and the like, a large amount of private video contents (for example, user created content (UCC)) have been created, and it has been generalized to
view television broadcasting, movie contents, and the like, on the web through a video on demand (VOD) service, or the like.
<br/><br/> In order to allow these video contents to be smoothly consumed, various video meta information including information on a specific scene is required so that search and browsing for desired contents are possible.  For example, in a current video
search or service, meta information in a text form related to a video, such as the title, the date, the number of views, and the like, of the video, has been mainly used, and the popularity of a video has been recognized using the number of views, and
the like, of the corresponding video.
<br/><br/> However, in a method of searching a video using only the meta information in a text form as described above, a popular scene in a corresponding video that should be viewed may not be recognized, and the number of discussions about the video on
the Internet (for example, other websites), except for the case in which the video is directly linked and posted, may also not be recognized.
<br/><br/> In addition, cases in which users upload a specific screen image captured from the video, rather than the video itself, on the web and have discussions about the contents of the corresponding video with other users on a social network service
(SNS) sites such as <b><i>Facebook,</i></b> Twitter, or the like, or their blogs or homepages through the specific screen image has been frequently generated.
<br/><br/> Particularly, in the case of contents requiring a copyright and a log-in in order to be reproduced, it is difficult to directly post and attach the contents on a website, and it is not easy to acquire a unique uniform resource locator (URL) for
a specific scene of a video about which the user wants to talk.  Therefore, even in news, only a specific scene of a broadcasting or a movie, instead of the corresponding video, is captured and utilized as an article.
<br/><br/> As described above, information including rich meta information related to a video distributed together with an image captured from the video has not been directly connected to the corresponding video.
<br/><br/> Meanwhile, as a technology for providing related information on video contents, a contents service method capable of generating source information in a video literary work, storing the source information in a database, and calculating similarity
between the source information and scene data to extract a source has been disclosed in Korean Patent Registration No. 10-0857381 (Patent Document 1) entitled "Content Service System and Method" and filed by NHN Inc.
<br/><br/> However, these methods according to the related art simply provide only the meta information on a video and do not provide related information on images in which main scenes are captured.
<br/><br/> Therefore, a method of effectively providing various related information on each image captured from a video to increase utilization of the video has been demanded.
<br/><br/>BRIEF SUMMARY OF THE INVENTION
<br/><br/> An object of the present invention is to provide an apparatus, a method, and a computer readable recording medium for analyzing a video using a captured image capable of providing various meta information on the video by comparing the respective
frames extracted from the video with the captured image collected by an external website, or the like.
<br/><br/> Another object of the present invention is to provide an apparatus, a method, and a computer readable recording medium for analyzing a video using a captured image capable of analyzing information from news, a blog, a social network service
(SNS) post, and the like, in which the respective frames extracted from the video are compared with the captured image collected by an external website, or the like.
<br/><br/> Still another object of the present invention is to provide an apparatus, a method, and a computer readable recording medium for analyzing a video using a captured image capable of analyzing social rankings, such as what frame in the video was
the largest issue or what video contents have gained popularity in an SNS by comparing the respective frames extracted from the video with the captured images collected by an external website, or the like.
<br/><br/> A characteristic configuration of the present invention for accomplishing the objects of the present invention as described above and unique effects of the present invention will be described below.
<br/><br/> According to an exemplary embodiment of the present invention, there is provided an apparatus for analyzing a video capture screen, the apparatus comprising: at least one storage device, a processor configured to execute the functions of a video
frame extracting unit configured to extract at least one frame from a video having a plurality of frames; an extracted frame digitizing unit configured to digitize features of each of the at least one frame extracted by the video frame extracting unit;
an image digitizing unit configured to digitize features of at least one collected search target image; an image comparing and searching unit configured to compare the search target image with the at least one frame extracted from the plurality of frames
by digitized values of the collected search target image and the at least one frame; and a search result processing unit configured to map related information of the collected search target image to a frame coinciding with the search target image and
storing the related information in a database, when the at least one extracted frame coinciding with the search target image is present in a comparison result.
<br/><br/> The apparatus may further comprise: a representative frame extracting unit configured to determine one frame selected among the at least one frame to be a representative frame in a case in which an image difference among a plurality of
continuous frames extracted by the video frame extracting unit is in a preset range, wherein the extracted frame is configured to digitize unit digitizes the representative frame determined by the representative frame extracting unit and store the
digitized representative frame in the database.
<br/><br/> The representative frame extracting unit may be configured to determine one frame selected among the plurality of continuous frames to be the representative frame in a case in which a difference value in a gray histogram among the plurality of
continuous frames is a predetermined value or less.
<br/><br/> The representative frame extracting unit may be configured to determine at least one frame to be the representative frame per a preset time.
<br/><br/> The extracted frame digitizing unit may be configured to digitize invariant points of the at least one extracted frame and then represent the invariant points by a frequency of visual words converted into predefined cluster values.
<br/><br/> The image digitizing unit may be configured to digitize invariant points of the at least one collected search target image and then represent the invariant points by a frequency of visual words converted into predefined cluster values.
<br/><br/> The image comparing and searching unit may be configured to compare the visual words of the at least one extracted frame with the visual words of the at least one collected search target image and determine that corresponding frames and images
coincide with each other when a number of same visual words is a preset number or more.
<br/><br/> The related information of the collected search target image may be any one or more selected among a source in which the collected search target image is posted, a number of retweets of the collected search target image on a social network
service (SNS), a number of replies, a number of recommendations, and evaluation information.
<br/><br/> The apparatus may further comprise a result analysis processing unit configured to automatically calculate statistical values for the extracted at least one frame for a specific video from the comparison result stored by the search result
processing unit to generate an analysis result.
<br/><br/> The result analysis processing unit may be configured to provide information selected from a number of citations in news, a blog, an SNS site referencing a corresponding image in the video and corresponding link information with respect to the
extracted at least one frame of the video according to a result analysis request.
<br/><br/> The result analysis processing unit may be configured to calculate and provide rankings of the extracted at least one frame from a number of citations in news, a blog, an SNS site referencing a corresponding image in the video with respect to
the respective frames of the specific video according to a result analysis request.
<br/><br/> According to another exemplary embodiment of the present invention, there is provided a method that uses a processor to analyze a video capture screen, the method comprising: extracting at least one frame from a video having a plurality of
frames; digitizing features of the extracted at least one frame; digitizing features of at least one collected search target image; comparing, by the processor, the search target image with the at least one frame extracted from the plurality of frames by
digitized values of the collected search target image and the at least one frame; and mapping related information of the collected search target image to a frame coinciding with the search target image and storing the related information in a database,
when the extracted at least one frame coinciding with the search target image is present in a comparison result.
<br/><br/> Information for providing the method of analyzing a video using a captured image may be stored in a server computer readable recording medium.  The recording medium may be any kind of recording media in which a program and data are stored so as
to be readable by a computer system.  The recording medium may include a read only memory (ROM), a random access memory (RAM), a compact disk (CD), a digital video disk (DVD) ROM, a magnetic tape, a floppy disk, an optical data storage, or the like, and
a medium implemented in the form of a carrier wave (for example, transmission through the Internet).  In addition, the recording medium may be distributed in a computer system connected by a network, such that a computer readable code may be stored and
executed in a distributed scheme. <br/><br/>BRIEF DESCRIPTION OF THE DRAWINGS
<br/><br/> FIG. 1 is a diagram showing a concept of analyzing a video using a captured image according to an exemplary embodiment of the present invention;
<br/><br/> FIG. 2 is a block diagram showing a detailed configuration of an apparatus for analyzing a video using a captured image according to the exemplary embodiment of the present invention;
<br/><br/> FIG. 3 is a graph showing a gray histogram of a specific frame for extracting a representative frame according to the exemplary embodiment of the present invention;
<br/><br/> FIG. 4 is a diagram showing a frame comparison for selecting a representative frame according to the exemplary embodiment of the present invention;
<br/><br/> FIG. 5 is a diagram showing a method for selecting a representative frame according to the exemplary embodiment of the present invention;
<br/><br/> FIG. 6 is a diagram showing a concept of a method for digitizing a representative frame according to the exemplary embodiment of the present invention;
<br/><br/> FIG. 7 is a flow chart showing a procedure for analyzing a video using a captured image according to the exemplary embodiment of the present invention; and
<br/><br/> FIGS. 8 to 10 are diagrams showing examples of a result of analyzing a video using a captured image according to the exemplary embodiment of the present invention.
<br/><br/>DETAILED DESCRIPTION OF THE ILLUSTRATED EMBODIMENTS
<br/><br/> The invention is described more fully hereinafter with reference to the accompanying drawings, in which exemplary embodiments of the invention are shown.  This invention may, however, be embodied in many different forms and should not be
construed as limited to the embodiments set forth herein.  Rather, these exemplary embodiments are provided so that this disclosure is thorough, and will fully convey the scope of the invention to those skilled in the art.  It will be understood that for
the purposes of this disclosure, "at least one of X, Y, and Z" can be construed as X only, Y only, Z only, or any combination of two or more items X, Y, and Z (e.g., XYZ, XZ, XYY, YZ, ZZ).  Throughout the drawings and the detailed description, unless
otherwise described, the same drawing reference numerals are understood to refer to the same elements, features, and structures.  The relative size and depiction of these elements may be exaggerated for clarity.
<br/><br/> The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the present disclosure.  As used herein, the singular forms "a", "an" and "the" are intended to include the plural
forms as well, unless the context clearly indicates otherwise.  Furthermore, the use of the terms a, an, etc. does not denote a limitation of quantity, but rather denotes the presence of at least one of the referenced item.  The use of the terms "first",
"second", and the like does not imply any particular order, but they are included to identify individual elements.  Moreover, the use of the terms first, second, etc. does not denote any order or importance, but rather the terms first, second, etc. are
used to distinguish one element from another.  It will be further understood that the terms "comprises" and/or "comprising", or "includes" and/or "including" when used in this specification, specify the presence of stated features, regions, integers,
steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, regions, integers, steps, operations, elements, components, and/or groups thereof.  Although some features may be described with
respect to individual exemplary embodiments, aspects need not be limited thereto such that features from one or more exemplary embodiments may be combinable with other features from one or more exemplary embodiments.
<br/><br/> In addition, embodiments described in the specification are wholly hardware, and may be partially software or wholly software.  In the specification, "unit", "module", "device", "system", or the like represents a computer related entity such as
hardware, combination of hardware and software, or software.  For example, in the specification, the unit, the module, the device, the system, or the like may be an executed process, a processor, an object, an executable file, a thread of execution, a
program, and/or a computer, but are not limited thereto.  For example, both of an application which is being executed in the computer and a computer may correspond to the unit, the module, the device, the system, or the like in the specification.
<br/><br/> Hereinafter, exemplary embodiments of the present invention will be described in detail with reference to the accompanying drawings.
<br/><br/> The present invention discloses a video capture image analyzing apparatus capable of providing various meta information on the video and a main frame of the video by collecting captured screen images through an external server, or the like, and
comparing the collected screen images with standard frames extracted from the video.
<br/><br/> That is, in the present invention, an image captured from a specific video is collected from an external server site (for example, a social network service (SNS)), a blog, a portal site, or the like, and is compared with a plurality of frames
extracted from a video, thereby judging to what frame of video the collected image corresponds.
<br/><br/> Therefore, in the case in which a frame coinciding with the collected specific image is searched, related information (for example, connected web contents information) of the collected image is mapped to the corresponding frame and is stored,
such that various analyses may be performed on video contents.  Here, the related information may be a source (that is, a posted website) of the contents, the number of posted corresponding images, or the like.  The video or a ranking of a specific image
in the video, the number of citations for each source, or the like, may be calculated as an analysis result through the related information.
<br/><br/> An `external server` in the following description indicates a server providing various web services separately from the video capture image analyzing apparatus performing a video analysis according to the present invention.  The external server
is physically and logically separated from the video capture image analyzing apparatus, and may be connected to the video capture image analyzing apparatus through a predetermined wired/wireless communication network.
<br/><br/> Hereinafter, exemplary embodiments of the present invention will be described in detail with reference to the accompanying drawings so that those skilled in the art may easily implement the spirit of the present invention.
<br/><br/> Concept of Analyzing Video Using Captured Image
<br/><br/> FIG. 1 is a diagram showing a concept of analyzing a video using a captured image according to an exemplary embodiment of the present invention.  Referring to FIG. 1, a plurality of frames are first extracted from at least one video.  Then,
feature values of the respective extracted frames are stored in a database.  In this case, it is preferable that representative frames among the plurality of frames are extracted and only the extracted representative frames are digitized and stored
according to the exemplary embodiment of the present invention.
<br/><br/> Meanwhile, a crawler collects a posting to which images captured from an external server (for example, a web server providing news, a blog, Twitter, <b><i>Facebook,</i></b> or the like) are attached and stores the collected posting as a search candidate in
the database.  Then, representative frames coinciding with the respective collected images among the representative frames extracted from the video are searched through a search engine.
<br/><br/> An issue scene database for each video and a post database for each scene mentioning a corresponding issue scene are built according to the search result.  In addition, various analyses such as calculation of a ranking of the video, selection of
an issue scene in the video, or the like, based on the search result may be performed.
<br/><br/> More specifically, in the case in which a representative frame coinciding with the collected specific image is searched, various information (for example, a source in which the corresponding collected image is posted, the number of references of
the corresponding representative frame, a degree of diffusion or preference of the corresponding image (for example, the number of retweets (RT), the number of replies, the number of recommendations, or the like) collected together in connection with the
collected image is mapped in the corresponding representative frame and is stored in the database.
<br/><br/> Therefore, as shown in FIGS. 8 to 10, an issue scene (that is, an issue frame) among representative frames extracted from a specific video, a list of posts mentioning the corresponding issue scene, rankings of videos, and the like, may be
provided.
<br/><br/> Hereinafter, an apparatus and a method for analyzing a video using a captured image according to an exemplary embodiment of the present invention will be described with reference to FIGS. 2 through 7.
<br/><br/> Apparatus for Analyzing Video Using Captured Image
<br/><br/> FIG. 2 is a block diagram showing a detailed configuration of an apparatus for analyzing a video using a captured image according to the exemplary embodiment of the present invention.  Referring to FIG. 2, the apparatus for analyzing a video
using a captured image according to the exemplary embodiment of the present invention may be configured to include a video frame extracting unit 210, a representative frame extracting unit 220, an extracted frame digitizing unit 230, an image information
collecting unit 240, an image digitizing unit 250, an image comparing and searching unit 260, a search result processing unit 270, a result analysis processing unit 280.  In addition, the apparatus for analyzing a video may include at least one database
such as a video contents database 291, a frame feature information database 292, an image feature information database 293, an image related information database 294, an image analysis information database 295.
<br/><br/> At least one video may be stored, for example, in a file form in the video contents database 291.  Here, the video may be configured to include a plurality of frames (for example, thirty frames in one second, or the like).  Any kind and any
format of video may be applied to the present invention.  For example, any kind of video such as a broadcasting video, a user created contents (UCC) video, a movie video, or the like, may be applied to the present invention.  In addition, various related
information (for example, meta information) on the stored video may be mapped to the stored image and be stored together in the video contents database 291.  For example, information such as the source, the reproduction time, the broadcasting time, the
broadcasting channel, the file format, and the like, of the video may be stored together.
<br/><br/> The video frame extracting unit 210 selects a specific video stored in the video contents database 291 and extracts a plurality of frames included in the selected video.
<br/><br/> The representative frame extracting unit 220 selects representative frames to be used as main frames among the plurality of frames extracted by the video frame extracting unit 210 according to the exemplary embodiment of the present invention. 
That is, in the case of the frames extracted by the vide frame extracting unit 210, since duplication (or similarity) between adjacent frames is strong, it is preferable to select and process representative frames according to the exemplary embodiment of
the present invention in order to increase efficiency of a database configuration and search.  Therefore, the present invention is not limited thereto, but may also be implemented so as to perform subsequent procedures on all frames in the extracted
video or a frame suitable for a specific condition in the video.
<br/><br/> In this case, a method of extracting the representative frames may be variously implemented.  For example, as in the exemplary embodiments to be described below, in the case in which a difference value in a gray histogram between frames is a
predetermined value or less, since images of the frames are images having strong duplication, the corresponding frames may be implemented to be excluded.  Exceptively, in the case in which a difference in a gray histogram between frames is small but the
images of the frames are different, the representative frames may be implemented to be forcibly selected per a predetermined time interval.  A detailed description thereof will be provided below with reference to FIGS. 3 to 5.
<br/><br/> As described above, when the representative frames are determined by the representative frame extracting unit 220, the extracted frame digitizing unit 230 digitizes features of the respective frames and stores the digitized features in the frame
feature information database 292.  A method of digitizing the features of the respective representative frames may be variously implemented.  For example, a well-known image analyzing method, or the like, may be used.  Although a visual word method is
described as the method of digitizing the features of the representative frames by way of example in the present invention, the present invention is not limited thereto.  A detailed description of the visual word method will be provided below with
reference to FIG. 6.
<br/><br/> The image information collecting unit 240 collects posts to which images (for example, images captured from a specific video) are attached from various external servers (for example, a portal site server, a social network service (SNS), a blog
server, and the like).  Here, the image information collecting unit 240 may be implemented using a crawler, or the like.  However, the present invention is not limited thereto.  The crawler, which is a kind of software used in a site opening a search
engine in order to automatically search and index various information on a web, is also called a spider, a bot, or an intelligent agent.
<br/><br/> The image information collecting unit 240 may additionally collect information such as a source in which the corresponding collected image is posted, a degree of diffusion or preference of the corresponding image (for example, the number of
retweets (RT), the number of replies, various evaluation information (for example, the number of recommendations, the number of positives/negatives (or the number of agreements/disagreements or the like) when it collects the posts to which the images are
attached.  Image related information additionally collected by the image information collecting unit 240 may be linked to the corresponding image and be stored in the image related information database 294.
<br/><br/> The image digitizing unit 250 digitizes the features of the respective images collected by the image information collecting unit 240 and stores the digitized features in the image feature information database 293.  Here, it is preferable that
the same method as the method in the extracted frame digitizing unit 230 is used as a digitizing method in the image digitizing unit 250.  However, the present invention is not limited thereto.  That is, a method of digitizing the features of the
respective collected images may be variously implemented.  For example, a well-known image analyzing method, or the like, may be used.  Although a visual word method is described as the method of digitizing the features of the collected images by way of
example in the present invention, the present invention is not limited thereto.
<br/><br/> As described above, when a procedure of digitizing the features of the representative frames and the features of the collected images is completed, the image comparing and searching unit 260 compares the digitized value of the specific collected
image with the digitized value of the representative frame stored in the frame feature information database 292.  As the comparison result, a representative frame having a value coinciding with the specific image or a representative frame having the most
similar value to that of the specific image is searched.  In the case in which a visual word method to be described below is used as the method of digitizing the features, a representative frame having the most visual words that are the same as the
specific image may be searched, or a frame in which a distance in image is in a predetermined distance when the number of appearances of the same words is defined as the distance between images may be searched.
<br/><br/> When the representative frame coinciding with the collected specific image is searched by the image comparing and searching unit 260, the search result processing unit 270 reads out information in connection with the image coinciding with the
searched corresponding frame from the image related information database 294 and stores the read out information in the image analysis information database 295.
<br/><br/> In addition, when a request for information on the video is received according to the exemplary embodiment of the present invention, the result analysis processing unit 280 automatically calculates statistical values for the respective main
frames for the specific video from the search result stored in the image analysis information database 295 to generate an analysis result and provides the generated analysis result to a user requesting the information.  For example, as shown in FIGS. 8
to 10, as a search result for the video, various related information (for example, the numbers of news, blogs, SNS sites, or the like, referencing a corresponding image in the video, corresponding link information, and the like) on main images (or
frames) of a specific video may be generated and provided to the user, according to the exemplary embodiment of the present invention.
<br/><br/> Method for Extracting Representative Frame
<br/><br/> FIG. 3 is a graph showing gray histograms of video frames for extracting a representative frame according to the exemplary embodiment of the present invention.  Referring to FIG. 3, the representative frame is extracted through a difference
value in a gray histogram between frames according to the exemplary embodiment of the present invention.
<br/><br/> That is, as shown in FIG. 3, the gray histogram for each frame is calculated, and it is judged that continuous frames are images having strong duplication (or similarity) in the case in which a difference value in a gray histogram between the
continuous frames is a preset value or less.  Therefore, only one frame is left as the representative frame, and remaining one or more duplicated frame is deleted.  Therefore, only a specific frame satisfying a condition among a number of frames included
in one video is extracted and processed as the representative frame, thereby making it possible to make a database configuration and search efficient.
<br/><br/> The gray histogram may be calculated, for example, as shown in FIG. 3.  More specifically, gray values of the respective pixels configuring the frame are calculated and whether or not they belong to a pre-divided section is calculated to
calculate a ratio.  Therefore, a distribution map of the gray values of the entire frame image may be calculated as shown in FIG. 3.
<br/><br/> FIGS. 4 and 5 show a method for selecting a representative frame using the gray histogram, or the like.  That is, referring to FIG. 4, it may be appreciated that a frame 410 and a frame 420, which are continuous frames, are images having strong
duplication since a difference in a gray histogram therebetween is a preset value or less.  Therefore, the frame 410 is excluded, and the frame 420 is selected as the representative frame.  Likewise, it may be appreciated that a frame 430 and a frame
440, which are continuous frames, are images having strong duplication since a difference in a gray histogram therebetween is a preset value or less.  Therefore, the frame 430 is excluded, and the frame 440 is selected as the representative frame. 
Likewise, in the case in which a difference in a gray histogram among three or more continuous frames is a preset value or less, only one frame may be left as the representative frame, and remaining two frames may be excluded.
<br/><br/> In a special case, there is a possibility that the difference in a gray histogram as described above will be the preset value or less, but the frames may be different images.  Therefore, according to the exemplary embodiment of the present
invention, even though the difference in a gray histogram is the preset value or less, the representative frame may be implemented to be forcibly selected once per a preset time (for example, one second).  That is, referring to FIG. 5, although all of
the frames 510 to 550 have a gray histogram of a preset value or less, images of the frames 520 and 530 may be different images.  Therefore, according to the exemplary embodiment of the present invention, the representative may be implemented to be
necessarily selected per a predetermined period.
<br/><br/> Method for Digitizing Representative Frame
<br/><br/> When the representative frames are selected by the method as shown in FIGS. 4 and 5, features of the respective selected frames are digitized and stored in the database.  FIG. 6 is a diagram showing a concept of a method for digitizing a
representative frame according to the exemplary embodiment of the present invention.
<br/><br/> That is, in order to compare the representative frames with the collected images according to the exemplary embodiment of the present invention, the respective representative frames and the respective collected images are digitized as the
feature values thereof, are stored, and are compared with each other.  As an example of a method of digitizing the image as the feature value, a visual word method may be used.
<br/><br/> The visual word method is a method of searching invariant points of the images or the frames, digitizing and representing the invariant points, and converting and using the digitized invariant points into predefined cluster values in order to be
easily searched.
<br/><br/> More specifically, as shown in FIG. 6, the respective clusters are divided into the predetermined numbers in advance using a plurality of training images.  Here, the respective unit clusters are called visual words.  Next, an invariant point is
obtained with respect to one image, and a descriptor is obtained as a statistical value capable of representing a feature of the invariant point.  Since the descriptor is a kind of vector, the closest cluster belonging to the vector is obtained and used
as a feature value of this point.  Through the above-mentioned process, as shown in FIG. 6, one image may be represented by one feature value histogram capable of being represented by frequencies of the visual words.
<br/><br/> The respective representative frames included in the video may be digitized and stored using the above-mentioned method.  The above-mentioned method is only an example for assisting in the understanding of the present invention.  Therefore, the
present invention is not limited thereto.
<br/><br/> Meanwhile, in FIG. 6, each of the clusters of the histogram representing the feature values may be considered as a visual word representative of the image (or the frame).  Finally, one image may be represented by at least one (preferably, a
plurality of) visual word(s) and frequencies of the respective visual words.
<br/><br/> Therefore, the images are collected by the image information collecting unit 240, such that when a query image is entered, the visual words representative of the collected image are obtained by, for example, the above-mentioned method, and are
compared with the visual words of the respective representative frames stored in the database.  In this case, a frame having the most same visual words may be considered as the same frame as the query image in a scheme used in an information search. 
That is, when a database having the video stored therein is stored in an inverted file form in a scheme similar to a general information search method, the image that is the most similar to the query image may be rapidly searched.  In addition, the
number of appearances of the same visual words may be defined as a distance between images, and only images between which the distance is a predetermined distance or less may be considered as the same frames.
<br/><br/> As described above, when the same frames as the respective collected images among the representative frames extracted from the video are searched, various related information (for example, a source in which the corresponding collected images are
posted, a degree of diffusion or preference of the corresponding images (for example, the number of retweets (RT), the number of replies, various evaluation information (for example, the number of recommendations, the number of positives/negatives (or
the number of agreements/disagreements, or the like) on the images collected together with the images is matched to the searched frames and is stored.
<br/><br/> Method for Analyzing Video Using Captured Image
<br/><br/> FIG. 7 is a flow chart showing a procedure for analyzing a video using a captured image according to the exemplary embodiment of the present invention.  Referring to FIG. 7, as described above, first, the representative frames are extracted from
the video contents and are stored in the database (S701).  Then, the features of the respective extracted representative frames are digitized and stored in the database (S702).
<br/><br/> In addition, the postings to which images are attached are collected from the external servers (S703), and the features of the collected images are digitized and stored in the database (S704).
<br/><br/> Then, according to the exemplary embodiment of the present invention, the representative frames coinciding with the collected images are searched from the respective representative frames stored in the database (S705).
<br/><br/> If the representative frames coinciding with the collected images are present (S706), the related information of the corresponding collected images is mapped to the searched representative frames and stored in the database (S707).
<br/><br/> Examples of Result of Analyzing Video
<br/><br/> FIGS. 8 to 10 are diagrams showing examples of a result of analyzing a video using a captured image according to the exemplary embodiment of the present invention.
<br/><br/> Referring to FIG. 8, in a web, news, an SNS, or the like, a video capture screen is attached and posted or referenced, or a ranking of a corresponding video or a corresponding capture screen of the video may be provided according to the number
of mentions.
<br/><br/> For example, when a television broadcasting title such as a `program A` is input as a search word in a search portal site as shown (800), statistical information and ranking information mentioned relating to the captured specific scenes in
broadcasting images in each date are provided according to the exemplary embodiment of the present invention, thereby making it possible to recognize what scene of what broadcasting date was the largest issue.  That is, a capture image 810 and related
information 820 of a specific scene may be sequentially provided according to the number of captures, postings, references, or mentions of the specific scene among the corresponding broadcasting videos.  For example, since an image captured in 14 seconds
in a broadcasting on May 23, 2012 among the broadcastings of the searched corresponding programs is a scene posted or referenced most, it is displayed on the top portion of the search result.  Here, the number of mentions 821 to 824 of the corresponding
scene may be displayed together with the corresponding capture image 810 for each source (for example, news, a blog, Twitter, <b><i>Facebook,</i></b> and the like).  Likewise, a capture image of the corresponding broadcasting posted and referenced second most may be
displayed under the image captured in 14 seconds in the broadcasting on May 23, 2012.
<br/><br/> A method of determining rankings of the respective scenes may be variously implemented.  For example, the rankings may be determined by summing up the number of postings or references for each source or be determined by allocating weights to
each source and summing up the weights.
<br/><br/> Meanwhile, when a `mention of scene in video` displayed as related information on a specific capture image on the search result screen is selected, information on a source in which a corresponding scene is mentioned may be provided as shown in
FIG. 9.
<br/><br/> That is, referring to FIG. 9, a list 900 for a mention of a corresponding scene in a corresponding video may be provided for each source.  For example, the respective sources 910 in which the corresponding scene is mentioned in the news, the
respective sources 920 in which the corresponding scene is mentioned in Twitter, the respective sources in which the corresponding scene is mentioned in a blog, the respective sources in which the corresponding scene is mentioned in <b><i>Facebook,</i></b> and the
like, may be provided.  In this case, when information on the respective sources is selected, direct connection to a corresponding posting such as the news, the blog, Twitter, <b><i>Facebook,</i></b> and the like, in which the corresponding scene is mentioned may also
be implemented.
<br/><br/> As described above, according to the exemplary embodiment of the present invention, selected scenes may be classified for each specific source (news, a blog, a cafe, an SNS, and the like) and a degree of diffusion or preference (RT, reply, good,
and the like) may be socially analyzed to provide rankings of popular scenes for each source or be utilized as meta information for rankings of the search results as shown in FIG. 8.
<br/><br/> As another implementation of the present invention, as shown in FIG. 10, an issue scene for a specific video may be listed and provided.  That is, corresponding issue information may be tagged on the video to be analyzed in advance based on an
image in the video such as &lt;popular scene&gt; or &lt;issue scene&gt; in a video replay service, or the like, thereby providing a service.
<br/><br/> For example, referring to FIG. 10, when a replay service of a specific video is selected, a video reproducing player 1000 may be disposed on the top and an issue scene ranking 1010 may be provided on the bottom according to the exemplary
embodiment of the present invention.  That is, as described above, the rankings for the respective scenes in the video may be calculated by a preset method, and the respective scenes may be displayed according to a sequence of the rankings.  In this
case, statistics and posting lists of each source for the corresponding scene may be provided together with the information on the scenes according to each ranking.  Therefore, when a video play mark 1020 is selected in the information on the
corresponding scene, the video reproducing player 1000 of the top may be implemented to jump to the corresponding scene to reproduce the corresponding scene.  In addition, when a specific source of a specific scene is selected in a scene link 1030, a
list and a link of a post including a capture of the corresponding scene in the corresponding source may be provided as shown in FIG. 9.  For example, when a Twitter 1040 of a screen is ranked third, the Twitter may provide a posting list and link of the
corresponding scene.
<br/><br/> In addition, as another implementation of the present invention, a video for a capture image captured and posted by an acquaintance may be provided, or other processed related information may be provided.  For example, an acquaintance
relationship or a contents subscription state of specific users are analyzed, thereby making it possible to provide a scene posted by the acquaintance in the SNS and a scene mentioned in a news service to which the user subscribes and a link of contents
related to the corresponding scene.  In addition, evaluation information such as positive evaluation, negative evaluation, or the like, of a contents link is collected and analyzed, thereby making it possible to analyze and provide a reaction of the
corresponding scene.
<br/><br/> Hereinabove, although the present invention is described by specific matters such as concrete components, and the like, exemplary embodiments, and drawings, they are provided only for assisting in the entire understanding of the present
invention.  Therefore, the present invention is not limited to the exemplary embodiments.  Various modifications and changes may be made by those skilled in the art to which the present invention pertains from this description.
<br/><br/> Therefore, the spirit of the present invention should not be limited to the above-described exemplary embodiments, and the following claims as well as all modified equally or equivalently to the claims are intended to fall within the scope and
spirit of the invention.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09305215&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D890%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D18%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D890%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D18%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209305215"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9305215&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D890%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D18%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209305215">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=890&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=17&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=890&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=890&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=889&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=891&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></body></html>