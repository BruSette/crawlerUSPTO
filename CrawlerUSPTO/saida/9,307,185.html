<html><head>
<base target="_top"/>
<title>United States Patent: 9307185</title></head>
<!---BUF1=9307185
BUF7=2016
BUF8=25026
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=859&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=17&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=859&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=859&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=858&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=860&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D859%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D18%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209307185"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9307185&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D859%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D18%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209307185">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09307185&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D859%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D18%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>859</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,307,185</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
         Oddou
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     April 5, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">Method to mark and exploit at least one sequence record of a video
     presentation
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> The present invention proposes a method to mark and exploit at least one
     sequence record of a video presentation played on a multimedia unit, said
     method comprising the steps of:--during the video presentation, receiving
     a command from a user to mark a currently displayed video sequence, said
     command initiating the step of:--creating a sequence record comprising a
     time index or frame index, allowing to locate the proper part of the
     video presentation, and a reference of the video presentation, At a later
     stage, requesting the edition of the sequence record by:--adding textual
     information which corresponds to the actual sequence,--storing the
     sequence record.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Oddou; Christophe</b> (Joinville le Pont, <b>FR</b>)<b>, Dagaeff; Thierry</b> (Lille, <b>CH</b>)<b>, Abdeljaoued; Yousri</b> (Ecublens, <b>CH</b>)<b>, Horisberger; Benoit</b> (Echichens, <b>CH</b>)<b>, Turini; Nicola</b> (Ardon, <b>CH</b>)<b>, Rossier; Jean</b> (Aubonne, <b>CH</b>)<b>, Garcia Rojas Martinez; Alejandra</b> (Clarmont, <b>CH</b>) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/>Oddou; Christophe
<br/>Dagaeff; Thierry
<br/>Abdeljaoued; Yousri
<br/>Horisberger; Benoit
<br/>Turini; Nicola
<br/>Rossier; Jean
<br/>Garcia Rojas Martinez; Alejandra</b> </td><td> <br/>Joinville le Pont
<br/>Lille
<br/>Ecublens
<br/>Echichens
<br/>Ardon
<br/>Aubonne
<br/>Clarmont </td><td align="center"> <br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A </td><td align="center"> <br/>FR
<br/>CH
<br/>CH
<br/>CH
<br/>CH
<br/>CH
<br/>CH </td> <td align="left"> </td> </tr> </tbody></table>
<!-- AANM>
~AANM Oddou; Christophe
~AACI Joinville le Pont
~AAST N/A
~AACO FR
~AANM Dagaeff; Thierry
~AACI Lille
~AAST N/A
~AACO CH
~AANM Abdeljaoued; Yousri
~AACI Ecublens
~AAST N/A
~AACO CH
~AANM Horisberger; Benoit
~AACI Echichens
~AAST N/A
~AACO CH
~AANM Turini; Nicola
~AACI Ardon
~AAST N/A
~AACO CH
~AANM Rossier; Jean
~AACI Aubonne
~AAST N/A
~AACO CH
~AANM Garcia Rojas Martinez; Alejandra
~AACI Clarmont
~AAST N/A
~AACO CH
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b>NAGRAVISION S.A.</b>
 (Cheseaux-sur-Lausanne, 
<b>CH</b>)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>45418625
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>13/991,254</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>December 1, 2011</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">PCT Filed:
       </th><td align="left" width="90%"><b>
       December 01, 2011
       </b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">PCT No.:
       </th><td align="left" width="90%"><b>
       PCT/EP2011/071492
       </b></td></tr>
         <tr><th align="left" scope="row" valign="top" width="15%">371(c)(1),(2),(4) Date:
         </th><td align="left" width="85%"><b>
         August 14, 2013
         </b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">PCT Pub. No.:
       </th><td align="left" width="90%">
       <b>
       WO2012/072730
       </b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">PCT Pub. Date:
       </th><td align="left" width="90%">
       <b>
       June 07, 2012
       </b></td></tr>
     </tbody></table>
<hr/> <center><b>Prior Publication Data</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Document Identifier</u></b></th><th align="center" scope="col"><b><u>Publication Date</u></b></th></tr><tr><td align="center"> </td><td align="center"> US 20130322853 A1</td><td align="center">Dec 5, 2013</td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table>
<hr/> <center><b>Related U.S. Patent Documents</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col" width="7%"></th><th scope="col"></th><th scope="col"></th> <th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Application Number</u></b></th><th align="center" scope="col"><b><u>Filing Date</u></b></th><th align="center" scope="col"><b><u>Patent Number</u></b></th><th align="center" scope="col"><b><u>Issue Date</u></b></th></tr><tr><td align="center"> </td><td align="center">61418925</td><td align="center">Dec 2, 2010</td><td align="center"></td><td align="center"></td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table><td< td=""></td<><td< td=""></td<>     <hr/>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">G11B 27/105 (20130101); G11B 27/322 (20130101); H04N 21/47217 (20130101); H04N 21/835 (20130101); H04N 5/91 (20130101); G06F 17/3082 (20130101); G06F 17/30852 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">H04N 5/91 (20060101); G11B 27/10 (20060101); G06F 17/30 (20060101); H04N 21/835 (20110101); G11B 27/32 (20060101); H04N 21/472 (20110101)</td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9307185">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8463917">8463917</a></td><td align="left">
June 2013</td><td align="left">
Sugimoto et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050013208&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0013208</a></td><td align="left">
January 2005</td><td align="left">
Hirabayashi et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060098941&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0098941</a></td><td align="left">
May 2006</td><td align="left">
Abe et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060161742&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0161742</a></td><td align="left">
July 2006</td><td align="left">
Sugimoto et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070050833&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0050833</a></td><td align="left">
March 2007</td><td align="left">
Park</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20070154190&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2007/0154190</a></td><td align="left">
July 2007</td><td align="left">
Gilley et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080141299&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0141299</a></td><td align="left">
June 2008</td><td align="left">
Eyer et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080313569&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0313569</a></td><td align="left">
December 2008</td><td align="left">
Aoki et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090202216&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0202216</a></td><td align="left">
August 2009</td><td align="left">
Kaminski et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090210779&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0210779</a></td><td align="left">
August 2009</td><td align="left">
Badoiu et al.</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
       <center><b>Foreign Patent Documents</b></center>
<table width="100%"> <tbody><tr><td></td><th scope="col"></th> <td></td><th scope="col"></th> <td></td><th scope="col"></th></tr> <tr> <td align="left">
</td><td align="left">1 596 594</td><td></td><td align="left">
Nov 2005</td><td></td><td align="left">
EP</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
<table width="90%">   <tbody><tr><td><align="left"><br/>International Search Report issued in International Application No. PCT/EP2011/071492 dated Mar. 12, 2012. cited by applicant
.<br/>Written Opinion issued in International Application No. PCT/EP2011/071492 dated Mar. 12. 2012. cited by applicant. </align="left"></td></tr> </tbody></table><br/><center><b>Other References</b></center> <br/>
       <i>Primary Examiner:</i> Adams; Eileen
<br/>
       <i>Attorney, Agent or Firm:</i> <coma>DLA Piper LLP (US)
<br/>
       <hr/>
       <center><b><i>Parent Case Text</i></b></center>
       <hr/>
       <br/><br/>CROSS REFERENCE TO RELATED APPLICATIONS
<br/><br/> This application is a U.S. National Stage Application of International
     Application No. PCT/EP2011/071492 filed Dec. 1, 2011, which claims
     priority from US Provisional Patent Application No. 61/418,925 filed Dec.
     2, 2010. The entirety of all the above-listed applications are
     incorporated herein by reference
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>The invention claimed is: <br/><br/> 1.  A method for marking and editing at least one sequence record of a video presentation broadcasted from a broadcast center and played on a multimedia unit, said
method comprising the steps of: during the playing of the broadcasted video presentation, receiving a command, at the multimedia unit, from a user to mark a currently displayed video sequence of the video presentation;  creating, at the multimedia unit,
a sequence record comprising a time index or frame index, and a reference of the video presentation;  wherein the time index or frame index allows the user to locate the marked portion of the video presentation;  and editing the sequence record of the
video presentation that comprises the time index or frame index;  whereby the editing of the sequence record of the video presentation occurs at the multimedia unit after completion of the playing of the broadcast video presentation, by: adding textual
information which corresponds to the edited sequence record of the video presentation;  and storing the edited sequence record of the video presentation that contains the corresponding textual information.
<br/><br/> 2.  The method of claim 1, wherein the editing the sequence record further comprises: adding a tag category to identify the type of video sequence.
<br/><br/> 3.  The method of claim 1, wherein the sequence record further comprises an identifier of the user, said identifier being obtained automatically from the multimedia unit or it can be introduced during the editing the sequence record.
<br/><br/> 4.  The method of claim 1, wherein the sequence record comprises a publication state indicating if the record is: private;  can be shared within a specific group;  or publicly available.
<br/><br/> 5.  The method of claim 1, wherein the sequence record comprises a video sample of the time corresponding to the index in the form of a still image or an video extract.
<br/><br/> 6.  The method of claim 1, wherein editing the sequence record further comprises saving the sequence record to a remote database.
<br/><br/> 7.  The method of claim 6, wherein the editing the sequence record further comprises: requesting one or more of the sequence records stored in the remote database;  and displaying to the user the sequence records.
<br/><br/> 8.  The method of claim 7, wherein the display to the user is synchronized with the video presentation using the time index or frame index contained in the sequence record.
<br/><br/> 9.  The method of claim 7, wherein the display to the user is synchronized in advance of time with the video presentation according to a predefined time.
<br/><br/> 10.  A decoder for displaying a broadcasted video presentation from a broadcast center, said decoder comprising a video decoder receiving a compressed video stream and an output toward a display unit, a processing device to execute a program, a
memory to store at least a program and a memory to store non executable data, an input to receive a command from a user, said decoder being configured to perform the following steps: during the playing of the broadcasted video presentation from the
broadcast center, receiving a command from a user to mark a currently displayed video sequence of the video presentation;  creating a sequence record comprising a time index or frame index and a reference of the video presentation;  wherein the time
index or frame index allows the user to locate the marked portion of the video presentation;  and editing the sequence record of the video presentation that comprises the time index or frame index;  whereby the editing of the sequence record of the video
presentation occurs after completion of the playing of the broadcast video presentation by: adding textual information which corresponds to the edited sequence record of the video presentation;  and storing the edited sequence record of the video
presentation that contains the corresponding textual information. <hr/> <center><b><i>Description</i></b></center> <hr/> <br/><br/>INTRODUCTION
<br/><br/> The invention refers to a Television system engaging the TV viewer in an enhanced interaction with the video content.  The invention further relies on a collaborative behaviour of TV viewers who actively participate in the annotation of video
assets in order to provide information to future viewers.
<br/><br/>Problem to be Solved
<br/><br/> When watching a movie or any video sequence, one may want to annotate/tag a particular image or sequence of images at some point in the video.
<br/><br/> The tag may be used as a bookmark to further jump from place to place along the video stream or it may be attached to an object (car, watch, tool .  . . ), a person (actor, guest .  . . ) or a geographical position or landscape that is
recognized by the TV viewer because he is familiar with the place.
<br/><br/> Such metadata attached to a video may be a personal tag which is kept private and used for subsequent viewings; alternatively, it may be shared with some friends or it may be public information which is shared with the whole community in order
to provide valuable information to future viewers.
<br/><br/> Unfortunately, this tagging activity may divert the attention of the viewer, especially when it is necessary to type some text with a remote control, a keyboard, a virtual keyboard, a smartphone, a tablet .  . . .
<br/><br/> One solution is to postpone this activity during an advertisement or simply at the end of the movie.  Unfortunately, the TV viewer may have forgotten the visual context which has triggered the wish to tag the video, especially if the viewer
wishes to submit multiple tags along the whole movie.
<br/><br/>BRIEF DESCRIPTION OF THE INVENTION
<br/><br/> The present invention proposes a method to exploit of at least one sequence record of a video presentation played on a multimedia unit, said method comprising the steps of: during the video presentation, receiving a command from a user to mark a
currently displayed video sequence, said command initiating the step of: creating a sequence record comprising a time index or frame index, allowing to locate the proper part of the video presentation, and a reference of the video presentation,
<br/><br/> At a later stage, requesting the edition of the sequence record by: adding textual information which corresponds to the actual sequence, storing the sequence record. <br/><br/>BRIEF DESCRIPTION OF THE DRAWINGS
<br/><br/> The present invention will be better understood thanks to the attached drawings in which:
<br/><br/> the FIG. 1 illustrates the system in which the invention can be implemented
<br/><br/> the FIG. 2 illustrates the method of the invention with the delayed annotation.
<br/><br/>DETAILED DESCRIPTION
<br/><br/> The FIG. 1 depicts the end-to-end system in the special case where the decoder is a PVR and the tagging point is defined with the remote control.  In this configuration, the video samples are stored on the local hard disk until the TV viewer
finalizes the annotation.  The pre-tags may be stored locally but the finalized tag objects are uploaded to the back office server to be used by the whole community.
<br/><br/> The FIG. 1 illustrates the system comprising a broadcaster 1 sending video presentations to at least one decoder 2.  The video presentation comprises a reference allowing to identify one video presentation among the video presentations
broadcasted by the broadcaster.  In the illustrated embodiment, the decoder has an Internet connection to share its sequence records or to combine with sequence records from other users.
<br/><br/> The FIG. 2 illustrate the method of the invention in which a video presentation or video stream is flagged at a time t1 or t2 corresponding to the time of the creation of the sequence record.  At the end of the video presentation, the user can
add additional data to annotate the sequence record.
<br/><br/> Let's first define what a tag consists of.  (A tag is equivalent as a sequence record)
<br/><br/> A tag is made of at least: the reference of the TV event or VoD movie to which it is attached.  a time index or frame index, allowing to locate the proper part of the TV event or VoD movie for example a word or a short sentence which corresponds
to the actual annotation In addition the tag can alternatively further comprises: a tag category (bookmark, object, person, location, .  . . ) an identifier which uniquely identifies the originator of the tag (e.g. subscriber_id, pseudo, email address . 
. . ) a publication state indicating if the tag is a personal tag which is kept private or if it is shared with some friends or if it can be released to the whole community as public information.
<br/><br/> The proposed solution for delayed tagging consists in first marking the video sequence when watching the video and then subsequently tagging the video sequence at a later time based on a video sample (or a set of still images) captured at
marking time to remember the visual context.
<br/><br/> Let's go through the various steps of the procedure.
<br/><br/> Marking
<br/><br/> When the user decides to tag a video at some point in time, he simply triggers the marking by either pressing a button of the remote control or a graphic button of an application running on a personal device like a tablet, a smartphone, a PC . 
. . .
<br/><br/> This operation generates a pre-tag object which contains at least the following information: the reference of the TV event or VoD movie to which it is attached.  a time index or frame index, an address pointer to a video sample (or a set of
still images) around the marking time
<br/><br/> Further information can be attached to the pre-tag, i.e.: the identifier of the originator
<br/><br/> This pre-tag object may be stored temporarily in the decoder or uploaded to the back office for remote storage.
<br/><br/> Simultaneously, a video sample (or a set of still images) is captured.
<br/><br/> 2 technical solutions are proposed to implement the acquisition of video samples around the marking time:
<br/><br/> 1--in a first embodiment, the decoder is a Personal Video Recorder equipped with a mass storage capacity like a Hard Disk and the TV viewer uses his remote control to mark a tagging point.  In the decoder, a circular buffer of pre-defined size
(corresponding to a few seconds) is continuously filled with the video stream.  When the user places a mark, the buffer is further filled for half of its total size; then it is fetched and stored for subsequent viewing.  The address pointer to the stored
buffer is added in the pre-tag object for later access.
<br/><br/> 2--in a second embodiment, the decoder is not necessarily a PVR decoder and it has limited storage capacity.  Therefore, the video sample is stored at the head-end based on a notification of the TV viewer who indicates a marking point.  This
notification can be made by the remote control through the decoder if it has a return path or from any portable device connected to the Internet like a PC, a tablet or a smartphone.
<br/><br/> Several implementations are possible at the Head-End: a unique video sample may be copied for each TV viewer indicating a marking point; alternatively, to save memory, the start/end address pointers to a unique copy of the video asset are saved
for further access (this option is typically applicable for a VoD service or a catch-up TV service for which the video file remains easily accessible in the Head-End once it has been broadcast).
<br/><br/> Alternatively, in case of limited storage capacity or limited bandwidth on the broadband link, still images can be captured and stored every n frames rather than a video sample in order to further remember the context around the marking point.
<br/><br/> The FIG. 2 depicts this pre-tagging step:
<br/><br/> Tagging
<br/><br/> When the user is available to finalize the tagging, he/she goes through the following steps: he/she selects a tagging session in his/her list of tagging sessions he/she selects a pre-tag in his/her list of pre-tags he/she accesses the video
sample captured at the related marking time in order to remember the visual context.  he/she enters the tag i.e. a word or a short sentence he/she optionally enters the tag category (bookmark, object, person, location .  . . ) he/she defines the
publication state.
<br/><br/> At this point, the tag is finalized.  The video sample is erased to free the memory, as well as the address pointer.
<br/><br/> Publication
<br/><br/> Once a tag is finalized, it is published according to its publication state.  In most cases, it is simply uploaded in a back office server for subsequent usage.  It can also be stored locally in the terminal, especially if it is a private tag.
<br/><br/> In addition, it can be explicitly published to one or several persons using various communication systems such as emails, social networks (e.g. <b><i>Facebook</i></b> wall), SMS, micro-blogging (e.g. Twitter) .  . . .
<br/><br/> When all tags are stored in the back-office, it is important to record them efficiently in order to easily access them when requested.  For example, all tags related to the same TV event/movie could be stored in a dedicated structure and ordered
chronologically.  More generally, an ontology could be used to structure the tag's database.
<br/><br/> Tag Usage
<br/><br/> Assuming a movie has been tagged by one or several persons, the goal is now to access the tags stored in the back office repository.
<br/><br/> Several applications may be envisioned:
<br/><br/> One option is to display all tags which have been associated to a movie by all contributors.  This can be done in a graphical window of the TV screen or on a portable device (smartphone, tablet, PC .  . . ) that the TV viewer may hold. 
Obviously, the system shall make sure that the tag is displayed at the right time according to the corresponding time stamp.  Optionally, it may be possible to display the tag a few seconds ahead in order to warn the TV viewer in advance.
<br/><br/> Alternatively, it may be possible to define some filters in order to display tags by category or by originator.
<br/><br/> In a third option, it may be possible to use tags as bookmarks in order to jump from tag to tag in a video sequence; this feature allows TV viewers to access key positions in the video sequence.  Regarding the implementation, the decoder uses
the next tag time index received from the back office server in order to command the video server to jump to the specified location in the stream.
<br/><br/> This invention is integrated as a software package into the decoder of the user.  This decoder or multimedia unit comprises a program memory allowing to store program file and a processing device to execute this program.  The decoder also
comprises a memory in the form of a hard drive or any non volatile memory to at least store locally the sequence record while the video presentation is going on.  The decoder can receive a command from a remote control via infrared or wireless means. 
The decoder is in charge of receiving the video presentation, usually in compressed form (via various means such as Internet, cable, antenna, satellite) and process the signal (i.e. decompress or decrypt it in case of conditional access video) and to
pass it to the television.  While the video signal is passed to the television, the decoder keeps track of an index of the video presentation, index that allows to identify the video sequence currently displayed.  This could be in the form of a time
index, a frame index or an index (e.g. packet index) that allows to retrieve later the video sequence during which the user has activated the tagging function.
<br/><br/> A predefined time can be set by the system or as a parameter accessible by the user to move backward the video presentation and to store this index rather than the current index.  The user can be interested to the few second prior to the time he
has pressed the command to tag the video presentation.  This predetermined time can be used to design the video buffer used to fetch a sample of the video presentation.  In an exemplary embodiment, the predetermined time is 3 seconds, i.e. the decoder
store at least 3 seconds of the video already displayed in case that the user request a tag.  In this case, the content of the buffer is stored in the hard drive of the decoder as past video event.  It is possible that the following 3 seconds are also
added to this sample, having the consequence that the 3 seconds before the user's command the 3 second after the user's command are store as video sample.
<br/><br/> According to another embodiment, the video sample is added at the end of the video presentation, at the time the user enters into the mode of completing the sequence record by additional data.
<br/><br/> In case that the video presentation is provided by a broadcast service that allows retrieval of video presentation at a later stage, the decoder does not need to buffer and store video sample at the time the user requests a tag.  The video
sample can be added later with a request sent to the broadcaster, this request containing at least the reference of the video presentation and the index.  This step of adding the video sample can be executed after the user has sent the sequence record to
the remote database.  The latter can have access to the entire video presentation and can obtain the video sample for storing it with the other data of the sequence record.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09307185&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D859%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D18%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D859%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D18%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209307185"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9307185&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D859%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D18%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209307185">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=859&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=17&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=859&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=859&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=19&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=858&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=860&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=18&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></body></html>