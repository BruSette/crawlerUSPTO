<html><head>
<base target="_top"/>
<title>United States Patent: 9372915</title></head>
<!---BUF1=9372915
BUF7=2016
BUF8=94210
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=252&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=5&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=252&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=6&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=252&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=7&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=251&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=6&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=253&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=6&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D252%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D6%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209372915"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9372915&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D252%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D6%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209372915">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09372915&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D252%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D6%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>252</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,372,915</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
         Long
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     June 21, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">System and method for probabilistic relational clustering
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> Relational clustering has attracted more and more attention due to its
     phenomenal impact in various important applications which involve
     multi-type interrelated data objects, such as Web mining, search
     marketing, bioinformatics, citation analysis, and epidemiology. A
     probabilistic model is presented for relational clustering, which also
     provides a principal framework to unify various important clustering
     tasks including traditional attributes-based clustering, semi-supervised
     clustering, co-clustering and graph clustering. The model seeks to
     identify cluster structures for each type of data objects and interaction
     patterns between different types of objects. Under this model, parametric
     hard and soft relational clustering algorithms are provided under a large
     number of exponential family distributions. The algorithms are applicable
     to relational data of various structures and at the same time unify a
     number of state-of-the-art clustering algorithms: co-clustering
     algorithms, the k-partite graph clustering, and semi-supervised
     clustering based on hidden Markov random fields.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Long; Bo</b> (Palo Alto, CA)<b>, Zhang; Zhongfei Mark</b> (Vestal, NY) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/>The Research Foundation for The State University of New York</b> </td><td> <br/>Binghamton </td><td align="center"> <br/>NY </td><td align="center"> <br/>US </td> <td align="left"> </td> </tr> </tbody></table>
<!-- AANM>
~AANM The Research Foundation for The State University of New York
~AACI Binghamton
~AAST NY
~AACO US
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b>The Research Foundation for The State University of New York</b>
 (Binghamton, 
NY)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>46964317
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>14/672,430</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>March 30, 2015</b></td></tr>
     </tbody></table>
<hr/> <center><b>Prior Publication Data</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Document Identifier</u></b></th><th align="center" scope="col"><b><u>Publication Date</u></b></th></tr><tr><td align="center"> </td><td align="center"> US 20150254331 A1</td><td align="center">Sep 10, 2015</td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table>
<hr/> <center><b>Related U.S. Patent Documents</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col" width="7%"></th><th scope="col"></th><th scope="col"></th> <th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Application Number</u></b></th><th align="center" scope="col"><b><u>Filing Date</u></b></th><th align="center" scope="col"><b><u>Patent Number</u></b></th><th align="center" scope="col"><b><u>Issue Date</u></b></th></tr><tr><td align="center"> </td><td align="center">14217939</td><td align="center">Mar 31, 2015</td><td align="center">8996528</td><td align="center"></td></tr><tr><td align="center"> </td><td align="center">13628559</td><td align="center">Mar 18, 2014</td><td align="center">8676805</td><td align="center"></td></tr><tr><td align="center"> </td><td align="center">12538835</td><td align="center">Oct 9, 2012</td><td align="center">8285719</td><td align="center"></td></tr><tr><td align="center"> </td><td align="center">61087168</td><td align="center">Aug 8, 2008</td><td align="center"></td><td align="center"></td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table><td< td=""></td<><td< td=""></td<><td< td=""></td<><td< td=""></td<><td< td=""></td<>     <hr/>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">G06F 17/30598 (20130101); G06N 7/005 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">G06F 7/00 (20060101); G06F 17/30 (20060101); G06N 7/00 (20060101)</td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9372915">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6654740">6654740</a></td><td align="left">
November 2003</td><td align="left">
Tokuda</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7092920">7092920</a></td><td align="left">
August 2006</td><td align="left">
Heard</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7720830">7720830</a></td><td align="left">
May 2010</td><td align="left">
Wen</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7730063">7730063</a></td><td align="left">
June 2010</td><td align="left">
Eder</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7849097">7849097</a></td><td align="left">
December 2010</td><td align="left">
Cao</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7853485">7853485</a></td><td align="left">
December 2010</td><td align="left">
Song</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7853596">7853596</a></td><td align="left">
December 2010</td><td align="left">
Ma</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7920745">7920745</a></td><td align="left">
April 2011</td><td align="left">
Song</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8185481">8185481</a></td><td align="left">
May 2012</td><td align="left">
Long</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8224072">8224072</a></td><td align="left">
July 2012</td><td align="left">
Porikli</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8285719">8285719</a></td><td align="left">
October 2012</td><td align="left">
Long</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8407230">8407230</a></td><td align="left">
March 2013</td><td align="left">
Slaney</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20020099594&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2002/0099594</a></td><td align="left">
July 2002</td><td align="left">
Heard</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20030232314&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2003/0232314</a></td><td align="left">
December 2003</td><td align="left">
Stout</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20040111220&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2004/0111220</a></td><td align="left">
June 2004</td><td align="left">
Ochs</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20050261953&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2005/0261953</a></td><td align="left">
November 2005</td><td align="left">
Malek</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20060040247&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2006/0040247</a></td><td align="left">
February 2006</td><td align="left">
Templin</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080168061&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0168061</a></td><td align="left">
July 2008</td><td align="left">
Liu</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080294686&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0294686</a></td><td align="left">
November 2008</td><td align="left">
Long</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20100153318&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2010/0153318</a></td><td align="left">
June 2010</td><td align="left">
Branavan</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20100268716&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2010/0268716</a></td><td align="left">
October 2010</td><td align="left">
Degaugue</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20100332475&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2010/0332475</a></td><td align="left">
December 2010</td><td align="left">
Birdwell</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110231347&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0231347</a></td><td align="left">
September 2011</td><td align="left">
Xu</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110264665&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0264665</a></td><td align="left">
October 2011</td><td align="left">
Mital</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20120054192&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2012/0054192</a></td><td align="left">
March 2012</td><td align="left">
Song</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20120233096&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2012/0233096</a></td><td align="left">
September 2012</td><td align="left">
Gupta</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20130013603&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2013/0013603</a></td><td align="left">
January 2013</td><td align="left">
Parker</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20130110843&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2013/0110843</a></td><td align="left">
May 2013</td><td align="left">
Ellingsworth</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20140201185&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2014/0201185</a></td><td align="left">
July 2014</td><td align="left">
Chang</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20140280193&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2014/0280193</a></td><td align="left">
September 2014</td><td align="left">
Cronin</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
       <i>Primary Examiner:</i> Liu; Hexing
<br/>
       <i>Attorney, Agent or Firm:</i> <coma><coma>Hoffberg; Steve M.
Ostrolenk Faber LLP
<br/>
       <hr/>
       <center><b><i>Government Interests</i></b></center>
       <hr/>
       <br/><br/>GOVERNMENT RIGHTS CLAUSE
<br/><br/> This invention was made with government support under U.S. Pat. No.
     0,535,162 awarded by The National Science Foundation and award
     FA8750-05-2-0284 awarded by AFRL and award FA9550-06-1-0327 awarded by
     AFOSR. The government has certain rights in the invention.
       <hr/>
       <center><b><i>Parent Case Text</i></b></center>
       <hr/>
       <br/><br/>CROSS REFERENCE TO RELATED APPLICATIONS
<br/><br/> The present application is a Continuation of U.S. patent application Ser.
     No. 14/217,939, filed Mar. 18, 2014, now U.S. Pat. No. 8,996,528, issued
     Mar. 31, 2015, which is a Continuation of U.S. patent application Ser.
     No. 13/628,559, filed Sep. 27, 2012, now U.S. Pat. No. 8,676,805, issued
     Mar. 18, 2014, which is a Continuation of U.S. patent application Ser.
     No. 12/538,835, filed Aug. 10, 2009, now U.S. Pat. No. 8,285,719, issued
     Oct. 9, 2012, which claims benefit of priority from U.S. Provisional
     Patent Application Ser. No. 61/087,168, filed Aug. 8, 2008, the entirety
     of which is expressly incorporated herein by reference.
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>What is claimed is: <br/><br/> 1.  A method of detection of a community in a network, comprising: automatically optimizing an unsupervised mixed membership relational clustering model based on at least
respective relationships between a plurality of interrelated data objects, dependent on different latent classes having respective latent class membership parameters, by maximizing a likelihood function to estimate unknown parameters of a joint
probability distribution over latent indicators of the plurality of interrelated data objects having at least one type of data associated with different latent classes, having at least one of respective data object attributes, homogeneous relations
between the respective data object and data objects having the same type, and heterogeneous relations between the respective data object and data objects having different types, and observations of the plurality of data object attributes;  clustering the
interrelated plurality of data objects according to the optimized unsupervised mixed membership relational clustering model;  wherein the plurality of interrelated data objects comprise a set of web documents, wherein the respective data object
attributes comprise a web document text and the relations between respective data objects comprise link information;  and responding to a web search query based on the clustering.
<br/><br/> 2.  The method according to claim 1, wherein said optimizing comprises iteratively maximizing an expectation function.
<br/><br/> 3.  The method according to claim 1, wherein the plurality of data objects comprise a comprising plurality of different interrelated types of data associated with respectively different latent classes, having respective data object attributes,
homogeneous relations between the respective data object and data objects having the same type, and heterogeneous relations between the respective data object and data objects having different types.
<br/><br/> 4.  The method according to claim 1, wherein the latent indicators have respective latent class membership parameters generated based on a distribution selected from the group consisting of a multinomial distribution, a Bernoulli distribution, a
normal distribution, and an exponential distribution.
<br/><br/> 5.  The method according to claim 1, wherein the likelihood function is maximized using a posterior computed using the Gibbs sampler.
<br/><br/> 6.  The method according to claim 1, wherein the clustering comprise using the optimized mixed membership relational clustering model to partition an arbitrarily complex graph involving at least the data object attributes, the homogeneous
relations and the heterogeneous relations.
<br/><br/> 7.  A method of detection of a community in a network, comprising: automatically optimizing an unsupervised mixed membership relational clustering model based on at least respective relationships between a plurality of interrelated data objects,
dependent on different latent classes having respective latent class membership parameters, by maximizing a likelihood function to estimate unknown parameters of a joint probability distribution over latent indicators of the plurality of interrelated
data objects having at least one type of data associated with different latent classes, having at least one of respective data object attributes, homogeneous relations between the respective data object and data objects having the same type, and
heterogeneous relations between the respective data object and data objects having different types, and observations of the plurality of data object attributes;  clustering the interrelated plurality of data objects according to the optimized
unsupervised mixed membership relational clustering model;  wherein the plurality of interrelated data objects comprise a set of media objects;  and providing a media recommendation based on the clustering.
<br/><br/> 8.  The method according to claim 7, wherein said optimizing comprises iteratively maximizing an expectation function.
<br/><br/> 9.  The method according to claim 7, wherein the plurality of data objects comprise a comprising plurality of different interrelated types of data associated with respectively different latent classes, having respective data object attributes,
homogeneous relations between the respective data object and data objects having the same type, and heterogeneous relations between the respective data object and data objects having different types.
<br/><br/> 10.  The method according to claim 7, wherein the latent indicators have respective latent class membership parameters generated based on a distribution selected from the group consisting of a multinomial distribution, a Bernoulli distribution,
a normal distribution, and an exponential distribution.
<br/><br/> 11.  The method according to claim 7, wherein the likelihood function is maximized using a posterior computed using the Gibbs sampler.
<br/><br/> 12.  The method according to claim 7, wherein the clustering comprise using the optimized mixed membership relational clustering model to partition an arbitrarily complex graph involving at least the data object attributes, the homogeneous
relations and the heterogeneous relations.
<br/><br/> 13.  The method according to claim 7, wherein the data object attributes of the media data objects comprise actor names.
<br/><br/> 14.  A method of detection of a community in a network, comprising: automatically optimizing an unsupervised mixed membership relational clustering model based on at least respective relationships between a plurality of interrelated data
objects, dependent on different latent classes having respective latent class membership parameters, by maximizing a likelihood function to estimate unknown parameters of a joint probability distribution over latent indicators of the plurality of
interrelated data objects having at least one type of data associated with different latent classes, having at least one of respective data object attributes, homogeneous relations between the respective data object and data objects having the same type,
and heterogeneous relations between the respective data object and data objects having different types, and observations of the plurality of data object attributes;  clustering the interrelated plurality of data objects according to the optimized
unsupervised mixed membership relational clustering model;  wherein the plurality of interrelated data objects comprise a set of social network data objects, and relations comprise social links;  and detecting a social community within the social network
data objects, based on the clustering.
<br/><br/> 15.  The method according to claim 14, wherein said optimizing comprises iteratively maximizing an expectation function.
<br/><br/> 16.  The method according to claim 14, wherein the plurality of data objects comprise a comprising plurality of different interrelated types of data associated with respectively different latent classes, having respective data object attributes,
homogeneous relations between the respective data object and data objects having the same type, and heterogeneous relations between the respective data object and data objects having different types.
<br/><br/> 17.  The method according to claim 14, wherein the latent indicators have respective latent class membership parameters generated based on a distribution selected from the group consisting of a multinomial distribution, a Bernoulli distribution,
a normal distribution, and an exponential distribution.
<br/><br/> 18.  The method according to claim 14, wherein the likelihood function is maximized using a posterior computed using the Gibbs sampler.
<br/><br/> 19.  The method according to claim 14, wherein the clustering comprise using the optimized mixed membership relational clustering model to partition an arbitrarily complex graph involving at least the data object attributes, the homogeneous
relations and the heterogeneous relations.
<br/><br/> 20.  The method according to claim 12, wherein the social network data comprises <b><i>Facebook</i></b> pages. <hr/> <center><b><i>Description</i></b></center> <hr/> <br/><br/>BACKGROUND OF THE INVENTION
<br/><br/>1.  Introduction
<br/><br/> Most clustering approaches in the literature focus on "flat" data in which each data object is represented as a fixed-length attribute vector [38].  However, many real-world data sets are much richer in structure, involving objects of multiple
types that are related to each other, such as documents and words in a text corpus, Web pages, search queries and Web users in a Web search system, and shops, customers, suppliers, shareholders and advertisement media in a marketing system.
<br/><br/> In general, relational data contain three types of information, attributes for individual objects, homogeneous relations between objects of the same type, heterogeneous relations between objects of different types.  For example, for a scientific
publication relational data set of papers and authors, the personal information such as affiliation for authors are attributes; the citation relations among papers are homogeneous relations; the authorship relations between papers and authors are
heterogeneous relations.  Such data violate the classic independently and identically distributed (IID) assumption in machine learning and statistics and present huge challenges to traditional clustering approaches.  An intuitive solution is that we
transform relational data into flat data and then cluster each type of objects independently.  However, this may not work well due to the following reasons.
<br/><br/> First, the transformation causes the loss of relation and structure information [14].  Second, traditional clustering approaches are unable to tackle influence propagation in clustering relational data, i.e., the hidden patterns of different
types of objects could affect each other both directly and indirectly (pass along relation chains).  Third, in some data mining applications, users are not only interested in the hidden structure for each type of objects, but also interaction patterns
involving multi-types of objects.  For example, in document clustering, in addition to document clusters and word clusters, the relationship between document clusters and word clusters is also useful information.  It is difficult to discover such
interaction patterns by clustering each type of objects individually.
<br/><br/> Moreover, a number of important clustering problems, which have been of intensive interest in the literature, can be viewed as special cases of relational clustering.  For example, graph clustering (partitioning) [7, 42, 13, 6, 20, 28] can be
viewed as clustering on singly-type relational data consisting of only homogeneous relations (represented as a graph affinity matrix); co-clustering [12, 2] which arises in important applications such as document clustering and micro-array data
clustering, can be formulated as clustering on bi-type relational data consisting of only heterogeneous relations.
<br/><br/> Recently, semi-supervised clustering [46, 4] has attracted significant attention, which is a special type of clustering using both labeled and unlabeled data.  Therefore, relational data present not only huge challenges to traditional
unsupervised clustering approaches, but also great need for theoretical unification of various clustering tasks.
<br/><br/>2.  Related Work
<br/><br/> Clustering on a special case of relational data, bi-type relational data consisting of only heterogeneous relations, such as the word-document data, is called co-clustering or bi-clustering.  Several previous efforts related to co-clustering are
model based [22, 23].  Spectral graph partitioning has also been applied to bi-type relational data [11, 25].  These algorithms formulate the data matrix as a bipartite graph and seek to find the optimal normalized cut for the graph.
<br/><br/> Due to the nature of a bipartite graph, these algorithms have the restriction that the clusters from different types of objects must have one-to-one associations.  Information-theory based co-clustering has also attracted attention in the
literature.  [12] proposes a co-clustering algorithm to maximize the mutual information between the clustered random variables subject to the constraints on the number of row and column clusters.  A more generalized co-clustering framework is presented
by [2] wherein any Bregman divergence can be used in the objective function.  Recently, co-clustering has been addressed based on matrix factorization.  [35] proposes an EM-like algorithm based on multiplicative updating rules.
<br/><br/> Graph clustering (partitioning) clusters homogeneous data objects based on pairwise similarities, which can be viewed as homogeneous relations.  Graph partitioning has been studied for decades and a number of different approaches, such as
spectral approaches [7, 42, 13] and multilevel approaches [6, 20, 28], have been proposed.  Some efforts [17, 43, 21, 21, 1] based on stochastic block modeling also focus on homogeneous relations.  Compared with co-clustering and
homogeneous-relation-based clustering, clustering on general relational data, which may consist of more than two types of data objects with various structures, has not been well studied in the literature.  Several noticeable efforts are discussed as
follows.  [45, 19] extend the probabilistic relational model to the clustering scenario by introducing latent variables into the model; these models focus on using attribute information for clustering.  [18] formulates star-structured relational data as
a star-structured m-partite graph and develops an algorithm based on semi-definite programming to partition the graph.  [34] formulates multi-type relational data as K-partite graphs and proposes a family of algorithms to identify the hidden structures
of a k-partite graph by constructing a relation summary network to approximate the original k-partite graph under a broad range of distortion measures.
<br/><br/> The above graph-based algorithms do not consider attribute information.  Some efforts on relational clustering are based on inductive logic programming [37, 24, 31].  Based on the idea of mutual reinforcement clustering, [51] proposes a
framework for clustering heterogeneous Web objects and [47] presents an approach to improve the cluster quality of interrelated data objects through an iterative reinforcement clustering process.  There are no sound objective function and theoretical
proof on the effectiveness and correctness (convergence) of the mutual reinforcement clustering.  Some efforts [26, 50, 49, 5] in the literature focus on how to measure the similarities or choosing cross-relational attributes.
<br/><br/> To summarize, the research on relational data clustering has attracted substantial attention, especially in the special cases of relational data.  However, there is still limited and preliminary work on general relational data clustering.
<br/><br/>BRIEF DESCRIPTION OF THE DRAWINGS
<br/><br/> FIGS. 1A-1C shows examples of the structures of relational data;
<br/><br/> FIG. 2 shows an NMI comparison of SGP, METIS and MMRC algorithms;
<br/><br/> FIG. 3 shows an NMI comparison of BSGP, RSN and MMRC algorithms for bi-type data; and
<br/><br/> FIG. 4 shows an NMI comparison of CBGC, RSN and MMRC algorithms for tri-type data.
<br/><br/>SUMMARY OF THE INVENTION
<br/><br/> Most clustering approaches in the literature focus on "flat" data in which each data object is represented as a fixed-length attribute vector.  However, many real-world data sets are much richer in structure, involving objects of multiple types
that are related to each other, such as documents and words in a text corpus, Web pages, search queries and Web users in a Web search system, and shops, customers, suppliers, share holders and advertisement media in a marketing system.
<br/><br/> In general, relational data contain three types of information, attributes for individual objects, homogeneous relations between objects of the same type, heterogeneous relations between objects of different types.  For example, for a scientific
publication, relational data sets of papers and authors, the personal information such as affiliation for authors are attributes; the citation relations among papers are homogeneous relations; the authorship relations between papers and authors are
heterogeneous relations.  Such data violate the classic IID assumption in machine learning and statistics, and present significant challenges to traditional clustering approaches.  An intuitive solution is that relational data is transformed into flat
data and then each type of object clustered independently.  However, this may not work well due to the following reasons.  First, the transformation causes the loss of relation and structure information.  Second, traditional clustering approaches are
unable to tackle influence propagation in clustering relational data, i.e., the hidden patterns of different types of objects could affect each other both directly and indirectly (pass along relation chains).  Third, in some data mining applications,
users are not only interested in the hidden structure for each type of objects, but also interaction patterns involving multi-types of objects.  For example, in document clustering, in addition to document clusters and word clusters, the relationship
between document clusters and word clusters is also useful information.  It is difficult to discover such interaction patterns by clustering each type of objects individually.
<br/><br/> Moreover, a number of important clustering problems, which have been of intensive interest in the literature, can be viewed as special cases of relational clustering.  For example, graph clustering (partitioning) can be viewed as clustering on
singly-type relational data consisting of only homogeneous relations (represented as a graph affinity matrix); co-clustering which arises in important applications such as document clustering and micro-array data clustering, can be formulated as
clustering on bi-type relational data consisting of only heterogeneous relations.
<br/><br/> Recently, semi-supervised clustering has attracted significant attention, which is a special type of clustering using both labeled and unlabeled data.  It can be formulated as clustering on single-type relational data consisting of attributes
and homogeneous relations.
<br/><br/> The present system and method is based on a probabilistic model for relational clustering, which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering, semi-supervised
clustering, co-clustering and graph clustering.  The model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects.  It is applicable to relational data of various structures.  Under
this model, parametric hard and soft (and hybrid) relational clustering algorithms are presented under a large number of exponential family distributions.
<br/><br/> There are three main advantages: (1) the technique is applicable to various relational data from various applications; (2) It is capable of adapting different distribution assumptions for different relational data with different statistical
properties; and (3) The resulting parameter matrices provide an intuitive summary for the hidden structure for the relational data.
<br/><br/>3.  Model Formulation
<br/><br/> A probabilistic model is herein proposed for relational clustering, which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering, semi-supervised clustering,
co-clustering and graph clustering.  The proposed model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects.  It is applicable to relational data of various structures.  Under
this model, parametric hard and soft relational clustering algorithms are provided under a large number of exponential family distributions.  The algorithms are applicable to various relational data from various applications and at the same time unify a
number of state-of-the-art clustering algorithms: co-clustering algorithms, the k-partite graph clustering, Bregman k-means, and semi-supervised clustering based on hidden Markov random fields.
<br/><br/> With different compositions of three types of information, attributes, homogeneous relations and heterogeneous relations, relational data could have very different structures.  FIGS. 1A-1C show three examples of the structures of relational
data.  FIG. 1A refers to a simple bi-type of relational data with only heterogeneous relations such as word-document data.  FIG. 1B represents a bi-type data with all types of information, such as actor-movie data, in which actors (type 1) have
attributes such as gender; actors are related to each other by collaboration in movies (homogeneous relations); actors are related to movies (type 2) by taking roles in movies (heterogeneous relations).  FIG. 1C represents the data consisting of
companies, customers, suppliers, share-holders and advertisement media, in which customers (type 5) have attributes.
<br/><br/> A relational data set as a set of matrices is represented.  Assume that a relational data set has m different types of data objects, .chi..sup.(1)={x.sub.i.sup.(1)}.sub.i=1.sup.n.sup.1, .  . . ,
.chi..sup.(m)={x.sub.i.sup.(m)}.sub.i=.sup.n.sup.m, where n.sub.j denotes the number of objects of the jth type and x.sub.p.sup.(j) denotes the name of the p.sup.th object of the j.sup.th type.  The observations of the relational data are represented as
three sets of matrices, attribute matrices {F.sup.(j).epsilon..sup.d.sup.j.sup..times.n.sup.j}.sub.j=1.sup.- m, where d.sub.j denotes the dimension of attributes for the j.sup.th type objects and denotes the attribute vector for object x.sub.p.sup.(j);
homogeneous relation matrices {S.sup.(j).epsilon..sup.n.sup.j.sup..times.n.sup.j}.sub.j=1.sup.(m), where S.sub.pq.sup.(j) denotes the relation between x.sub.p.sup.(j) and x.sub.q.sup.(j); heterogeneous relation matrices
{R.sup.(ij).epsilon..sup.n.sup.i.sup..times.n.sup.j}.sub.i,j=1.sup.m, where R.sub.pq.sup.(ij) denotes the relation between x.sub.p.sup.(i) and x.sub.q.sup.(j).  The above representation is a general formulation.  In real applications, not every type of
object has attributes, homogeneous relations and heterogeneous relations.  For example, the relational data set in FIG. 1A is represented by only one heterogeneous matrix R.sup.(12), and the one in FIG. 1B is represented by three matrices, F.sup.(1),
S.sup.(1) and R.sup.(12).  Moreover, for a specific clustering task, not use all available attributes and relations are used after feature or relation selection pre-processing.
<br/><br/> Mixed membership models, which assume that each object has mixed membership denoting its association with classes, have been widely used in the applications involving soft classification [16], such as matching words and pictures [39], race
genetic structures [39, 48], and classifying scientific publications [15].
<br/><br/> A relational mixed membership model is provided to cluster relational data (referred to as mixed membership relational clustering or MMRC).
<br/><br/> Assume that each type of objects X.sup.(j) has k.sub.j latent classes.  The membership vectors for all the objects in x.sup.(j) are represented as a membership matrix .LAMBDA..sup.(j).epsilon.[0,1].sup.k.sup.j.sup..times.n.sup.ji such that the
sum of elements of each column .LAMBDA..sub..cndot.p.sup.(j) is 1 and .LAMBDA..sub..cndot.p.sup.(j) denotes the membership vector for object x.sub.p.sup.(j), i.e., .LAMBDA..sub.gp.sup.(j) denotes the probability that object x.sub.p.sup.(j) associates
with the g.sup.th latent class.  The parameters of distributions to generate attributes, homogeneous relations and heterogeneous relations are expressed in matrix forms.  Let .THETA..sup.(j).epsilon..sup.d.sup.j.sup..times.k.sup.j denote the distribution
parameter matrix for generating attributes F.sup.(j) such that .THETA..sub.g.sup.(j) denotes the parameter vector associated with the g.sup.th latent class.  Similarly, .GAMMA..sup.(j).epsilon..sup.k.sup.j.sup..times.k.sup.j denotes the parameter matrix
for generating homogeneous relations S.sup.(j); .gamma..sup.(ij).epsilon..sup.k.sup.i.sup..times.k.sup.j denotes the parameter matrix for generating heterogeneous relations R.sup.(ij).  In summary, the parameters of MMRC model are
.OMEGA.={{.LAMBDA..sup.(j))}.sub.j=1.sup.m,{.THETA..sup.(j)}.sub.j=1.sup.- m,{.gamma..sup.(ij)}.sub.i,j=1.sup.m}.
<br/><br/> In general, the meanings of the parameters, .THETA., .LAMBDA., and .gamma., depend on the specific distribution assumptions.  However, in Section 4.1, it is shown that for a large number of exponential family distributions, these parameters can
be formulated as expectations with intuitive interpretations.
<br/><br/> Next, the latent variables are introduced into the model.  For each object x.sub.p.sup.j, a latent cluster indicator vector is generated based on its membership parameter .LAMBDA..sub..cndot.p.sup.(j), which is denoted as C.sub..cndot.p.sup.(p),
i.e., C.sup.(j).epsilon.{0,1}.sup.k.sup.j.sup..times.k.sup.j is a latent indicator matrix for all the j.sup.th type objects in X.sup.(j).
<br/><br/> Finally, we present the generative process of observations, {F.sup.(j)}.sub.j=1.sup.m {S.sup.(j)}.sub.j=1.sup.m, and {R.sup.(ij)}.sub.i,j=1.sup.m as follows:
<br/><br/> 1.  For each object x.sub.p.sup.(j) Sample C.sub..cndot.p.sup.(j).about.multinomial (.LAMBDA..sub..cndot.p.sup.(j),1).
<br/><br/> 2.  For each object x.sub.p.sup.(j) Sample F.sub..cndot.p.sup.(j).about.Pr(F.sub..cndot.p.sup.(j)|.THETA..sup.(j)C.s- ub..cndot.p.sup.(j)).
<br/><br/> 3.  For each pair of objects x.sub.p.sup.(j) and x.sub.q.sup.(j) Sample S.sub.pq.sup.(j).about.Pr(S.sub.pq.sup.(j)|C.sub..cndot.p.sup.(j)).sup.T.- GAMMA..sup.(j)C.sub..cndot.q.sup.(j)).
<br/><br/> 4.  For each pair of objects x.sub.p.sup.(j) and x.sub.q.sup.(j) Sample R.sub.pq.sup.(ij).about.Pr(R.sub.pq.sup.(ij)|C.sub..cndot.p.sup.(i)).sup.- T.GAMMA..sup.(ij)C.sub..cndot.q.sup.(j)).
<br/><br/> In the above generative process, a latent indicator vector for each object is generated based on multinomial distribution with the membership vector as parameters.  Observations are generated independently conditioning on latent indicator
variables.  The parameters of condition distributions are formulated as products of the parameter matrices and latent indicators, i.e., Pr(F.sub..cndot.p.sup.(j))|C.sub..cndot.p.sup.(j),.THETA..sup.(j)=Pr(F.su-
b..cndot.p.sup.(j))|.THETA..sup.(j)C.sub..cndot.p.sup.(j), Pr(S.sub.pq.sup.(j)|C.sub..cndot.p.sup.(j),C.sub..cndot.p.sup.(j)=Pr(S.su- b.pq.sup.(j)|(C.sub..cndot.p.sup.(j)).sup.T.GAMMA..sup.(j)C.sub..cndot.q.s- up.(j) and
Pr(R.sub.pq.sup.(ij)|C.sub..cndot.p.sup.(j),C.sub..cndot.p.sup.- (j),.gamma..sup.(j)=Pr(R.sub.pq.sup.(ij)|(C.sub..cndot.p.sup.(i)).sup.T.ga- mma..sup.(ij)C.sub..cndot.q.sup.(j).
<br/><br/> Under this formulation, an observation is sampled from the distributions of its associated latent classes.  For example, if C.sub..cndot.p.sup.(i) indicates that x.sub.p.sup.(i) is with the g.sup.th latent class and C.sub..cndot.q.sup.(j)
indicates that x.sub.q.sup.(j) is with the h.sup.th latent class, then (C.sub..cndot.p.sup.(i)).sup.T.gamma..sup.(ij)C.sub..cndot.q.sup.(j)=.gam- ma..sub.gh.sup.(ij).  Hence, Pr(R.sub.pq.sup.(ij)|.gamma..sub.gh.sup.(ij) implies that the relation between
x.sub.p.sup.(i) and x.sub.q.sup.(j) is sampled by using the parameter .gamma..sub.gh.sup.(ij).
<br/><br/> With matrix representation, the joint probability distribution over the observations and the latent variables can be formulated as follows,
<br/><br/> .function..PSI..OMEGA..times..function..LAMBDA..times..times..function..T- HETA..times..times..times..times..function..times..GAMMA..times..times..ti- mes..times..times..function.II.times..UPSILON.I.times.  ##EQU00001##
<br/><br/> where
<br/><br/> .PSI.={{C.sup.(j)}.sub.j=1.sup.m,{F.sup.(j)}.sub.j=1.sup.m,{R.sup.(ij)}.s- ub.i,j=1.sup.m},
<br/><br/> Pr(C.sup.(j)|.LAMBDA..sup.(j))=.PI..sub.p=1.sup.n.sup.jmultinomial(.LAMBD- A..sub..cndot.p.sup.(j)),
<br/><br/> Pr(F.sup.(j)|.THETA..sup.(j)C.sup.(j)=.PI..sub.p=1.sup.n.sup.jPr(F.sub..c- ndot.p.sup.(j)|.THETA..sup.(j)C.sub..cndot.p.sup.(j)),
<br/><br/> Pr(S.sup.(j))|C.sup.(j)).sup.T.GAMMA..sup.(j)C.sup.(j)=.PI..sub.p,q=1.sup- .n.sup.jPr(S.sub.pq.sup.(j)|C.sub..cndot.p.sup.(j)).sup.T.GAMMA..sup.(j)C.- sub..cndot.q.sup.(j),
<br/><br/> and similarly for R.sup.(ij).
<br/><br/>4.  Algorithm Derivation
<br/><br/> In this section, the parametric soft and hard relational clustering algorithms based on the MMRC model are derived under a large number of exponential family distributions.
<br/><br/> 4.1 MMRC with Exponential Families
<br/><br/> To avoid clutter, instead of general relational data, relational data similar to the one in FIG. 1(b) may be employed, which is a representative relational data set containing all three types of information for relational data, attributes,
homogeneous relations and heterogeneous relations.  However, the derivation and algorithms are applicable to general relational data.  For the relational data set in FIG. 1(b), there are two types of objects, one attribute matrix F, one homogeneous
relation matrix S and one heterogeneous relation matrix R. Based on Eq.(1), we have the following likelihood function, L(.OMEGA.|.PSI.)=Pr(C.sup.(1)|.LAMBDA..sup.(1))Pr(C.sup.(2)|.LAMBDA..sup.- (2))Pr(F|.THETA.C.sup.(1))
Pr(S|C.sup.(1)).sup.T.GAMMA.C.sup.(1))Pr(R|C.sup.(1)).sup.T.gamma.C.sup.(- 2) (2)
<br/><br/> One goal is to maximize the likelihood function in Eq.  (2) to estimate unknown parameters.
<br/><br/> For the likelihood function in Eq.(2), the specific forms of condition distributions for attributes and relations depend on specific applications.  Presumably, for a specific likelihood function, a specific algorithm should be derived.  However,
a large number of useful distributions, such as normal distribution, Poisson distribution, and Bernoulli distributions, belong to exponential families and the distribution functions of exponential families can be formulated as a general form.  This
advantageous property facilitates derivation of a general EM algorithm for the MMRC model.
<br/><br/> It is shown in the literature [3, 9] that there exists bijection between exponential families and Bregman divergences [40].  For example, the normal distribution, Bernoulli distribution, multinomial distribution and exponential distribution
correspond to Euclidean distance, logistic loss, KL-divergence and Itakura-Satio distance, respectively.  Based on the bijection, an exponential family density Pr(x) can always be formulated as the following expression with a Bregman divergence
D.sub..phi., Pr(x)=exp(-D.sub..phi.(x,.mu.))f.sub..phi.(x), (3) where f.sub..phi.(x) is a uniquely determined function for each exponential probability density, and .mu.  a is the expectation parameter.  Therefore, for the MMRC model under exponential
family distributions: Pr(F|.THETA.C.sup.(1))=exp(-D.sub..phi..sub.1(F|.THETA.C.sup.(1)))f.sub..- phi..sub.1(F) (4), Pr((S|(C.sup.(1)).sup.T.GAMMA.C.sup.(1))=exp(-D.sub..phi..sub.2(S|(C.sup.- (1)).sup.T.GAMMA.C.sup.(1)))f.sub..phi..sub.2(S) (5),
Pr(R|(C.sup.(1)).sup.T.gamma.C.sup.(2))=exp(-D.sub..phi.3(R,(C.sup.(1)).s- up.T.gamma.C.sup.(2))f.sub..phi.3(R) (6)
<br/><br/> In the above equations, a Bregman divergence of two matrices is defined as the sum of the Bregman divergence of each pair of elements from the two matrices.  Another advantage of the above formulation is that under this formulation, the
parameters, .THETA., .LAMBDA., and .gamma., are expectations of intuitive interpretations.  .THETA.  consists of center vectors of attributes; .GAMMA.  provides an intuitive summary of cluster structure within the same type objects, since
.GAMMA..sub.gh.sup.(1) implies expectation relations between the g.sup.th cluster and the h.sup.th cluster of type 1 objects; similarly, .gamma.  provides an intuitive summary for cluster structures between the different type objects.  In the above
formulation, different Bregman divergences are used, D.sub..phi..sub.1, D.sub..phi..sub.2, and D.sub..phi..sub.3, for the attributes, homogeneous relations and heterogeneous relations, since they could have different distributions in real applications. 
For example, suppose
<br/><br/> .THETA.  ##EQU00002## for normal distribution,
<br/><br/> .GAMMA.  ##EQU00003## for Bernoulli distribution, and
<br/><br/> .UPSILON.  ##EQU00004## for Poisson distribution; then the cluster structures of the data are very intuitive.  First, the center attribute vectors for the two clusters of type 1 are
<br/><br/> .times..times..times.  ##EQU00005## second, by .GAMMA..sup.(1) we know that the type 1 nodes from different clusters are barely related and cluster 1 is denser that cluster 2; third, by .gamma..sup.(12) we know that cluster 1 of type 1 nodes are
related to cluster 2 of type 2 nodes more strongly than to cluster 1 of type 2, and so on so forth.
<br/><br/> Since the distributions of C.sup.(1) and C.sup.(2) are modeled as multinomial distributions:
<br/><br/> .function..LAMBDA..times..times..LAMBDA..times..times..function..LAMBDA..- times..times..LAMBDA.  ##EQU00006##
<br/><br/> Substituting Eqs.  (4), (5), (6), (7), and (8) into Eq, (2) and taking some algebraic manipulations, the following log-likelihood function is obtained for MMRC under exponential families,
<br/><br/> .times..times..function..OMEGA..PSI..times..times..times..times..times..L- AMBDA..times..times..times..times..times..LAMBDA..PHI..function..THETA..ti- mes..times..PHI..function..times..GAMMA..times..times..PHI..function..time-
s..UPSILON..times..times..tau.  ##EQU00007##
<br/><br/> where .tau.=log f.sub..phi..sub.1(F)+log f.sub..phi..sub.2(S)+log f.sub..phi..sub.3(R), which is a constant in the log-likelihood function.
<br/><br/> Expectation Maximization (EM) is a general approach to find the maximum-likelihood estimate of the parameters when the model has latent variables.  EM does maximum likelihood estimation by iteratively maximizing the expectation of the complete
(log-)likelihood, which is the following under the MMRC model, Q(.OMEGA.,{tilde over (.OMEGA.)})=E[log(L(.OMEGA.|.PSI.))|C.sup.(1),C.sup.(2),{tilde over (.OMEGA.)}] (10)
<br/><br/> where {tilde over (.OMEGA.)} denotes the current estimation of the parameters and .OMEGA.  is the new parameters that we optimize to increase Q. Two steps, E-step (expectation step) and M-step (minimization step), are alternatively performed to
maximize the objective function in Eq.  (10).
<br/><br/> 4.2 Monte Carlo E-Step
<br/><br/> In the E-step, based on Bayes's rule, the posterior probability of the latent variables,
<br/><br/> .function..OMEGA..function..OMEGA..times..function..OMEGA.  ##EQU00008##
<br/><br/> is updated using the current estimation of the parameters.  However, conditioning on observations, the latent variables are not independent, i.e., there exist dependencies between the posterior probabilities of C.sup.(1) and C.sup.(2), and
between those of C.sub..cndot.p.sup.(1) and C.sub..cndot.q.sup.(1).  Hence, directly computing the posterior based on Eq.  (11) is prohibitively expensive.
<br/><br/> There exist several techniques for computing intractable posterior, such as Monte Carlo approaches, belief propagation, and variational methods.  A Monte Carlo approach, Gibbs sampler, is further analyzed, which is a method of constructing a
Markov chain whose stationary distribution is the distribution to be estimated.  It is of course understood that other known techniques may be employed.
<br/><br/> It is relatively easy to compute the posterior of a latent indicator vector while fixing all other latent indicator vectors, i.e.,
<br/><br/> .function..OMEGA..function..OMEGA..times..function..OMEGA.  ##EQU00009##
<br/><br/> where C.sub..cndot.-p.sup.(1) denotes all the latent indicator vectors except for C.sub..cndot.p.sup.(1).  Therefore, the following Markov chain is presented to estimate the posterior in Eq.  (11).  Sample C.sub..cndot.1.sup.(1) from
distribution Pr(C.sub..cndot.1.sup.(1))|C.sub..cndot.-1.sup.(1), C.sup.(2),F,S,R,{tilde over (.OMEGA.)}) .  . . Sample C.sub..cndot.n.sub.1.sup.(1) from distribution Pr(C.sub..cndot.n.sub.1.sup.(1)|C.sub..cndot.-n.sub.1.sup.(1), C.sup.(2)),F,S,R,{tilde
over (.OMEGA.)}) Sample C.sub..cndot.1.sup.(2) from distribution Pr(C.sub..cndot.1.sup.(2)|C.sub..cndot.-1.sup.(2), C.sup.(1),F,S,R,{tilde over (.OMEGA.)}) .  . . Sample C.sub..cndot.n.sub.2.sup.(2) from distribution
Pr(C.sub..cndot.n.sub.2.sup.(2))|C.sub..cndot.-2.sup.(2), C.sup.(1),F,S,R,{tilde over (.OMEGA.)});
<br/><br/> Note that at each sampling step in the above procedure, the latent indicator variables sampled from previous steps are used.  The above procedure iterates until the stop criterion is satisfied.  It can be shown that the above procedure is a
Markov chain converging to Pr(C.sup.(1),C.sup.(2)|F,S,R,{tilde over (.OMEGA.)}.  Assume that we keep l samples for estimation; then the posterior can be obtained simply by the empirical joint distribution of C.sup.(1) and C.sup.(2) in the l samples.
<br/><br/> 4.3 M-Step
<br/><br/> After the E-step, the posterior probability of latent variables is available to evaluate the expectation of the complete log-likelihood,
<br/><br/> .function..OMEGA..OMEGA..times..function..function..OMEGA..PSI..times..fu- nction..OMEGA.  ##EQU00010##
<br/><br/> In the M-step, the unknown parameters are optimized by
<br/><br/> .OMEGA..times..times..OMEGA..times..function..OMEGA..OMEGA.  ##EQU00011##
<br/><br/> First, the update rules for membership parameters .LAMBDA..sup.(1) and .LAMBDA..sup.(2) are derived.  To derive the expression for each .LAMBDA..sub.hp.sup.(1), the Lagrange multiplier a is introduced with the constraint
.SIGMA..sub.g=1.sup.k.sup.1.LAMBDA..sub.gp.sup.(1)=1 and the following equation solved,
<br/><br/> .differential..differential..LAMBDA..times..function..OMEGA..OMEGA..alpha- ..kappa..times..LAMBDA.  ##EQU00012##
<br/><br/> Substituting Eqs.  (9) and (13) into Eq.  (15), after some algebraic manipulations: Pr(C.sub.hp.sup.(1)=1|F,S,R,.OMEGA.)-.alpha..LAMBDA..sub.hp.sup.(1)=0 (16)
<br/><br/> Summing both sides over h, .alpha.=1 is obtained, resulting in the following update rule, .LAMBDA..sub.hp.sup.(1)=Pr(C.sub.hp.sup.(1)=1|F,S,R,{tilde over (.OMEGA.)}), (17)
<br/><br/> i.e., .LAMBDA..sub.hp.sup.(1) is updated as the posterior probability that the p.sup.th object is associated with the h.sup.th cluster.  Similarly, the following update rule for .LAMBDA..sub.hp.sup.(2) is provided:
.LAMBDA..sub.hp.sup.(2)=Pr(C.sub.hp.sup.(2)=1|F,S,R,{tilde over (.OMEGA.)}) (18)
<br/><br/> Second, the update rule for .THETA.  is derived.  Based on Eqs.  (9) and (13), optimizing .THETA.  is equivalent to the following optimization,
<br/><br/> .times..times..THETA..times..times..PHI..function..THETA..times..times..t- imes..function..times..OMEGA.  ##EQU00013##
<br/><br/> The above expression may be reformulated as,
<br/><br/> .times..times..THETA..times..times..times..times..PHI..function..THETA..t- imes..function..OMEGA.  ##EQU00014##
<br/><br/> To solve the above optimization, an important property of Bregman divergence presented in the following theorem may be used.
<br/><br/> Theorem 1.
<br/><br/> Let X be a random variable taking values in .chi.={x.sub.i}.sub.i=1.sup.n.OR right.S.OR right..sup.d following v. Given a Bregman divergence D.sub..phi.: S.times.int(S).fwdarw.[0,.infin.), the problem
<br/><br/> .di-elect cons..times..times..PHI..function.  ##EQU00015##
<br/><br/> has a unique minimizer given by S*=E.sub.v[X]|
<br/><br/> The proof of Theorem 1 is omitted (please refer [3, 40]).  Theorem 1 states that the Bregman representative of a random variable is always the expectation of the variable.  Based on Theorem 1 and the objective function in (20), we update
.THETA..sub..cndot.g as follows,
<br/><br/> .THETA..times.  .times..function..OMEGA..times..function..OMEGA.  ##EQU00016##
<br/><br/> Third, the update rule for .GAMMA.  is derived.  Based on Eqs.  (9) and (13), optimizing .GAMMA.  is formulated as the following optimization,
<br/><br/> .times..times..GAMMA..times..times..times..times..times..PHI..function..G- AMMA..times.  ##EQU00017##
<br/><br/> where {tilde over (p)} denotes Pr(C.sub.gp.sup.(1)=1, C.sub.hq.sup.(1)=1|F,S,R,{tilde over (.OMEGA.)}) and 1.ltoreq.p; q.ltoreq.n.sub.1.  Based on Theorem 1, we update each .GAMMA..sub.gh as follows,
<br/><br/> .GAMMA..times..times..function..OMEGA..times..function..OMEGA.  ##EQU00018##
<br/><br/> Fourth, the update rule for .gamma.  is derived.  Based on Eqs.  (9) and (13), optimizing .gamma.  is formulated as the following optimization,
<br/><br/> .times..times..UPSILON..times..times..times..times..times..PHI..function.- .UPSILON..times.  ##EQU00019##
<br/><br/> where {tilde over (p)} denotes Pr(C.sub.gp.sup.(1)=1, C.sub.hq.sup.(2)=1|F,S,R,{tilde over (.OMEGA.)}), 1.ltoreq.p.ltoreq.n.sub.1 and 1.ltoreq.q.ltoreq.n.sub.2.  Based on Theorem 1, each .GAMMA..sub.gh is updated as follows,
<br/><br/> .UPSILON..times..times..times..function..OMEGA..times..times..function..O- MEGA.  ##EQU00020##
<br/><br/> Combining the E-step and M-step, a general relational clustering algorithm is provided, Exponential Family MMRC (EF-MMRC) algorithm, which is summarized in Algorithm 1.  Since it is straightforward to apply the algorithm derivation to a
relational data set of any structure, Algorithm 1 is proposed based on the input of a general relational data set.  Despite that the input relational data could have various structures, EF-MMRC works simply as follows: in the E-step, EF-MMRC iteratively
updates the posterior probabilities that an object is associated with the clusters (the Markov chain in Section 4.2); in the M-step, based on the current cluster association (posterior probabilities), the cluster representatives of attributes and
relations are updated as the weighted mean of the observations no matter which exponential distributions are assumed.
<br/><br/> Therefore, with the simplicity of the traditional centroid-based clustering algorithms, EF-MMRC is capable of making use of all attribute information and homogeneous and heterogenous relation information to learn hidden structures from various
relational data.  Since EF-MMRC simultaneously clusters multi-type interrelated objects, the cluster structures of different types of objects may interact with each other directly or indirectly during the clustering process to automatically deal with the
influence propagation.  Besides the local cluster structures for each type of objects, the output of EF-MMRC also provides the summary of the global hidden structure for the data, i.e., based on .GAMMA.  and .gamma., we know how the clusters of the same
type and different types are related to each other.  Furthermore, relational data from different applications may have different probabilistic distributions on the attributes and relations; it is easy for EF-MMRC to adapt to this situation by simply
using different Bregman divergences corresponding to different exponential family distributions.
<br/><br/> TABLE-US-00001 Algorithm 1 Exponential Family MMRC Algorithm Input: A relational data set { {F.sup.(j) }.sub.j=1.sup.m , {S.sup.(j) }.sub.j=1.sup.m , {R .sup.(ij) }.sub.i,j=1.sup.m } , a set of exponential family distributions (Bregman
divergences) assumed for the data set.  Output: Membership Matrices {.LAMBDA..sup.(j) }.sub.j=1.sup.m , attribute expectation matrices {.THETA..sup.(j) }.sub.j=1.sup.m , homogeneous relation expectation matrices {.GAMMA..sup.(j) }.sub.j=1.sup.m , and
heterogeneous relation expectation matrices {.LAMBDA..sup.(ij) }.sub.i,j=1.sup.m .  Method: 1: Initialize the parameters as {tilde over (.OMEGA.)} = { {{tilde over (.LAMBDA.)}.sup.(j) }.sub.j=1.sup.m , {{tilde over (.THETA.)}.sup.(j) }.sub.j=1.sup.m ,
{{tilde over (.GAMMA.)}.sup.(j) }.sub.j=1.sup.m , {{tilde over (.LAMBDA.)}.sup.(ij) }.sub.i,j=1.sup.m .  2: repeat 3: {E-step} 4: Compute the posterior Pr({C.sup.(j) }|F.sup.(j) }.sub.j=1.sup.m , {S.sup.(j) }.sub.j=1.sup.m , {R.sup.(ij) }.sub.i,j=1.sup.m
, {tilde over (.OMEGA.)}) using the Gibbs sampler.  5: {M-step} 6: for j = 1 to m do 7: Compute .LAMBDA..sup.(j) using update rule (17).  8: Compute .THETA..sup.(j) using update rule (22).  9: Compute .GAMMA..sup.(j) using update rule (24).  10: for i =
1 to m do 11: Compute .LAMBDA..sup.(ij) using update rule (26).  12: end for 13: end for 14: {tilde over (.OMEGA.)} = .OMEGA.  15: until convergence
<br/><br/> If we assume O(m) types of heterogeneous relations among m types of objects, which is typical in real applications, and let n=.THETA.(n.sub.i) and k=.THETA.(k.sub.i), the computational complexity of EF-MMRC can be shown to be O(tmn.sup.2k) for t
iterations.  If the k-means algorithm are applied to each type of nodes individually by transforming the relations into attributes for each type of nodes, the total computational complexity is also O(tmn.sup.2k).
<br/><br/> 4.4 Hard MMRC Algorithm
<br/><br/> Due to its simplicity, scalability, and broad applicability, k-means algorithm has become one of the most popular clustering algorithms.  Hence, it is desirable to extend k-means to relational data.  Some efforts [47, 2, 12, 33] in the
literature work in this direction.  However, these approaches apply to only some special and simple cases of relational data, such as bi-type heterogeneous relational data.
<br/><br/> As traditional k-means can be formulated as a hard version of Gaussian mixture model EM algorithm [29], the hard version of MMRC algorithm is presented as a general relational k-means algorithm (Algorithm 1 is herein referred to as "soft
EF-MMRC"), which applies to various relational data.
<br/><br/> To derive the hard version MMRC algorithm, soft membership parameters .LAMBDA..sup.(j) are omitted in the MMRC model (C.sup.(j) in the model provides the hard membership for each object).  Next, the computation of the posterior probabilities in
the E-step is changed to a reassignment procedure, i.e., in the E-step, based on the estimation of the current parameters, cluster labels, {C.sup.(j)}.sub.j=1.sup.m, are reassigned to maximize the objective function in (9).  In particular, for each
object, while fixing the cluster assignments of all other objects, each cluster is assigned to find the optimal cluster assignment maximizing the objective function in (9), which is equivalent to minimizing the Bregman distances between the observations
and the corresponding expectation parameters.  After all objects are assigned, the re-assignment process is repeated until no object changes its cluster assignment between two successive iterations.
<br/><br/> In the M-step, the parameters are estimated based on the cluster assignments from the E-step.  A simple way to derive the update rules is to follow the derivation in Section 4.3 but replace the posterior probabilities by its hard versions.  For
example, after the E-step, if the object x.sub.j.sup.(p) is assigned to the g.sup.th cluster, i.e., C.sub.gp.sup.(j)=1, then the posterior Pr(C.sub.gp.sup.(1))=1|F,S,R,{tilde over (.OMEGA.)}=1 and Pr(C.sub.hp.sup.(1))=1|F,S,R,{tilde over (.OMEGA.)}=0 for
h.noteq.g.
<br/><br/> Using the hard versions of the posterior probabilities, the following update rule is derived:
<br/><br/> .THETA..times..times.  ##EQU00021##
<br/><br/> In the above update rule, since .SIGMA..sub.p=1.sup.n.sup.1C.sub.gp.sup.(j) is the size of the g.sup.th cluster, .THETA..sub..cndot.g.sup.(j) is actually updated as the mean of the attribute vectors of the objects assigned to the g.sup.th
cluster.  Similarly, the following update rule:
<br/><br/> .GAMMA..times..times..times..times.  ##EQU00022##
<br/><br/> i.e., .GAMMA..sub.gh.sup.(j) is updated as the mean of the relations between the objects of the j.sup.th type from the g.sup.th cluster and from the h.sup.th cluster.
<br/><br/> Each heterogeneous relation expectation parameter .gamma..sub.gh.sup.(ij) is updated as the mean of the objects of the i.sup.th type from the g.sup.th cluster and of the j.sup.th type from the h.sup.th cluster,
<br/><br/> .UPSILON..times..times..times..times.  ##EQU00023##
<br/><br/> The hard version of EF-MMRC algorithm is summarized in Algorithm 2.  It works simply as the classic k-means.  However, it is applicable to various relational data under various Bregman distance functions corresponding to various assumptions of
probability distributions.  Based on the EM framework, its convergence is guaranteed.  When applied to some special cases of relational data, it provides simple and new algorithms for some important data mining problems.  For example, when applied to the
data of one homogeneous relation matrix representing a graph affinity matrix, it provides a simple and new graph partitioning algorithm.
<br/><br/> Based on Algorithms 1 and 2, there is another version of EF-MMRC, i.e., soft and hard EF-MMRC may be combined together to have mixed EF-MMRC.  For example, hard EF-MMRC may be run several times as initialization, then soft EF-MMRC run.
<br/><br/> TABLE-US-00002 Algorithm 2 Hard MMRC Algorithm Input: A relational data set { {F.sup.(j) }.sub.j=1.sup.m , {S.sup.(j) }.sub.j=1.sup.m , {R .sup.(ij) }.sub.i,j=1.sup.m } , a set of exponential family distributions (Bregman divergences) assumed
for the data set.  Output: Cluster indicator matrices {C.sup.(j) }.sub.j=1.sup.m , attribute expectation matrices {.THETA..sup.(j) }.sub.j=1.sup.m , homogeneous relation expectation matrices {.GAMMA..sup.(j) }.sub.j=1.sup.m , and heterogeneous relation
expectation matrices {.LAMBDA..sup.(ij) }.sub.i,j=1.sup.m .  Method: 1: Initialize the parameters as {tilde over (.OMEGA.)} = { {{tilde over (.LAMBDA.)}.sup.(j) }.sub.j=1.sup.m , {{tilde over (.THETA.)}.sup.(j) }.sub.j=1.sup.m , {{tilde over
(.GAMMA.)}.sup.(j) }.sub.j=1.sup.m , {{tilde over (.LAMBDA.)}.sup.(ij) }.sub.i,j=1.sup.m .  2: repeat 3: {E-step} 4: Based on the current parameters, reassign cluster labels for each objects, i.e., update {C.sup.(j)}.sub.j=1.sup.m , to maximize the
objective function in Eq.  (9).  5: {M-step} 6: for j = 1 to m do 7: Compute .THETA..sup.(j) using update rule (27).  8: Compute .GAMMA..sup.(j) using update rule (28).  9: for i = 1 to m do 10: Compute .LAMBDA..sup.(ij) using update rule (29).  11: end
for 12: end for 13: {tilde over (.OMEGA.)} = .OMEGA.  14: until convergence
<br/><br/>5.  A Unified View to Clustering
<br/><br/> The connections between existing clustering approaches and the MMRF model and EF-MMRF algorithms are now discussed.  By considering them as special cases or variations of the MMRF model, MMRF is shown to provide a unified view to the existing
clustering approaches from various important data mining applications.
<br/><br/> 5.1 Semi-Supervised Clustering
<br/><br/> Recently, semi-supervised clustering has become a topic of significant interest [4, 46], which seeks to cluster a set of data points with a set of pairwise constraints.
<br/><br/> Semi-supervised clustering can be formulated as a special case of relational clustering, clustering on the single-type relational data set consisting of attributes F and homogeneous relations S. For semi-supervised clustering, S.sub.pq denotes
the pairwise constraint on the pth object and the qth object.
<br/><br/> [4] provides a general model for semi-supervised clustering based on Hidden Markov Random Fields (HMRFs).  It can be formulated as a special case of MMRC model.  As in [4], the homogeneous relation matrix S can be defined as follows,
<br/><br/> .function..times..times..di-elect cons..function..times..times..di-elect cons.  ##EQU00024##
<br/><br/> where denotes a set of must-link constraints; denotes a set of cannot-link constraints; f.sub.M(x.sub.p, x.sub.q) is a function that penalizes the violation of must-link constraint; and f.sub.C(x.sub.p, x.sub.q) is a penalty function for
cannot-links.
<br/><br/> If a Gibbs distribution [41] is assumed for S,
<br/><br/> .function..times..times.  ##EQU00025##
<br/><br/> where z.sub.1 is the normalization constant.  Since [4] focuses on only hard clustering, the soft member parameters may be omitted in the MMRC model to consider hard clustering.  Based on Eq.(30) and Eq.(4), the likelihood function of hard
semi-supervised clustering under MMRC model is
<br/><br/> .function..THETA..times..times..times..function..PHI..function..LAMBDA..t- imes..times.  ##EQU00026##
<br/><br/> Since C is an indicator matrix, Eq.  (31) can be formulated as
<br/><br/> .function..THETA..times..times..times..times..times..PHI..function..LAMBD- A. ##EQU00027##
<br/><br/> The above likelihood function is equivalent to the objective function of semi-supervised clustering based on HMRFs [4].  Furthermore, when applied to optimizing the objective function in Eq.(32), hard MMRC provides a family of semi-supervised
clustering algorithms similar to HMRF-K Means in [4]; on the other hand, soft EF-MMRC provides new and soft version semi-supervised clustering algorithms.
<br/><br/> 5.2 Co-Clustering
<br/><br/> Co-clustering or bi-clustering arise in many important applications, such as document clustering, micro-array data clustering.  A number of approaches [12, 8, 33, 2] have been proposed for co-clustering.  These efforts can be generalized as
solving the following matrix approximation problem [34],
<br/><br/> .times..times..UPSILON..times..times..function..times..UPSILON..times..ti- mes.  ##EQU00028##
<br/><br/> where R.epsilon..sup.n.sup.1.sup..times.n.sup.2 is the data matrix, C.sup.(1).epsilon.{0,1}.sup.k.sup.1.sup..times.n.sup.1 and C.sup.(2).epsilon.{0,1}.sup.k.sup.2.sup..times.n.sup.2 are indicator matrices,
.gamma..epsilon..sup.k.sup.1.sup..times.k.sup.2 is the relation representative matrix, and D is a distance function.  For example, [12] uses KL-divergences as the distance function; [8, 33] use Euclidean distances.
<br/><br/> Co-clustering is equivalent to clustering on relational data of one heterogeneous relation matrix R. Based on Eq.(9), by omitting the soft membership parameters, maximizing log-likelihood function of hard clustering on a heterogeneous relation
matrix under the MMRC model is equivalent to the minimization in (33).  The algorithms proposed in [12, 8, 33, 2] can be viewed as special cases of hard EF-MMRC.  At the same time, soft EF-MMRC provides another family of new algorithms for co-clustering.
<br/><br/> [34] proposes the relation summary network model for clustering k-partite graphs, which can be shown to be equivalent on clustering on relational data of multiple heterogeneous relation matrices.  The proposed algorithms in [34] can also be
viewed as special cases of the hard EF-MMRC algorithm.
<br/><br/> 5.3 Graph Clustering
<br/><br/> Graph clustering (partitioning) is an important problem in many domains, such as circuit partitioning, VLSI design, task scheduling.  Existing graph partitioning approaches are mainly based on edge cut objectives, such as Kernighan-Lin objective
[30], normalized cut [42], ratio cut [7], ratio association [42], and min-max cut [13].
<br/><br/> Graph clustering is equivalent to clustering on single-type relational data of one homogeneous relation matrix S. The log-likelihood function of the hard clustering under MMRC model is -D.sub..phi.(S,(C).sup.T.GAMMA.C).  We propose the following
theorem to show that the edge cut objectives are mathematically equivalent to a special case of the MMRC model.  Since most graph partitioning objective functions use weighted indicator matrix such that CC.sup.T=I.sub.k, where I.sub.k is an identity
matrix, we follow this formulation in the following theorem.
<br/><br/> Theorem 2.
<br/><br/> With restricting .GAMMA.  to be the form of rI.sub.k for r&gt;0, maximizing the log-likelihood of hard MMRC clustering on S under normal distribution, i.e.,
<br/><br/> .di-elect cons..times..times..times..times.  ##EQU00029##
<br/><br/> is equivalent to the trace maximization maxtr(CSC.sup.T), (35)
<br/><br/> where tr denotes the trace of a matrix.
<br/><br/> Proof.
<br/><br/> Let L denote the objective function in Eq.  (34).
<br/><br/> .times..times..times..function..times..times..times..times..function..tim- es..times..function..times..times..function..times..times..times..function- ..times..times..times..times..times.  ##EQU00030##
<br/><br/> The above deduction uses the property of trace tr(XY)=tr(YX).  Since tr(S.sup.TS), r and k are constants, the maximization of L is equivalent to the maximization of tr(CSC.sup.T).
<br/><br/> The proof is completed.
<br/><br/> Since it is shown in the literature [10] that the edge cut objectives can be formulated as the trace maximization, Theorem 2 states that edge-cut based graph clustering is equivalent to MMRC model under normal distribution with the diagonal
constraint on the parameter matrix .GAMMA..  This connection provides not only a new understanding for graph partitioning but also a family of new algorithms (soft and hard MMRC algorithms) for graph clustering.
<br/><br/> Finally, we point out that MMRC model does not exclude traditional attribute-based clustering.  When applied to an attribute data matrix under Euclidean distances, hard MMRC algorithm is actually reduced to the classic k-means; soft MMRC
algorithm is very close to the traditional mixture model EM clustering except that it does not involve mixing proportions in the computation.
<br/><br/> In summary, MMRC model provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering, semi-supervised clustering, co-clustering and graph clustering; soft and hard EF-MMRC algorithms
unify a number of state-of-the-art clustering algorithms and at the same time provide new solutions to various clustering tasks.
<br/><br/>6.  Experiments
<br/><br/> This section provides empirical evidence to show the effectiveness of the MMRC model and algorithms.  Since a number of state-of-the-art clustering algorithms [12, 8, 33, 2, 3, 4] can be viewed as special cases of EF-MMRC model and algorithms,
the experimental results in these efforts also illustrate the effectiveness of the MMRC model and algorithms.  MMRC algorithms are applied to tasks of graph clustering, bi-clustering, tri-clustering, and clustering on a general relational data set of all
three types of information.  In the experiments, mixed version MMRC was employed, i.e., hard MMRC initialization followed by soft MMRC.  Although MMRC can adopt various distribution assumptions, due to space limit, MMRC is used under normal or Poisson
distribution assumption in the experiments.  However, this does not imply that they are optimal distribution assumptions for the data.  Therefore, one can select or derive an optimal distribution assumption as may be appropriate.
<br/><br/> For performance measure, the Normalized Mutual Information (NMI) [44] between the resulting cluster labels and the true cluster labels was used, which is a standard way to measure the cluster quality.  The final performance score is the average
of ten runs.
<br/><br/> TABLE-US-00003 TABLE 1 Summary of relational data for Graph Clustering.  Name n k Balance Source tr11 414 9 0.046 TREC tr23 204 6 0.066 TREC NG1-20 14000 20 1.0 20-newsgroups k1b 2340 6 0.043 WebACE
<br/><br/> 6.1 Graph Clustering
<br/><br/> Experiments on the MMRC algorithm are presented under normal distribution in comparison with two representative graph partitioning algorithms, the spectral graph partitioning (SGP) from [36] that is generalized to work with both normalized cut
and ratio association, and the classic multilevel algorithm, METIS [28].
<br/><br/> The graphs based on the text data have been widely used to test graph partitioning algorithms [13, 11, 25].  In this study, we use various data sets from the 20-newsgroups [32], WebACE and TREC [27], which cover data sets of different sizes,
different balances and different levels of difficulties.  The data are pre-processed by removing the stop words and each document is represented by a term-frequency vector using TF-IDF weights.  Relational data are then constructed for each text data set
such that objects (documents) are related to each other with cosine similarities between the term-frequency vectors.  A summary of all the data sets to construct relational data used in this paper is shown in Table 1, in which n denotes the number of
objects in the relational data, k denotes the number of true clusters, and balance denotes the size ratio of the smallest clusters to the largest clusters.
<br/><br/> For the number of clusters k, the number of the true clusters is used.  Determining the optimal number of clusters analytically is a model selection problem, otherwise this may be determined empirically or iteratively.
<br/><br/> FIG. 2 shows the NMI comparison of the three algorithms.  Although there is no single winner on all the graphs, it may be observed overall that the MMRC algorithm performs better than SGP and METIS.  Especially on the difficult data set tr23,
MMRC increases performance about 30%.  Hence, MMRC under normal distribution provides a new graph partitioning algorithm which is viable and competitive compared with the two existing state-of-the-art graph partitioning algorithms.  Note that although
the normal distribution is most popular, MMRC under other distribution assumptions may be more desirable in specific graph clustering applications depends on the statistical properties of the graphs.
<br/><br/> TABLE-US-00004 TABLE 2 Subsets of Newsgroup Data for bi-type relational data Dataset # Documents Total # Name Newsgroups Included per Group Documents BT-NG1 rec.sport.baseball, 200 400 rec.sport.hockey BT-NG2 comp.os.ms-windows.misc, 200 1000
comp.windows.x, rec.motorcycles, sci.crypt, sci.space BT-NG3 comp.os.ms-windows.misc, 200 1600 comp.windows.x, misc.forsale, rec.motorcycles, rec.motorcycles, sci.crypt, sci.space, talk.politics.mideast, talk.religion.misc
<br/><br/> TABLE-US-00005 TABLE 3 Taxonomy structures of two data sets for constructing tri-partite relational data Data set Taxonomy structure TT-TM1 {rec.sport.baseball, rec.sport.hockey}, {talk.politics.guns, talk.politics.mideast, talk.politics.misc}
TT-TM2 {comp.graphics, comp.os.ms-windows.misc}, {rec.autos, rec.motorcycles}, {sci.crypt, sci.electronics}
<br/><br/> 6.2 Biclustering and Triclustering
<br/><br/> The MMRC algorithm are now applied under Poisson distribution to clustering bi-type relational data, word-document data, and tri-type relational data, word-document-category data.  Two algorithms, Bi-partite Spectral Graph partitioning (BSGP)
[11] and Relation Summary Network under Generalized I-divergence (RSN-GI) [34], are used as comparison in bi-clustering.  For tri-clustering, Consistent Bipartite Graph Co-partitioning (CBGC) [18] and RSN-GI are used as comparison.
<br/><br/> The bi-type relational data, word-document data, are constructed based on various subsets of the 20-Newsgroup data.  The data is pre-processed by selecting the top 2000 words by the mutual information.  The document-word matrix is based on
tf.idf weighting scheme and each document vector is normalized to a unit L.sub.2 norm vector.  Specific details of the data sets are listed in Table 2.  For example, for the data set BT-NG3 200 documents are randomly and evenly sampled from the
corresponding newsgroups; then a bi-type relational data set of 1600 document and 2000 word is formulated.
<br/><br/> The tri-type relational data are built based on the 20-newsgroups data for hierarchical taxonomy mining.  In the field of text categorization, hierarchical taxonomy classification is widely used to obtain a better trade-off between effectiveness
and efficiency than flat taxonomy classification.  To take advantage of hierarchical classification, one must mine a hierarchical taxonomy from the data set.  We see that words, documents, and categories formulate a sandwich structure tri-type relational
data set, in which documents are central type nodes.  The links between documents and categories are constructed such that if a document belongs to k categories, the weights of links between this document and these k category nodes are 1=k (please refer
[18] for details).  The true taxonomy structures for two data sets, TP-TM1 and TP-TM2, are documented in Table 3.
<br/><br/> TABLE-US-00006 TABLE 4 Two Clusters from actor-movie data cluster 23 of actors Viggo Mortensen, Sean Bean, Miranda Otto, Ian Holm, Christopher Lee, Cate Blanchett, Ian McKellen, Liv Tyler, David Wenham, Brad Dourif, John Rhys-Davies, Elijah
Wood, Bernard Hill, Sean Astin , Andy Serkis, Dominic Monaghan, Karl Urban, Orlando Bloom, Billy Boyd John Noble, Sala Baker cluster 118 of movies The Lord of the Rings: The Fellowship of the Ring (2001) The Lord of the Rings: The Two Towers (2002) The
Lord of the Rings: The Return of the King (2003)
<br/><br/> FIG. 3 and FIG. 4 show the NMI comparison of the three algorithms on bi-type and tri-type relational data, respectively.  It may be observed that the MMRC algorithm performs significantly better than BSGP and CBGC.  MMRC performs slightly better
than RSN on some data sets.  Since RSN is a special case of hard MMRC, this shows that mixed MMRC improves hard MMRC's performance on the data sets.  Therefore, compared with the existing state-of-the-art algorithms, the MMRC algorithm performs more
effectively on these bi-clustering or tri-clustering tasks and on the other hand, it is flexible for different types of multi-clustering tasks which may be more complicated than tri-type clustering.
<br/><br/> 6.3 A Case Study on Actor-Movie Data
<br/><br/> The MMRC algorithm was also run on the actor-movie relational data based on IMDB movie data set for a case study.  In the data, actors are related to each other by collaboration (homogeneous relations); actors are related to movies by taking
roles in movies (heterogeneous relations); movies have attributes such as release time and rating (note that there is no links between movies).  Hence the data have all the three types of information.  A data set of 20000 actors and 4000 movies is
formulated.  Experiments were run with k=200.  Although there is no ground truth for the data's cluster structure, it may be observed that most resulting clusters that are actors or movies of the similar style such as action, or tight groups from
specific movie serials.  For example, Table 4 shows cluster 23 of actors and cluster 118 of movies; the parameter .gamma..sub.23;118 shows that these two clusters are strongly related to each other.  In fact, the actor cluster contains the actors in the
movie series "The Lord of the Rings".  Note that if we only have one type of actor objects, we only get the actor clusters, but with two types of nodes, although there are no links between the movies, we also get the related movie clusters to explain how
the actors are related.
<br/><br/>7.  Conclusions
<br/><br/> A probabilistic model is formulated for relational clustering, which provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering, semi-supervised clustering, co-clustering and
graph clustering.  Under this model, parametric hard and soft relational clustering algorithms are presented under a large number of exponential family distributions.  The algorithms are applicable to relational data of various structures and at the same
time unify a number of state-of-the-art clustering algorithms.  The theoretic analysis and experimental evaluation show the effectiveness and great potential of the model and algorithms.
<br/><br/> The invention is applicable to various relational data from various applications.  It is capable of adapting different distribution assumptions for different relational data with different statistical properties.  While the above analysis
discuss in depth certain types of statistical distributions, the system and method may be used with any statistical distribution.  The resulting parameter matrices provides an intuitive summary for the hidden structure for relational data.  Therefore, in
addition to finding application in clustering data objects, the present system and method may be used for more general analysis of relationships of data, for other end purposes and/or as an intermediary step in a larger or more complex data analysis
paradigm.
<br/><br/> The present invention has significant versatility, and can be applied to a wide range of applications involving relational data.  Examples include, but are not limited to:
<br/><br/> (1) Clustering web documents using both text and link information;
<br/><br/> (2) Rating prediction in a recommendation system;
<br/><br/> (3) Community detection in social network analysis; and
<br/><br/> (4) Discovering gene patterns in bioinformatics application.
<br/><br/> The present method may be implemented on a general purpose computer or a specially adapted machine.  Typically, a programmable processor will execute machine-readable instructions stored on a computer-readable medium.  In other cases, the method
will be implemented using application specific hardware, and may not be reprogrammable.
<br/><br/> An exemplary programmable computing device for implementing an embodiment of the invention includes at least a processing unit and a memory.  Depending on the exact configuration and type of computing device, the memory may be volatile (such as
RAM), non-volatile (such as ROM, flash memory, etc.) or some combination of the two.  Additionally, the device may also have additional features/functionality.  For example, the device may also include additional storage (removable and/or non-removable)
including, but not limited to, magnetic or optical disks or tapes.  Computer storage media includes volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer readable
instructions, data structures, program modules or other data.  The memory, the removable storage and the non-removable storage are all examples of computer storage media.  Computer storage media includes, but is not limited to, RAM, ROM, EEPROM, flash
memory, FRAM, or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which can be used to store the
desired information and which can accessed by the device.  The device may also contain one or more communications connections that allow the device to communicate with other devices.  Such communication connections may include, for example, Ethernet,
wireless communications, optical communications, serial busses, parallel busses, and the like.  Communication media typically embodies computer readable instructions, data structures, program modules or other data in a modulated data signal such as a
carrier wave or other transport mechanism and includes any information delivery media.  The term "modulated data signal" means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. 
By way of example, and not limitation, communication media includes wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, RF, infrared and other wireless media.  As discussed above, the term computer
readable media as used herein includes both storage media and communication media.
<br/><br/> One use for the present method is to process information databases, which may be private or public.  For example, the information database may comprise information received from the Internet, such as the content of various web pages from world
wide web sites, or other information found on the Internet.  In other cases, the data may be more structured, for example the content of the <b><i>Facebook</i></b> social networking site/system.  Further, the information may be private user information, such as the
contents of a user's hard drive, especially, for example, the user generated or downloaded content.
<br/><br/> Having described specific embodiments of the present invention, it will be understood that many modifications thereof will readily appear or may be suggested to those skilled in the art, and it is intended therefore that this invention is
limited only by the spirit and scope of the following claims.
<br/><br/>8.  References
<br/><br/> The following are expressly incorporated herein by reference: Bo Long Mark (Zhongfei) Zhang, Philip S. Yu, "Graph Partitioning Based on Link Distributions", AAAI (2007).  Bo Long Mark (Zhongfei) Zhang, Philip S. Yu, "A Probabilistic Framework
for Relational Clustering" KDD (2007).  Bo Long Mark (Zhongfei) Zhang, Xiaoyun Wu, Philip S. Yu, "Relational Clustering by Symmetric Convex Coding".  Proceedings of the 24th International Conference on Machine Learning, Corvallis, Oreg.  (2007).  [1] E.
Airoldi, D. Blei, E. Xing, and S. Fienberg.  Mixed membership stochastic block models for relational data with application to protein-protein interactions.  In ENAR-2006.  [2] A. Banerjee, I. S. Dhillon, J. Ghosh, S. Merugu, and D. S. Modha.  A
generalized maximum entropy approach to bregman co-clustering and matrix approximation.  In KDD, pages 509-514, 2004.  [3] A. Banerjee, S. Merugu, I. S. Dhillon, and J. Ghosh.  Clustering with bregman divergences.  J. Mach.  Learn.  Res., 6:1705-1749,
2005.  [4] S. Basu, M. Bilenko, and R. J. Mooney.  A probabilistic framework for semi-supervised clustering.  In KDD04, pages 59-68, 2004.  [5] I. Bhattachrya and L. Getor.  Entity resolution in graph data.  Technical Report CS-TR-4758, University of
Maryland, 2005.  [6] T. N. Bui and C. Jones.  A heuristic for reducing fill-in in sparse matrix factorization.  In PPSC, pages 445-452, 1993.  [7] P. K. Chan, M. D. F. Schlag, and J. Y. Zien.  Spectral k-way ratio-cut partitioning and clustering.  In DAC
'93.  [8] H. Cho, I. Dhillon, Y. Guan, and S. Sra.  Minimum sum squared residue co-clustering of gene expression data.  In SDM, 2004.  [9] M. Collins, S. Dasgupta, and R. Reina.  A generalization of principal component analysis to the exponential family. In NIPS'01, 2001.  [10] I. Dhillon, Y. Guan, and B. Kulis.  A unified view of kernel k-means, spectral clustering and graph cuts.  Technical Report TR-04-25, University of Texas at Austin, 2004.  [11] I. S. Dhillon.  Co-clustering documents and words
using bipartite spectral graph partitioning.  In KDD'01.  [12] I. S. Dhillon, S. Mallela, and D. S. Modha.  Information-theoretic co-clustering.  In KDD'03, pages 89-98.  [13] C. H. Q. Ding, X. He, H. Zha, M. Gu, and H. D. Simon.  A min-max cut algorithm
for graph partitioning and data clustering.  In Proceedings of ICDM 2001, pages 107-114, 2001.  [14] S. Dzeroski and N. Lavrac, editors.  Relational Data Mining.  Springer, 2001.  [15] E. Erosheva, S. Fienberg, and J. Lafferty.  Mixed membership models
of scientific publications.  In NAS.  [16] E. Erosheva and S. E. Fienberg.  Bayesian mixed membership models for soft clustering and classification.  Classification--The Ubiquitous Challenge, pages 11-26, 2005.  [17] S. E. Fienberg, M. M. Meyer, and S.
Wasserman.  Satistical analysis of multiple cociometric relations.  Journal of American Satistical Association, 80:51-87, 1985.  [18] B. Gao, T.-Y. Liu, X. Zheng, Q.-S. Cheng, and W.-Y. Ma.  Consistent bipartite graph co-partitioning for star-structured
high-order heterogeneous data co-clustering.  In KDD '05, pages 41-50, 2005.  [19] L. Getoor.  An introduction to probabilistic graphical models for relational data.  Data Engineering Bulletin, 29, 2006.  [20] B. Hendrickson and R. Leland.  A multilevel
algorithm for partitioning graphs.  In Supercomputing '95, page 28, 1995.  [21] P. Hoff, A. Rafery, and M. Handcock.  Latent space approaches to social network analysis.  Journal of American Satistical Association, 97:1090-1098, 2002.  [22] T. Hofmann. 
Probabilistic latent semantic analysis.  In Proc.  of Uncertainty in Artificial Intelligence, UAI'99, Stockholm, 1999.  [23] T. Hofmann and J. Puzicha.  Latent class models for collaborative filtering.  In IJCAI'99, Stockholm, 1999.  [24] L. B. Holder
and D. J. Cook.  Graph-based relational learning: current and future directions.  SIGKDD Explor.  Newsl., 5(1):90-93, 2003.  [25] M. X. H. Zha, C. Ding and H. Simon.  Bi-partite graph partitioning and data clustering.  In ACM CIKM'01, 2001.  [26] G. Jeh
and J. Widom.  Simrank: A measure of structural-context similarity.  In KDD-2002, 2002.  [27] G. Karypis.  A clustering toolkit, 2002.  [28] G. Karypis and V. Kumar.  A fast and high quality multilevel scheme for partitioning irregular graphs.  SIAM J.
Sci.  Comput., 20(1):359-392, 1998.  [29] M. Kearns, Y. Mansour, and A. Ng.  An information-theoretic analysis of hard and soft assignment methods for clustering.  In UAI'97, pages 282-293, 2004.  [30] B. Kernighan and S. Lin.  An efficient heuristic
procedure for partitioning graphs.  The Bell System Technical Journal, 49(2):291-307, 1970.  [31] M. Kirsten and S. Wrobel.  Relational distance-based clustering.  In Proc.  Fachgruppentreffen Maschinelles Lernen (FGML-98), pages 119-124, 1998.  [32] K.
Lang.  News weeder: Learning to filter netnews.  In ICML, 1995.  [33] T. Li.  A general model for clustering binary data.  In KDD'05, 2005.  [34] B. Long, X. Wu, Z. M. Zhang, and P. S. Yu.  Unsupervised learning on k-partite graphs.  In KDD-2006, 2006. 
[35] B. Long, Z. Zhang, and P. Yu.  Co-clustering by block value decomposition.  In KDD'05, 2005.  [36] A. Ng, M. Jordan, and Y. Weiss.  On spectral clustering: Analysis and an algorithm.  In Advances in Neural Information Processing Systems 14, 2001. 
[37] L. D. Raedt and H. Blockeel.  Using logical decision trees for clustering.  In Proceedings of the 7th International Workshop on Inductive Logic Programming, 1997.  [38] R. O. Duda, P. E. Hart, and D. G. Stork.  Pattern Classification.  John Wiley &amp;
Sons, New York, 2000.  [39] N. Rosenberg, J. Pritchard, J. Weber, and H. Cann.  Genetic structure of human population.  Science, 298, 2002.  [40] J. S. D. Pietra, V. D. Pietera.  Duality and auxiliary functions for bregman distances.  Technical Report
CMU-CS-01-109, Carnegie Mellon University, 2001.  [41] S. Geman and D. Geman.  Stochastic relaxation, gibbs distribution, and the bayesian restoration of images.  Pattern Analysis and Machine Intelligence, 6:721-742, 1984.  [42] J. Shi and J. Malik. 
Normalized cuts and image segmentation.  IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888-905, 2000.  [43] T. Snijders.  Markov chain monte carlo estimation of exponential random graph models.  Journal of Social Structure, 2002. 
[44] A. Strehl and J. Ghosh.  Cluster ensembles { a knowledge reuse framework for combining partitionings.  In AAAI 2002, pages 93-98, 2002.  [45] B. Taskar, E. Segal, and D. Koller.  Probabilistic classification and clustering in relational data.  In
Proceeding of IJCAI-01, 2001.  [46] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl.  Constrained k-means clustering with background knowledge.  In ICML-2001, pages 577-584, 2001.  [47] J. Wang, H. Zeng, Z. Chen, H. Lu, L. Tao, and W.-Y. Ma.  Recom:
reinforcement clustering of multi-type interrelated data objects.  In SIGIR '03, pages 274-281, 2003.  [48] E. Xing, A. Ng, M. Jorda, and S. Russel.  Distance metric learning with applications to clustering with side information.  In NIPS'03, volume 16,
2003.  [49] X. Yin, J. Han, and P. Yu.  Cross-relational clustering with user's guidance.  In KDD-2005, 2005.  [50] X. Yin, J. Han, and P. Yu.  Linkclus: Efficient clustering via heterogeneous semantic links.  In VLDB-2006, 2006.  [51] H.-J. Zeng, Z.
Chen, and W.-Y. Ma.  A unified framework for clustering heterogeneous web objects.  In WISE '02, pages 161-172, 2002.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09372915&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D252%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D6%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D252%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D6%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209372915"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9372915&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D252%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D6%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209372915">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=252&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=5&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=252&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=6&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=252&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=7&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=251&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=6&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=253&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=6&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></coma></body></html>