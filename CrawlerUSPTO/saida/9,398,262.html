<html><head>
<base target="_top"/>
<title>United States Patent: 9398262</title></head>
<!---BUF1=9398262
BUF7=2016
BUF8=50628
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=11&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=1&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=11&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=2&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=10&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=1&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=12&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=1&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D11%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D1%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209398262"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9398262&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D11%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D1%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209398262">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09398262&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D11%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D1%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>11</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,398,262</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
         Li
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     July 19, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">Communication using avatar
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> Generally this disclosure describes a video communication system that
     replaces actual live images of the participating users with animated
     avatars. A method may include selecting an avatar, initiating
     communication, capturing an image, detecting a face in the image,
     extracting features from the face, converting the facial features to
     avatar parameters, and transmitting at least one of the avatar selection
     or avatar parameters.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Li; Wenlong</b> (Beijing, <b>CN</b>)<b>, Tong; Xioafeng</b> (Beijing, <b>CN</b>)<b>, Du; Yangzhou</b> (Beijing, <b>CN</b>)<b>, Li; Qiang Eric</b> (Beijing, <b>CN</b>)<b>, Zhang; Yimin</b> (Beijing, <b>CN</b>)<b>, Hu; Wei</b> (Beijing, <b>CN</b>)<b>, Tennant; John G.</b> (El Dorado Hills, CA)<b>, Li; Hui A.</b> (Antioch, CA) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/>Li; Wenlong
<br/>Tong; Xioafeng
<br/>Du; Yangzhou
<br/>Li; Qiang Eric
<br/>Zhang; Yimin
<br/>Hu; Wei
<br/>Tennant; John G.
<br/>Li; Hui A.</b> </td><td> <br/>Beijing
<br/>Beijing
<br/>Beijing
<br/>Beijing
<br/>Beijing
<br/>Beijing
<br/>El Dorado Hills
<br/>Antioch </td><td align="center"> <br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A
<br/>CA
<br/>CA </td><td align="center"> <br/>CN
<br/>CN
<br/>CN
<br/>CN
<br/>CN
<br/>CN
<br/>US
<br/>US </td> <td align="left"> </td> </tr> </tbody></table>
<!-- AANM>
~AANM Li; Wenlong
~AACI Beijing
~AAST N/A
~AACO CN
~AANM Tong; Xioafeng
~AACI Beijing
~AAST N/A
~AACO CN
~AANM Du; Yangzhou
~AACI Beijing
~AAST N/A
~AACO CN
~AANM Li; Qiang Eric
~AACI Beijing
~AAST N/A
~AACO CN
~AANM Zhang; Yimin
~AACI Beijing
~AAST N/A
~AACO CN
~AANM Hu; Wei
~AACI Beijing
~AAST N/A
~AACO CN
~AANM Tennant; John G.
~AACI El Dorado Hills
~AAST CA
~AACO US
~AANM Li; Hui A.
~AACI Antioch
~AAST CA
~AACO US
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b>Intel Corporation</b>
 (Santa Clara, 
CA)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>1000001984089
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>13/993,612</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>December 29, 2011</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">PCT Filed:
       </th><td align="left" width="90%"><b>
       December 29, 2011
       </b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">PCT No.:
       </th><td align="left" width="90%"><b>
       PCT/CN2011/084902
       </b></td></tr>
         <tr><th align="left" scope="row" valign="top" width="15%">371(c)(1),(2),(4) Date:
         </th><td align="left" width="85%"><b>
         April 14, 2014
         </b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">PCT Pub. No.:
       </th><td align="left" width="90%">
       <b>
       WO2013/097139
       </b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">PCT Pub. Date:
       </th><td align="left" width="90%">
       <b>
       July 04, 2013
       </b></td></tr>
     </tbody></table>
<hr/> <center><b>Prior Publication Data</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Document Identifier</u></b></th><th align="center" scope="col"><b><u>Publication Date</u></b></th></tr><tr><td align="center"> </td><td align="center"> US 20140218459 A1</td><td align="center">Aug 7, 2014</td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table>
     <hr/>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">H04N 7/157 (20130101); G06K 9/00248 (20130101); H04N 21/4223 (20130101); H04N 21/44008 (20130101); H04N 21/4788 (20130101); H04N 21/8146 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">H04N 7/15 (20060101); H04N 21/4223 (20110101); H04N 21/44 (20110101); H04N 21/4788 (20110101); H04N 21/81 (20110101); G06K 9/00 (20060101)</td></tr>
       <tr><td align="left" valign="top" width="30%"><b>Field of Search: </b></td>
       <td align="right" valign="top" width="70%">
       
 ;348/14.01
       </td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9398262">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6580811">6580811</a></td><td align="left">
June 2003</td><td align="left">
Maurer et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F7076118">7076118</a></td><td align="left">
July 2006</td><td align="left">
Westerman</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080059570&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0059570</a></td><td align="left">
March 2008</td><td align="left">
Bill</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090055484&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0055484</a></td><td align="left">
February 2009</td><td align="left">
Vuong et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20120206558&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2012/0206558</a></td><td align="left">
August 2012</td><td align="left">
Setton</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20130109302&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2013/0109302</a></td><td align="left">
May 2013</td><td align="left">
Levien et al.</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
       <center><b>Foreign Patent Documents</b></center>
<table width="100%"> <tbody><tr><td></td><th scope="col"></th> <td></td><th scope="col"></th> <td></td><th scope="col"></th></tr> <tr> <td align="left">
</td><td align="left">1972274</td><td></td><td align="left">
May 2007</td><td></td><td align="left">
CN</td></tr><tr><td align="left">
</td><td align="left">101472158</td><td></td><td align="left">
Jul 2009</td><td></td><td align="left">
CN</td></tr><tr><td align="left">
</td><td align="left">102087750</td><td></td><td align="left">
Jun 2011</td><td></td><td align="left">
CN</td></tr><tr><td align="left">
</td><td align="left">2012139276</td><td></td><td align="left">
Oct 2012</td><td></td><td align="left">
WO</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
<table width="90%">   <tbody><tr><td><align="left"><br/>International Search Report and Written Opinion received for PCT Patent Application No. PCT/CN2011/084902, mailed on Oct. 18, 2012, 10 Pages. cited by
applicant
.<br/>"Evaluation of Face Recognition Algorithms", Home Algorithms Data Related, Colorado State University, 2010, retrieved on Dec. 22, 2011, 1 page, available at: http://www.cs.colostate.edu/evalfacerec/index10.php. cited by applicant
.<br/>3D Face Modeller: 3D Face Generator, downloaded from www.facegen.com/modeller. cited by applicant
.<br/>3D PhotoFace, downloaded from www.oddcase.com/technologies/photoface. cited by applicant. </align="left"></td></tr> </tbody></table><br/><center><b>Other References</b></center> <br/>
       <i>Primary Examiner:</i> Nguyen; Joseph J
<br/>
       <i>Attorney, Agent or Firm:</i> <coma>Grossman, Tucker, Perreault &amp; Pfleger, PLLC
<br/>
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>What is claimed: <br/><br/> 1.  A system, comprising: a display;  a camera configured to capture a first and second image;  a communication circuitry configured to transmit and receive information;  and
one or more non-transitory storage memories having stored thereon, individually or in combination, instructions that when executed by one or more processors result in the following operations comprising: initiating a communication link with a remote
system;  and during the communication link, detecting a face of a user in each of the first and the second images;  extracting a plurality of facial features from the face detected in the first image;  extracting a plurality of facial features from the
face detected in the second image;  converting the facial features extracted from the first image to a first set of a plurality of avatar parameters;  converting the facial features extracted from the second image to a second set of a plurality of avatar
parameters;  causing the first set of the plurality of avatar parameters to be transmitted to the remote system;  after causing the first set of the plurality of avatar parameters to be transmitted, causing the second set of the plurality of avatar
parameters to be transmitted to the remote system to cause a remote avatar on the remote system to be substantially continuously animated;  and generating a feedback avatar on said display of said system that is substantially continuously animated based
on the first set and the second set of the plurality of avatar parameters, said feedback avatar corresponding to said remote avatar being displayed on said remote system.
<br/><br/> 2.  The system of claim 1, wherein extracting features from the face comprises determining a facial expression in the face.
<br/><br/> 3.  The system of claim 1, wherein the avatar parameters are used to generate an avatar on a remote device, the avatar being based on the facial features.
<br/><br/> 4.  The system of claim 1, wherein the avatar parameters are used to generate an avatar in a virtual space, the avatar being based on the facial features.
<br/><br/> 5.  The system of claim 1, wherein the instructions that when executed by one or more processors result in the following additional operations: receiving at least one of a remote avatar selection or remote avatar parameters.
<br/><br/> 6.  The system of claim 5, further comprising a display, wherein the instructions that when executed by one or more processors result in the following additional operations: displaying an avatar based on the remote avatar selection.
<br/><br/> 7.  The system of claim 6, wherein the instructions that when executed by one or more processors result in the following additional operations: animating the displayed avatar based on the remote avatar parameters.
<br/><br/> 8.  A system comprising one or more non-transitory storage memories having stored thereon, individually or in combination, instructions that when executed by one or more processors result in the following operations comprising: initiating a
communication link between a user system and a remote system;  capturing a first and a second image;  and during the communication link, detecting a face of a user in each of the first and the second images;  extracting a plurality of facial features
from the face detected in the first image;  extracting a plurality of facial features from the face detected in the second image;  converting the facial features extracted from the first image to a first set of a plurality of avatar parameters; 
converting the facial features extracted from the second image to a second set of a plurality of avatar parameters;  causing the first set of the plurality of avatar parameters to be transmitted to the remote system;  after causing the first set of the
plurality of avatar parameters to be transmitted, causing the second set of the plurality of avatar parameters to be transmitted to the remote system to cause a remote avatar on the remote system to be substantially continuously animated;  and generating
a feedback avatar on a display of said user system that is substantially continuously animated based on the first set and the second set of the plurality of avatar parameters, said feedback avatar corresponding to said remote avatar being displayed on
said remote system.
<br/><br/> 9.  The system of claim 8, wherein extracting features from the face comprises determining a facial expression in the face.
<br/><br/> 10.  The system of claim 8, wherein the avatar parameters are used to generate an avatar on a remote device, the avatar being based on the facial features.
<br/><br/> 11.  The system of claim 8, wherein the avatar parameters are used to generate an avatar in a virtual space, the avatar being based on the facial features.
<br/><br/> 12.  The system of claim 8, wherein the instructions that when executed by one or more processors result in the following additional operations: receiving at least one of a remote avatar selection or remote avatar parameters.
<br/><br/> 13.  The system of claim 12, further comprising a display, wherein the instructions that when executed by one or more processors result in the following additional operations: displaying an avatar based on the remote avatar selection.
<br/><br/> 14.  The system of claim 13, wherein the instructions that when executed by one or more processors result in the following additional operations: animating the displayed avatar based on the remote avatar parameters.
<br/><br/> 15.  A method, comprising: initiating a communication link between a user system and a remote system;  capturing a first and a second image with a camera of said user system;  and during the communication link, detecting a face of a user in each
of the first and the second images;  extracting a plurality of facial features from the face detected in the first image;  extracting a plurality of facial features from the face detected in the second image;  converting the facial features extracted
from the first image to a first set of a plurality of avatar parameters;  converting the facial features extracted from the second image to a second set of a plurality of avatar parameters;  transmitting the first set of the plurality of avatar
parameters to the remote system;  after transmitting the first set of the plurality of avatar parameters, transmitting the second set of the plurality of avatar parameters to the remote system to cause a remote-avatar on the remote system to be
substantially continuously animated;  and generating a feedback avatar on a display of said user system that is substantially continuously animated based on the first set and the second set of the plurality of avatar parameters, said feedback avatar
corresponding to said remote avatar being displayed on said remote system.
<br/><br/> 16.  The method of claim 15, wherein extracting features from the face comprises determining a facial expression in the face.
<br/><br/> 17.  The method of claim 15, wherein the avatar parameters are used to generate an avatar on a remote device, the avatar being based on the facial features.
<br/><br/> 18.  The method of claim 15, wherein the avatar parameters are used to generate an avatar in a virtual space, the avatar being based on the facial features.
<br/><br/> 19.  The method of claim 15, further comprising receiving at least one of a remote avatar selection or remote avatar parameters.
<br/><br/> 20.  The method of claim 19, further comprising displaying an avatar based on the remote avatar selection.
<br/><br/> 21.  The method of claim 20, further comprising animating the displayed avatar based on the remote avatar parameters. <hr/> <center><b><i>Description</i></b></center> <hr/> <br/><br/>FIELD
<br/><br/> The following disclosure relates to video communication and interaction, and, more particularly, to methods and for video communication and interaction using avatars.
<br/><br/>BACKGROUND
<br/><br/> The increasing variety of functionality available in mobile devices has spawned a desire for users to communicate via video in addition to simple calls.  For example, users may initiate "video calls," "videoconferencing," etc., wherein a camera
and microphone in a device transmits audio and real-time video of a user to one or more other recipients such as other mobile devices, desktop computers, videoconferencing systems, etc. The communication of real time video may involve the transmission of
substantial amounts of data (e.g., depending on the technology of the camera, the particular video codec employed to process the real time image information, etc.).  Given the bandwidth limitations of existing 2G/3G wireless technology, and the still
limited availability of emerging 4G wireless technology, the proposition of many device users conducting concurrent video calls places a large burden on bandwidth in the existing wireless communication infrastructure, which may impact negatively on the
quality of the video call. <br/><br/>BRIEF DESCRIPTION OF THE DRAWINGS
<br/><br/> Features and advantages of various embodiments of the claimed subject matter will become apparent as the following Detailed Description proceeds, and upon reference to the Drawings, wherein like numerals designate like parts, and in which:
<br/><br/> FIG. 1A illustrates an example device-to-device system in accordance with various embodiments of the present disclosure;
<br/><br/> FIG. 1B illustrates an example virtual space system in accordance with various embodiments of the present disclosure;
<br/><br/> FIG. 2 illustrates an example device in accordance with various embodiments of the present disclosure;
<br/><br/> FIG. 3 illustrates an example system implementation in accordance with at least one embodiment of the present disclosure; and
<br/><br/> FIG. 4 is a flowchart of example operations in accordance with at least one embodiment of the present disclosure.
<br/><br/> Although the following Detailed Description will proceed with reference being made to illustrative embodiments, many alternatives, modifications and variations thereof will be apparent to those skilled in the art.
<br/><br/>DETAILED DESCRIPTION
<br/><br/> Generally, this disclosure describes systems and methods for video communication and interaction using avatars.  Using avatars, as opposed to live images, substantially reduces the amount of data to be transmitted, and thus, the avatar
communication requires less bandwidth.  In one embodiment an application is activated in a device coupled to a camera.  The application may be configured to allow a user to select an avatar for display on a remote device, in a virtual space, etc. The
device may then be configured to initiate communication with at least one other device, a virtual space, etc. For example, the communication may be established over a 2G, 3G, 4G cellular connection.  Alternatively, the communication may be established
over the Internet via a WiFi connection.  After the communication is established, the camera may be configured to start capturing images.  Facial detection/tracking is then performed on the captured images, and feature extraction is performed on the
face.  The detected face/head movements and/or changes in facial features are then converted into parameters usable for animating the avatar on the at least one other device, within the virtual space, etc. At least one of the avatar selection or avatar
parameters are then transmitted.  In one embodiment at least one of a remote avatar selection or remote avatar parameters are received.  The remote avatar selection may cause the device to display an avatar, while the remote avatar parameters may cause
the device to animate the displayed avatar.  Audio communication accompanies the avatar animation via known methods.
<br/><br/> FIG. 1A illustrates device-to-device system 100 consistent with various embodiments of the present disclosure.  System 100 may generally include devices 102 and 112 communicating via network 122.  Device 102 includes at least camera 104,
microphone 106 and display 108.  Device 112 includes at least camera 114, microphone 116 and display 118.  Network 122 includes at least server 124.
<br/><br/> Devices 102 and 112 may include various hardware platforms that are capable of wired and/or wireless communication.  For example, devices 102 and 112 may include, but are not limited to, videoconferencing systems, desktop computers, laptop
computers, tablet computers, smart phones, (e.g., iPhones.RTM., Android.RTM.-based phones, Blackberries.RTM., Symbian.RTM.-based phones, Palm.RTM.-based phones, etc.), cellular handsets, etc. Cameras 104 and 114 include any device for capturing digital
images representative of an environment that includes one or more persons, and may have adequate resolution for face analysis of the one or more persons in the environment as described herein.  For example, cameras 104 and 114 may include still cameras
(e.g., cameras configured to capture still photographs) or a video cameras (e.g., cameras configured to capture a moving images comprised of a plurality of frames).  Cameras 104 and 114 may be configured to operate using light in the visible spectrum or
with other portions of the electromagnetic spectrum not limited to the infrared spectrum, ultraviolet spectrum, etc. Cameras 104 and 114 may be incorporated within devices 102 and 112, respectively, or may be separate devices configured to communicate
with devices 102 and 112 via wired or wireless communication.  Specific examples of cameras 104 and 114 may include wired (e.g., Universal Serial Bus (USB), Ethernet, Firewire, etc.) or wireless (e.g., WiFi, Bluetooth, etc.) web cameras as may be
associated with computers, video monitors, etc., mobile device cameras (e.g., cell phone or smart phone cameras integrated in, for example, the previously discussed example devices), integrated laptop computer cameras, integrated tablet computer cameras
(e.g., iPad.RTM., Galaxy Tab.RTM., and the like), etc. Devices 102 and 112 may further comprise microphones 106 and 116.
<br/><br/> Microphones 106 and 116 include any devices configured to sense sound.  Microphones 106 and 116 may be integrated within devices 102 and 112, respectively, or may interact with the devices via wired or wireless communication such as described in
the above examples regarding cameras 104 and 114.  Displays 108 and 118 include any devices configured to display text, still images, moving images (e.g., video), user interfaces, graphics, etc. Displays 108 and 118 may be integrated within devices 102
and 112, respectively, or may interact with the devices via wired or wireless communication such as described in the above examples regarding cameras 104 and 114.  In one embodiment, displays 108 and 118 are configured to display avatars 110 and 120,
respectively.  As referenced herein, an Avatar is defined as graphical representation of a user in either two-dimensions (2D) or three-dimensions (3D).  Avatars do not have to resemble the looks of the user, and thus, while avatars can be lifelike
representations they can also take the form of drawings, cartoons, sketches, etc. In system 100, device 102 may display avatar 110 representing the user of device 112 (e.g., a remote user), and likewise, device 112 may display avatar 120 representing the
user of device 102.  In this way users may see a representation of others user without having to exchange the large amounts of information involved with device-to-device communication employing live images.
<br/><br/> Network 122 may include various second generation (2G), third generation (3G), fourth generation (4G) cellular-based data communication technologies, Wi-Fi wireless data communication technology, etc. Network 122 includes at least one server 124
configured to establish and maintain communication connections when using these technologies.  For example, server 124 may be configured to support Internet-related communication protocols like Session Initiation Protocol (SIP) for creating, modifying
and terminating two-party (unicast) and multi-party (multicast) sessions, Interactive Connectivity Establishment Protocol (ICE) for presenting a framework that allows protocols to be built on top of bytestream connections, Session Traversal Utilities for
Network Access Translators, or NAT, Protocol (STUN) for allowing applications operating through a NAT to discover the presence of other NATs, IP addresses and ports allocated for an application's User Datagram Protocol (UDP) connection to connect to
remote hosts, Traversal Using Relays around NAT (TURN) for allowing elements behind a NAT or firewall to receive data over Transmission Control Protocol (TCP) or UDP connections, etc.
<br/><br/> FIG. 1B illustrates virtual space system 126 consistent with various embodiments of the present disclosure.  System 126 may employ device 102, device 112 and server 124.  Device 102, device 112 and server 124 may continue to communicate in the
manner similar to that illustrated in FIG. 1A, but user interaction may take place in virtual space 128 instead of in a device-to-device format.  As referenced herein, a virtual space may be defined as a digital simulation of a physical location.  For
example, virtual space 128 may resemble an outdoor location like a city, road, sidewalk, field, forest, island, etc., or an inside location like an office, house, school, mall, store, etc. Users, represented by avatars, may appear to interact in virtual
space 128 as in the real world.  Virtual space 128 may exist on one or more servers coupled to the Internet, and may be maintained by a third party.  Examples of virtual spaces include virtual offices, virtual meeting rooms, virtual worlds like Second
Life.RTM., massively multiplayer online role-playing games (MMORPGs) like World of Warcraft.RTM., massively multiplayer online real-life games (MMORLGs), like The Sims Online.RTM., etc. In system 126, virtual space 128 may contain a plurality of avatars
corresponding to different users.  Instead of displaying avatars, displays 108 and 118 may display encapsulated (e.g., smaller) versions of virtual space (VS) 128.  For example, display 108 may display a perspective view of what the avatar corresponding
to the user of device 102 "sees" in virtual space 128.  Similarly, display 118 may display a perspective view of what the avatar corresponding to the user of device 112 "sees" in virtual space 128.  Examples of what avatars might see in virtual space 128
include, but are not limited to, virtual structures (e.g., buildings), virtual vehicles, virtual objects, virtual animals, other avatars, etc.
<br/><br/> FIG. 2 illustrates an example device 102 in accordance with various embodiments of the present disclosure.  While only device 102 is described, device 112 (e.g., remote device) may include resources configured to provide the same or similar
functions.  As previously discussed, device 102 is shown including camera 104, microphone 106 and display 108.  Camera 104 and microphone 106 may provide input to camera and audio framework module 200.  Camera and audio framework module 200 may include
custom, proprietary, known and/or after-developed audio and video processing code (or instruction sets) that are generally well-defined and operable to control at least camera 104 and microphone 106.  For example, camera and audio framework module 200
may cause camera 104 and microphone 106 to record images and/or sounds, may process images and/or sounds, may cause images and/or sounds to be reproduced, etc. Camera and audio framework module 200 may vary depending on device 102, and more particularly,
the operating system (OS) running in device 102.  Example operating systems include iOS.RTM., Android.RTM., Blackberry.RTM.  OS, Symbian.RTM., Palm.RTM.  OS, etc. Speaker 202 may receive audio information from camera and audio framework module 200 and
may be configured to reproduce local sounds (e.g., to provide audio feedback of the user's voice) and remote sounds (e.g., the sound of the other parties engaged in a telephone, video call or interaction in a virtual place).
<br/><br/> Facial detection and tracking module 204 may be configured to identify and track a head, face and/or facial region within image(s) provided by camera 104.  For example, facial detection module 204 may include custom, proprietary, known and/or
after-developed face detection code (or instruction sets), hardware, and/or firmware that are generally well-defined and operable to receive a standard format image (e.g., but not limited to, a RGB color image) and identify, at least to a certain extent,
a face in the image.  Facial detection and tracking module 204 may also be configured to track the detected face through a series of images (e.g., video frames at 24 frames per second) and to determine a head position based on the detected face.  Known
tracking systems that may be employed by facial detection/tracking module 104 may include particle filtering, mean shift, Kalman filtering, etc., each of which may utilize edge analysis, sum-of-square-difference analysis, feature point analysis,
histogram analysis, skin tone analysis, etc.
<br/><br/> Feature extraction module 206 may be configured to recognize features (e.g., the location and/or shape of facial landmarks such as eyes, eyebrows, nose, mouth, etc.) in the face detected by face detection module 204.  In one embodiment, avatar
animation may be based directly on sensed facial actions (e.g., changes in facial features) without facial expression recognition.  The corresponding feature points on an avatar's face may follow or mimic the movements of the real person's face, which is
known as "expression clone" or "performance-driven facial animation." Feature extraction module 206 may include custom, proprietary, known and/or after-developed facial characteristics recognition code (or instruction sets) that are generally
well-defined and operable to receive a standard format image (e.g., but not limited to a RGB color image) from camera 104 and to extract, at least to a certain extent, one or more facial characteristics in the image.  Such known facial characteristics
systems include, but are not limited to, the CSU Face Identification Evaluation System by Colorado State University.
<br/><br/> Feature extraction module 206 may also be configured to recognize an expression associated with the detected features (e.g., identifying whether a previously detected face happy, sad, smiling, frown, surprised, excited, etc.)).  Thus, feature
extraction module 206 may further include custom, proprietary, known and/or after-developed facial expression detection and/or identification code (or instruction sets) that is generally well-defined and operable to detect and/or identify expressions in
a face.  For example, feature extraction module 206 may determine size and/or position of the facial features (e.g., eyes, mouth, cheeks, teeth, etc.) and may compare these facial features to a facial feature database which includes a plurality of sample
facial features with corresponding facial feature classifications (e.g., smiling, frown, excited, sad, etc.).
<br/><br/> Avatar selection module 208 is configured to allow a user of device 102 to select an avatar for display on a remote device.  Avatar selection module 208 may include custom, proprietary, known and/or after-developed user interface construction
code (or instruction sets) that are generally well-defined and operable to present different avatars to a user so that the user may select one of the avatars.  In one embodiment one or more avatars may be predefined in device 102.  Predefined avatars
allow all devices to have the same avatars, and during interaction only the selection of an avatar (e.g., the identification of a predefined avatar) needs to be communicated to a remote device or virtual space, which reduces the amount of information
that needs to be exchanged.  Avatars are selected prior to establishing communication, but may also be changed during the course of an active communication.  Thus, it may be possible to send or receive an avatar selection at any point during the
communication, and for the receiving device to change the displayed avatar in accordance with the received avatar selection.
<br/><br/> Avatar control module 210 is configured to generate parameters for animating an avatar.  Animation, as referred to herein, may be defined as altering the appearance of an image/model.  A single animation may alter the appearance of a 2-D still
image, or multiple animations may occur in sequence to simulate motion in the image (e.g., head turn, nodding, blinking, talking, frowning, smiling, laughing, winking, blinking, etc.) An example of animation for 3-D models includes deforming a 3-D
wireframe model, applying a texture mapping, and re-computing the model vertex normal for rendering.  A change in position of the detected face and/or extracted facial features may be may converted into parameters that cause the avatar's features to
resemble the features of the user's face.  In one embodiment the general expression of the detected face may be converted into one or more parameters that cause the avatar to exhibit the same expression.  The expression of the avatar may also be
exaggerated to emphasize the expression.  Knowledge of the selected avatar may not be necessary when avatar parameters may be applied generally to all of the predefined avatars.  However, in one embodiment avatar parameters may be specific to the
selected avatar, and thus, may be altered if another avatar is selected.  For example, human avatars may require different parameter settings (e.g., different avatar features may be altered) to demonstrate emotions like happy, sad, angry, surprised, etc.
than animal avatars, cartoon avatars, etc. Avatar control module 208 may include custom, proprietary, known and/or after-developed graphics processing code (or instruction sets) that are generally well-defined and operable to generate parameters for
animating the avatar selected by avatar selection module 208 based on the face/head position detected by face detection and tracking module 204 and/or the facial features detected by feature extraction module 206.  For facial feature-based animation
methods, 2-D avatar animation may be done with, for example, image warping or image morphing, whereas 3-D avatar animation may be done with free form deformation (FFD) or by utilizing the animation structure defined in a 3-D model of a head.  Oddcast is
an example of a software resource usable for 2-D avatar animation, while FaceGen is an example of a software resource usable for 3-D avatar animation.
<br/><br/> In addition, in system 100 avatar control module 210 may receive a remote avatar selection and remote avatar parameters usable for displaying and animating an avatar corresponding to a user at a remote device.  Avatar control module may cause
display module 212 to display avatar 110 on display 108.  Display module 208 may include custom, proprietary, known and/or after-developed graphics processing code (or instruction sets) that are generally well-defined and operable to display and animate
an avatar on display 108 in accordance with the example device-to-device embodiment.  For example, avatar control module 210 may receive a remote avatar selection and may interpret the remote avatar selection to correspond to a predetermined avatar. 
Display module 212 may then display avatar 110 on display 108.  Moreover, remote avatar parameters received in avatar control module 210 may be interpreted, and commands may be provided to display module 212 to animate avatar 110.  In one embodiment more
than two users may engage in the video call.  When more than two users are interacting in a video call, display 108 may be divided or segmented to allow more than one avatar corresponding to remote users to be displayed simultaneously.  Alternatively, in
system 126 avatar control module 210 may receive information causing display module 212 to display what the avatar corresponding to the user of device 102 is "seeing" in virtual space 128 (e.g., from the visual perspective of the avatar).  For example,
display 108 may display buildings, objects, animals represented in virtual space 128, other avatars, etc. In one embodiment avatar control module 210 may be configured to cause display module 212 to display "feedback" avatar 214.  Feedback avatar 214
represents how the selected avatar appears on the remote device, in a virtual place, etc. In particular, feedback avatar 214 appears as the avatar selected by the user and may be animated using the same parameters generated by avatar control module 210. 
In this way the user may confirm what the remote user is seeing during their interaction.
<br/><br/> Communication module 216 is configured to transmit and receive information for selecting avatars, displaying avatars, animating avatars, displaying virtual place perspective, etc. Communication module 216 may include custom, proprietary, known
and/or after-developed communication processing code (or instruction sets) that are generally well-defined and operable to transmit avatar selections, avatar parameters and receive remote avatar selections and remote avatar parameters.  Communication
module 216 may also transmit and receive audio information corresponding to avatar-based interactions.  Communication module 216 may transmits and receive the above information via network 122 as previously described.
<br/><br/> FIG. 3 illustrates an example system implementation in accordance with at least one embodiment.  Device 102' is configured to communicate wirelessly via WiFi connection 300 (e.g., at work), server 124' is configured to negotiate a connection
between devices 102' and 112' via Internet 302, and apparatus 112' is configured to communicate wirelessly via another WiFi connection 304 (e.g., at home).  In one embodiment a device-to-device avatar-based video call application is activated in
apparatus 102'.  Following avatar selection, the application may allow at least one remote device (e.g., device 112') to be selected.  The application may then cause device 102' to initiate communication with device 112'.  Communication may be initiated
with device 102' transmitting a connection establishment request to device 112' via enterprise access point (AP) 306.  Enterprise AP 306 may be an AP usable in a business setting, and thus, may support higher data throughput and more concurrent wireless
clients than home AP 314.  Enterprise AP 306 may receive the wireless signal from device 102' and may proceed to transmit the connection establishment request through various business networks via gateway 308.  The connection establishment request may
then pass through firewall 310, which may be configured to control information flowing into and out of the WiFi network 300.
<br/><br/> The connection establishment request of device 102' may then be processed by server 124'.  Server 124' may be configured for registration of IP addresses, authentication of destination addresses and NAT traversals so that the connection
establishment request may be directed to the correct destination on Internet 302.  For example, server 124' may resolve the intended destination (e.g., remote device 112') from information in the connection establishment request received from device
102', and may route the signal to through the correct NATs, ports and to the destination IP address accordingly.  These operations may only have to be performed during connection establishment, depending on the network configuration.  In some instances
operations may be repeated during the video call in order to provide notification to the NAT to keep the connection alive.  Media and Signal Path 312 may carry the video (e.g., avatar selection and/or avatar parameters) and audio information direction to
home AP 314 after the connection has been established.  Device 112' may then receive the connection establishment request and may be configured to determine whether to accept the request.  Determining whether to accept the request may include, for
example, presenting a visual narrative to a user of device 112' inquiring as to whether to accept the connection request from device 102'.  Should the user of device 112' accept the connection (e.g., accept the video call) the connection may be
established.  Cameras 104' and 114' may be configured to then start capturing images of the respective users of devices 102' and 112', respectively, for use in animating the avatars selected by each user.  Microphones 106' and 116' may be configured to
then start recording audio from each user.  As information exchange commences between devices 102' and 112', displays 108' and 118' may display and animate avatars corresponding to the users of devices 102' and 112'.
<br/><br/> FIG. 4 is a flowchart of example operations in accordance with at least one embodiment.  In operation 402 an application (e.g., an avatar-based voice call application) may be activated in a device.  Activation of the application may be followed
by selection of an avatar.  Selection of an avatar may include an interface being presented by the application, the interface allowing the user to select a predefined avatar.  After avatar selection, communications may be configured in operation 404. 
Communication configuration includes the identification of at least one remote device or a virtual space for participation in the video call.  For example, a user may select from a list of remote users/devices stored within the application, stored in
association with another system in the device (e.g., a contacts list in a smart phone, cell phone, etc.), stored remotely, such as on the Internet (e.g., in a social media website like <b><i>Facebook,</i></b> LinkedIn, Yahoo, Google+, MSN, etc.).  Alternatively, the
user may select to go online in a virtual space like Second Life.
<br/><br/> In operation 406, communication may be initiated between the device and the at least one remote device or virtual space.  For example, a connection establishment request may be transmitted to the remote device or virtual space.  For the sake of
explanation herein, it is assumed that the connection establishment request is accepted by the remote device or virtual space.  A camera in the device may then begin capturing images in operation 408.  The images may be still images or live video (e.g.,
multiple images captured in sequence).  In operation 410 image analysis may occur starting with detection/tracking of a face/head in the image.  The detected face may then be analyzed in order to extract facial features (e.g., facial landmarks, facial
expression, etc.).  In operation 412 the detected face/head position and/or facial features are converted into Avatar parameters.  Avatar parameters are used to animate the selected avatar on the remote device or in the virtual space.  In operation 414
at least one of the avatar selection or the avatar parameters may be transmitted.
<br/><br/> Avatars may be displayed and animated in operation 416.  In the instance of device-to-device communication (e.g., system 100), at least one of remote avatar selection or remote avatar parameters may be received from the remote device.  An avatar
corresponding to the remote user may then be displayed based on the received remote avatar selection, and may be animated based on the received remote avatar parameters.  In the instance of virtual place interaction (e.g., system 126), information may be
received allowing the device to display what the avatar corresponding to the device user is seeing.  A determination may then be made in operation 418 as to whether the current communication is complete.  If it is determined in operation 418 that the
communication is not complete, operations 408-416 may repeat in order to continue to display and animate an avatar on the remote apparatus based on the analysis of the user's face.  Otherwise, in operation 420 the communication may be terminated.  The
video call application may also be terminated if, for example, no further video calls are to be made.
<br/><br/> While FIG. 4 illustrates various operations according to an embodiment, it is to be understood that not all of the operations depicted in FIG. 4 are necessary for other embodiments.  Indeed, it is fully contemplated herein that in other
embodiments of the present disclosure, the operations depicted in FIG. 4 and/or other operations described herein may be combined in a manner not specifically shown in any of the drawings, but still fully consistent with the present disclosure.  Thus,
claims directed to features and/or operations that are not exactly shown in one drawing are deemed within the scope and content of the present disclosure.
<br/><br/> As used in any embodiment herein, the term "module" may refer to software, firmware and/or circuitry configured to perform any of the aforementioned operations.  Software may be embodied as a software package, code, instructions, instruction
sets and/or data recorded on non-transitory computer readable storage medium.  Firmware may be embodied as code, instructions or instruction sets and/or data that are hard-coded (e.g., nonvolatile) in memory devices.  "Circuitry", as used in any
embodiment herein, may comprise, for example, singly or in any combination, hardwired circuitry, programmable circuitry such as computer processors comprising one or more individual instruction processing cores, state machine circuitry, and/or firmware
that stores instructions executed by programmable circuitry.  The modules may, collectively or individually, be embodied as circuitry that forms part of a larger system, for example, an integrated circuit (IC), system on-chip (SoC), desktop computers,
laptop computers, tablet computers, servers, smart phones, etc.
<br/><br/> Any of the operations described herein may be implemented in a system that includes one or more storage mediums having stored thereon, individually or in combination, instructions that when executed by one or more processors perform the methods. Here, the processor may include, for example, a server CPU, a mobile device CPU, and/or other programmable circuitry.  Also, it is intended that operations described herein may be distributed across a plurality of physical devices, such as processing
structures at more than one different physical locations.  The storage medium may include any type of tangible medium, for example, any type of disk including hard disks, floppy disks, optical disks, compact disk read-only memories (CD-ROMs), compact
disk rewritables (CD-RWs), and magneto-optical disks, semiconductor devices such as read-only memories (ROMs), random access memories (RAMs) such as dynamic and static RAMs, erasable programmable read-only memories (EPROMs), electrically erasable
programmable read-only memories (EEPROMs), flash memories, Solid State Disks (SSDs), magnetic or optical cards, or any type of media suitable for storing electronic instructions.  Other embodiments may be implemented as software modules executed by a
programmable control device.  The storage medium may be non-transitory.
<br/><br/> Thus, the present disclosure provides a method and system for conducting a video communication using avatars instead of live images.  The use of avatars reduces the amount of information to exchange as compared to the sending of live images.  An
avatar is selected and then communication may be established.  A camera in each device may captures images of the participants.  The images may be analyzed to determine face position and facial features.  The face position and/or facial features are then
converted into avatar parameters, and at least one of the avatar selection or the avatar parameters are transmitted to display/animate.
<br/><br/> According to one aspect there is provided a method.  The method may include selecting an avatar, initiating communication, capturing an image, detecting a face in the image, extracting features from the face, converting the facial features to
avatar parameters, and transmitting at least one of the avatar selection or avatar parameters.
<br/><br/> According to another aspect there is provided a system.  The system may include a camera configured to capture images, a communication module configured to transmit and receive information, and one or more storage mediums.  In addition, the one
or more storage mediums having stored thereon, individually or in combination, instructions that when executed by one or more processors result in the following operations comprising selecting an avatar, initiating communication, capturing an image,
detecting a face in the image, extracting features from the face, converting the facial features to avatar parameters, and transmitting at least one of the avatar selection or avatar parameters.
<br/><br/> According to another aspect there is provided a system.  The system may include one or more storage mediums having stored thereon, individually or in combination, instructions that when executed by one or more processors result in the following
operations comprising selecting an avatar, initiating communication, capturing an image, detecting a face in the image, extracting features from the face, converting the facial features to avatar parameters, and transmitting at least one of the avatar
selection or avatar parameters.
<br/><br/> The terms and expressions which have been employed herein are used as terms of description and not of limitation, and there is no intention, in the use of such terms and expressions, of excluding any equivalents of the features shown and
described (or portions thereof), and it is recognized that various modifications are possible within the scope of the claims.  Accordingly, the claims are intended to cover all such equivalents.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09398262&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D11%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D1%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D11%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D1%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209398262"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9398262&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D11%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D1%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209398262">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=11&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=1&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=11&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=2&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=10&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=1&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=12&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=1&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></body></html>