<html><head>
<base target="_top"/>
<title>United States Patent: 9357242</title></head>
<!---BUF1=9357242
BUF7=2016
BUF8=48030
BUF9=/1/
BUF51=9
---->
<body bgcolor="#FFFFFF">
<a name="top"></a>
<center>
<img alt="[US Patent &amp; Trademark Office, Patent Full Text and Image Database]" src="/netaicon/PTO/patfthdr.gif"/>
<br/>
<table>
<tbody><tr><td align="center">
<a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
<a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
<a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
<a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
</td></tr>
<tr><td align="center">
   <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=387&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=7&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=387&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=387&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=386&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=388&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

<a href="#bottom"><img alt="[Bottom]" border="0" src="/netaicon/PTO/bottom.gif" valign="middle"/></a>
</td></tr>
   <tr><td align="center">
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D387%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D8%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209357242"><img alt="[
View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9357242&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D387%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D8%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209357242">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09357242&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D387%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D8%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>

   </td></tr>
</tbody></table>
</center>
<table width="100%">
<tbody><tr><td align="left" width="50%"> </td>
<td align="right" valign="bottom" width="50%"><font size="-1">( <strong>387</strong></font> <font size="-2">of</font> <strong><font size="-1">7895</font></strong> <font size="-1">)</font></td></tr></tbody></table>
<hr/>
   <table width="100%">
   <tbody><tr>	<td align="left" width="50%"><b>United States Patent </b></td>
   <td align="right" width="50%"><b>9,357,242</b></td>
   </tr>
     <tr><td align="left" width="50%"><b>
         Sinha
,   et al.</b>
     </td>
     <td align="right" width="50%"> <b>
     May 31, 2016
</b></td>
     </tr>
     </tbody></table>
       <hr/>
       <font size="+1">Method and system for automatic tagging in television using crowd sourcing
     technique
</font><br/>
       <br/><center><b>Abstract</b></center>
       <p> A method and system for tracking of objects in a video is disclosed. The
     method of the present invention enables user to indicate a boundary-box
     to the identified object of interest in the broadcast video on television
     or any other communication media. The object indicted in the boundary-box
     is than tracked by the users connected in a social community network in
     the upcoming video frames of the broadcasted video. The tracked object is
     then tagged by the users in the social community network. Further, the
     present invention enables augmentation of the tracked object in the video
     by extracting additional information from the online service providers.
     The augmentation and tagging of the object generates metadata related to
     the object. The metadata generated is stored on a server to track the
     object in future based on the metadata related to the object.
</p>
       <hr/>
<table width="100%"> <tbody><tr> <th align="left" scope="row" valign="top" width="10%">Inventors:</th> <td align="left" width="90%">
 <b>Sinha; Priyanka</b> (Kolkata, <b>IN</b>)<b>, Gupta; Rohit Kumar</b> (Kolkata, <b>IN</b>)<b>, Ghose; Avik</b> (Kolkata, <b>IN</b>)<b>, Chirabrata; Bhaumik</b> (Kolkata, <b>IN</b>)<b>, Das; Diptesh</b> (Kolkata, <b>IN</b>) </td> </tr>
<tr><th align="left" scope="row" valign="top" width="10%">Applicant: </th><td align="left" width="90%"> <table> <tbody><tr> <th align="center" scope="column">Name</th> <th align="center" scope="column">City</th> <th align="center" scope="column">State</th> <th align="center" scope="column">Country</th> <th align="center" scope="column">Type</th> </tr> <tr> <td> <b><br/>Sinha; Priyanka
<br/>Gupta; Rohit Kumar
<br/>Ghose; Avik
<br/>Chirabrata; Bhaumik
<br/>Das; Diptesh</b> </td><td> <br/>Kolkata
<br/>Kolkata
<br/>Kolkata
<br/>Kolkata
<br/>Kolkata </td><td align="center"> <br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A
<br/>N/A </td><td align="center"> <br/>IN
<br/>IN
<br/>IN
<br/>IN
<br/>IN </td> <td align="left"> </td> </tr> </tbody></table>
<!-- AANM>
~AANM Sinha; Priyanka
~AACI Kolkata
~AAST N/A
~AACO IN
~AANM Gupta; Rohit Kumar
~AACI Kolkata
~AAST N/A
~AACO IN
~AANM Ghose; Avik
~AACI Kolkata
~AAST N/A
~AACO IN
~AANM Chirabrata; Bhaumik
~AACI Kolkata
~AAST N/A
~AACO IN
~AANM Das; Diptesh
~AACI Kolkata
~AAST N/A
~AACO IN
</AANM -->
</td></tr>
<tr> <th align="left" scope="row" valign="top" width="10%">Assignee:</th>
<td align="left" width="90%">

<b>Tata Consultancy Services Limited</b>
 (<b>IN</b>)
<br/>

</td>
</tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Family ID:
       </th><td align="left" width="90%">
       <b>48043240
</b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">Appl. No.:
       </th><td align="left" width="90%">
       <b>14/125,011</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">Filed:
       </th><td align="left" width="90%">
       <b>June 7, 2012</b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">PCT Filed:
       </th><td align="left" width="90%"><b>
       June 07, 2012
       </b></td></tr>
       <tr><th align="left" scope="row" valign="top" width="10%">PCT No.:
       </th><td align="left" width="90%"><b>
       PCT/IN2012/000404
       </b></td></tr>
         <tr><th align="left" scope="row" valign="top" width="15%">371(c)(1),(2),(4) Date:
         </th><td align="left" width="85%"><b>
         December 09, 2013
         </b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">PCT Pub. No.:
       </th><td align="left" width="90%">
       <b>
       WO2013/051014
       </b></td></tr>
       <tr><th align="left" nowrap="" scope="row" valign="top" width="10%">PCT Pub. Date:
       </th><td align="left" width="90%">
       <b>
       April 11, 2013
       </b></td></tr>
     </tbody></table>
<hr/> <center><b>Prior Publication Data</b></center> <hr/> <table width="100%"> <tbody><tr><th scope="col"></th><th scope="col"></th><td></td></tr> <tr><td align="left">
</td><th align="center" scope="col"><b><u>Document Identifier</u></b></th><th align="center" scope="col"><b><u>Publication Date</u></b></th></tr><tr><td align="center"> </td><td align="center"> US 20140101691 A1</td><td align="center">Apr 10, 2014</td></tr><tr><td align="center"> 
</td>
</tr> </tbody></table>
     <hr/>
<center><b>Foreign Application Priority Data</b></center> <hr align="center" width="30%"/> <table width="100%"> <tbody><tr><th scope="col"></th><td></td><td></td><th scope="col"></th><td></td></tr> <tr><td align="center">
Jun 10, 2011
[IN]</td><td></td><td></td><td align="left">
1702/MUM/2011</td></tr><tr><td align="center">

</td>
</tr> </tbody></table>
<p> <table width="100%"> <tbody><tr><td align="left" valign="top" width="30%"><b>Current U.S. Class:</b></td> <td align="right" valign="top" width="70%"><b>1/1</b> </td></tr> 
       <tr><td align="left" valign="top" width="30%"><b>Current CPC Class: </b></td>
       <td align="right" valign="top" width="70%">G06F 17/3082 (20130101); H04N 21/235 (20130101); H04N 21/24 (20130101); H04N 21/278 (20130101); H04N 21/45455 (20130101); H04N 21/47205 (20130101); H04N 21/8133 (20130101); H04H 60/48 (20130101); H04H 60/64 (20130101); H04H 60/80 (20130101); H04H 20/08 (20130101)</td></tr>
         <tr><td align="left" valign="top" width="30%"><b>Current International Class: </b></td>
         <td align="right" valign="top" width="70%">H04N 7/10 (20060101); H04N 7/025 (20060101); G06F 3/00 (20060101); H04N 21/235 (20110101); G06F 17/30 (20060101); H04N 21/24 (20110101); H04N 21/278 (20110101); H04N 21/4545 (20110101); H04N 21/472 (20110101); H04N 21/81 (20110101); H04H 60/48 (20080101); H04H 60/64 (20080101); H04H 60/80 (20080101); H04H 20/08 (20080101)</td></tr>
       <tr><td align="left" valign="top" width="30%"><b>Field of Search: </b></td>
       <td align="right" valign="top" width="70%">
       


 ;725/32,51,60
       </td></tr>
     </tbody></table>
</p><hr/><center><b>References Cited  <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2Fsearch-adv.htm&amp;r=0&amp;f=S&amp;l=50&amp;d=PALL&amp;Query=ref/9357242">[Referenced By]</a></b></center>       <hr/>
       <center><b>U.S. Patent Documents</b></center>
<table width="100%"> <tbody><tr><th scope="col" width="33%"></th> <th scope="col" width="33%"></th> <th scope="col" width="34%"></th></tr> <tr> <td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F6357042">6357042</a></td><td align="left">
March 2002</td><td align="left">
Srinivasan et al.</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8285121">8285121</a></td><td align="left">
October 2012</td><td align="left">
Kulas</td></tr><tr><td align="left">
<a href="/netacgi/nph-Parser?Sect2=PTO1&amp;Sect2=HITOFF&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&amp;r=1&amp;f=G&amp;l=50&amp;d=PALL&amp;RefSrch=yes&amp;Query=PN%2F8520979">8520979</a></td><td align="left">
August 2013</td><td align="left">
Conwell</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20010023436&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2001/0023436</a></td><td align="left">
September 2001</td><td align="left">
Srinivasan</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080086369&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0086369</a></td><td align="left">
April 2008</td><td align="left">
Kiat</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20080313272&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2008/0313272</a></td><td align="left">
December 2008</td><td align="left">
Nguyen et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090094520&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0094520</a></td><td align="left">
April 2009</td><td align="left">
Kulas</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20090327894&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2009/0327894</a></td><td align="left">
December 2009</td><td align="left">
Rakib</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20100008639&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2010/0008639</a></td><td align="left">
January 2010</td><td align="left">
Greenberg et al.</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20100242074&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2010/0242074</a></td><td align="left">
September 2010</td><td align="left">
Rouse</td></tr><tr><td align="left">
<a href="http://appft.uspto.gov/netacgi/nph-Parser?TERM1=20110040754&amp;Sect1=PTO1&amp;Sect2=HITOFF&amp;d=PG01&amp;p=1&amp;u=%2Fnetahtml%2FPTO%2Fsrchnum.html&amp;r=0&amp;f=S&amp;l=50" target="_blank">2011/0040754</a></td><td align="left">
February 2011</td><td align="left">
Peto et al.</td></tr><tr><td align="left">

</td>
</tr> </tbody></table>
<table width="90%">   <tbody><tr><td><align="left"><br/>"Demo Abstract: mCrowd--A Platform for Mobile Crowdsourcing. Nov. 4, 2009". cited by examiner
.<br/>Julia Noordegraaf, "Remembering the Past in the Dynarchive: The State of Knowledge in Digital Archives", University of Amsterdam (May 13-15, 2011). cited by applicant
.<br/>Riste Gligorov et al. "On the role of user-generated metadata in audio visual collections." Proceedings of the sixth international conference on Knowledge capture. ACM (2011). cited by applicant
.<br/>Tingxin Yan et al. "mCrowd: a platform for mobile crowdsourcing." Proceedings of the 7th ACM Conference on Embedded Networked Sensor Systems. ACM (2009). cited by applicant
.<br/>International Search Report mailed Mar. 15, 2013, in International Application No. PCT/IN2012/000404. cited by applicant. </align="left"></td></tr> </tbody></table><br/><center><b>Other References</b></center> <br/>
       <i>Primary Examiner:</i> Alata; Yassin
<br/>
       <i>Attorney, Agent or Firm:</i> <coma>Finnegan, Henderson, Farabow, Garrett &amp; Dunner, LLP
<br/>
         <hr/>
<center><b><i>Claims</i></b></center> <hr/> <br/><br/>We claim: <br/><br/> 1.  A method of tagging of objects of interests in a video during real-time broadcasting using crowd sourcing, the method comprising: providing, via a first communication device
including a hardware processor, at least one media for display to a user of the first communication device, wherein the at least one media is being broadcasted via a second communication device, and wherein the at least one media is provided for display,
via the first communication device, by pairing the first communication device with the second communication device, wherein the screen area of the first communication device is adjusted corresponding to viewable area of the second communication device; 
receiving, via the first communication device, user input, from the user, indicating at least one bounding-box to be displayed, corresponding to the at least one media being broadcasted, on the first communication device;  identifying at least one object
of interest, depicted in the at least one media being broadcasted, within the at least one bounding-box wherein at least one object of interest is identified at first video frame f1 at time t1 on the at least one media being broadcasted at location x1; 
searching, via the first communication device, the identified at least one object of interest in one or more subsequent broadcast frames of the at least one media being broadcasted wherein the identified at least one object of interest at time t1 is
matched at time t2, t3 .  . . tn in the one or more subsequent frames f2, f3 .  . . fn at locations x2, x3 .  . . xn;  tagging and augmenting the tracked at least one object by using a crowd sourcing technique, wherein the tagging and augmentation, using
the crowd sourcing technique, comprises receiving, on the first communication device, a plurality of tags associated with the at least one object, wherein the plurality of tags is received, during the broadcast of the at least one media, from the user
and a plurality of users communicatively coupled with the user over a plurality of social networking websites via an internet, wherein at least one tag is received in at least one broadcast frame of the at least one media being broadcasted, and
augmenting the plurality of tags with additional information received from the plurality of users;  generating, in real time, metadata related to the at least one object based on the tagging and the augmentation of the at least one object;  and uploading
the generated metadata related to the at least one object.
<br/><br/> 2.  The method of claim 1, wherein the at least one bounding-box is indicated as an area of tracking the at least one object in the at least one media being broadcasted.
<br/><br/> 3.  The method of claim 1, wherein the at least one object is tracked using an image matching algorithm.
<br/><br/> 4.  The method of claim 1, wherein the metadata generated related to the at least one tagged object is uploaded to an aggregation server using REST protocol.
<br/><br/> 5.  The method of claim 1, wherein the at least one bounding-box is indicated by a user using an input device associated with one of the first communication device and the second communication device or using gesture inputs associated with the
user.
<br/><br/> 6.  The method of claim 1, wherein the first communication device is selected from a group consisting of: Tablet PC, desktop, laptop and Smartphone.
<br/><br/> 7.  The method of claim 5, wherein the second communication device is selected from a group consisting of: television, desktop, laptop and mobile phone.
<br/><br/> 8.  A system for tagging of objects of interests in a video during real-time broadcasting using crowd sourcing, the system comprising: a first communication device paired with a second communication device, wherein the first communication device
comprises a processor;  and a memory coupled to the processor storing instructions that when executed cause the processor to: provide at least one media for display a user of the first communication devices, wherein the at least one media is being
broadcasted via the second communication device, and wherein the at least one media is provided for display, via the first communication device, based upon the pairing of the first communication device with the second communication device, wherein the
screen area of the first communication device is adjusted corresponding to viewable area of the second communication device;  receive user input, from the user, to indicate at least one bounding-box to be displayed, corresponding to the at least one
media being broadcasted, on the first communication device;  identify at least one object of interest depicted in the at least one media being broadcasted, within the at least one bounding-box wherein at least one object of interest is identified at
first video frame f1 at time t1 on the at least one media being broadcasted at location x1;  search the identified at least one object of interest in one or more subsequent broadcast frames of the at least one media being broadcasted wherein the
identified at least one object of interest at time t1 is matched at time t2, t3 .  . . tn in the one or more subsequent frames f2, f3 .  . . fn at locations x2, x3 .  . . xn;  tag and augment the tracked at least one object by using a crowd sourcing
technique, wherein the tagging and augmentation, using the crowd sourcing technique, comprises receiving, on the first communication device, a plurality of tags associated with the at least one object, wherein the plurality of tags is received, during
the broadcast of the at least one media, from the user and a plurality of users communicatively coupled with the user over a plurality of social networking websites via an internet, wherein at least one tag is received in at least one broadcast frame of
the at least one media being broadcasted, and augmenting the plurality of tags with additional information received from the plurality of users;  generate, in real time, metadata related to the at least one object based on the tagging and the
augmentation of the at least one object;  and upload the generated metadata.
<br/><br/> 9.  The system of claim 8, wherein the at least one bounding-box is indicated as an area of tracking the at least one object in the at least one media being broadcasted.
<br/><br/> 10.  The system of claim 8, wherein the at least one object is tracked using an image matching algorithm.
<br/><br/> 11.  The system of claim 8, wherein the metadata generated related to the at least one tagged object is uploaded to an aggregation server using REST protocol.
<br/><br/> 12.  The system of claim 8, wherein the at least one bounding-box is indicated by a user using an input device associated with one of the first communication device and the second communication device or gesture inputs associated with the user.
<br/><br/> 13.  The system of claim 8, wherein the second communication device further includes a camera for capturing the user inputs via hand gestures to capture the objects of interest indicated by a boundary-box. <hr/>
<center><b><i>Description</i></b></center> <hr/> <br/><br/>CROSS-REFERENCE TO RELATED PATENT APPLICATIONS
<br/><br/> This application is a National Stage Entry of International Application No. PCT/IN2012/000404, filed Jun.  7, 2012, which claims priority from Indian Patent Application No. 1702/MUM/2011, filed Jun.  10, 2011.  The entire contents of the
above-referenced applications are expressly incorporated herein by reference.
<br/><br/>FIELD OF THE INVENTION
<br/><br/> The invention generally relates to the field of identification and tagging of objects in video.  More particularly, the invention relates to a method and system for efficient tagging of objects in a video broadcasted over a communication media
using crowd sourcing technique.
<br/><br/>BACKGROUND OF THE INVENTION
<br/><br/> There has been a rapid growth in the availability and retrieval of information through the World Wide Web.  The web contains large volume of data that is stored in database data structures such as database tables or indexes or files.  The data
stored may be of different formats, different types, and may contain different domain-specific information.
<br/><br/> The web has gained importance due to its usage in different applications such as e-commerce, hospitality, social networking etc. It enables individuals across the globe to collaborate together.  The internet websites contain various files stored
on the server receiving requests from different clients for retrieval of data hosted by these websites.
<br/><br/> Search engines such as Google.TM., Yahoo.RTM.  etc facilitate searching through the large volume of data by indexing the keywords and establishing relevancy thereof with user search criterion.  Most of the search engines provide set of results
that are based on keywords used by the user in the search queries formulated for searching over the web and are generally presented in the descending order of relevancy.
<br/><br/> Most of the users tend to search the information from the web that is relevant to the domain in which they are interested.  So, while searching users normally use keywords and variants thereof to obtain relevant set of results.  However,
information retrieval from large volume of data with different formats, types and context is a problematic and a tedious task.
<br/><br/> Moreover, if the file to be searched is media file then it is not possible to search the required media file using keywords media content does not contain associated keywords.
<br/><br/> For example, if the user is searching a video or a photo from the website storing videos or photos, then it will be very difficult for the user to search for a particular video or photo in which she is interested due to lack of keywords or
metadata associated with the file.  Thus, these files need to be tracked and identified in an efficient manner so that they can be retrieved easily.
<br/><br/> One method that has gained importance today is online tagging of objects.  The tagging helps in object recognition.  The tagging is the phenomenon wherein the users connected on internet add tags to the file or movie clip or image stored on the
internet so that it can be searched easily by the user interested in searching these stored files on the internet.  This is referred as "Social Tagging".
<br/><br/> Further, the social tagging enables storage of tags generated from multiple users on the server and associates these tags to a particular group.  This allows social communities to maintain their own sets of tags for the same objects in the same
video.  This leads to faster and more relevant searches when a user is using the tag to search for objects (and related video, metadata, etc) in a particular social community.  For example, the social community interested in Smartphones may generate a
social group in context to Smartphones and store the tags generated from the Smartphone community members in a database storing tags generated from the users related to the Smartphones.  This facilitates in searching of any particular social community
group as well, for example Smartphones group in this case.
<br/><br/> Efforts have been made in the past for online tagging of objects.  Few of those known to us are as follows:
<br/><br/> One such augmented reality application known to us is Google.TM.  Goggles that is able to direct relevant searches and therefore information from an image.
<br/><br/> Further, there exist open source projects that deal with object tracking, in particular motion history image.  In OpenCv there are a number of tracking methods such as Camshift &amp; Meanshift demo, lkedemo, eye tracking.
<br/><br/> Social networking on TV is although a very recent concept, has been gaining traction as a showcase application for Social TV.
<br/><br/> Tagging on the other hand has been used with some success to auto-generate metadata information by geo-locating photographs taken by mobile phones.
<br/><br/> Manual tagging has greatly improved image search as can be seen with Flickr.RTM.  and Picasa.TM.  from Yahoo.RTM.  and Google.TM.  respectively.
<br/><br/> Also, there exists APIs in the art such as Future API and Amazon's Mechanical Turk HITs that are being used to tag proprietary videos or surveillance videos for a fee.
<br/><br/> Current social TV applications allow users to view what other users connected to them are watching and often use that to create communities and allow users to chat on the same.  This level of interaction, however, is limited to text.
<br/><br/> Although there has been extensive research on the motion tracking of the objects, attaching meaning to the object tracked and identified is still a hard computational problem.  Moreover, tracking of videos itself is a problem due to very low
resolution of the videos.  Further, the problem with tracking of videos increases due to varying environmental conditions such as illumination, occlusion problems etc.
<br/><br/> Also, if the object to be tracked is moving at high speed, it is difficult to tack it.  Similarly, the object with low frame rate is difficult to track.
<br/><br/> As will be appreciated, there is a clear need for an improved method of processing and metatagging image content such as video content that would simplify the long-standing computationally hard problem of image processing for object
identification and recognition, which would further alleviate many of the problems outlined above.
<br/><br/>OBJECTIVES OF THE INVENTION
<br/><br/> The principal objective of the present invention to provide a method and system that enables real-time tracking of objects in a video broadcasted on a television using crowd sourcing.
<br/><br/> Yet another objective of the invention is to enable indication of box to set boundaries for identifying the object to be tracked in a video broadcasted on a television or any other communication media.
<br/><br/> Yet another objective of the invention is to enable tagging of the identified object in the video by various users connected in a social community network.
<br/><br/> Yet another objective of the invention is to enable augmentation of tagged object by collecting additional information i.e. metadata from different online service providers related to the tagged object.
<br/><br/> Yet another objective of the invention is to enable generation of metadata based on tagging and augmentation of the object.
<br/><br/> Still another objective of the invention is enable storage of the metadata generated related to the object on a server for tracking the said object in the future.
<br/><br/>SUMMARY OF THE INVENTION
<br/><br/> Before the present methods, systems, and hardware enablement are described, it is to be understood that this invention is not limited to the particular systems, and methodologies described, as there can be multiple possible embodiments of the
present invention which are not expressly illustrated in the present disclosure.  It is also to be understood that the terminology used in the description is for the purpose of describing the particular versions or embodiments only, and is not intended
to limit the scope of the present invention.
<br/><br/> The following text presents a simplified, incomplete summary in order to provide an orientation to certain aspects of the disclosed subject matter.  This Summary is not an extensive overview.  It is not intended to identify key/critical elements
or to delineate the scope of the claimed subject matter.  Its sole purpose is to present some concepts in a simplified form as a prelude to the more detailed description that follows.
<br/><br/> The present invention provides method and system tracking objects of interest on a broadcasted video by tagging the objects tracked using crowd sourcing technique.
<br/><br/> The present invention enables a method by which users can interact using objects on a broadcast video.  The objects of interest may include an actor, a merchandise item, a gadget or a vehicle etc. These objects are identified, bounded by a box
and then tagged by the users.
<br/><br/> Further, the present invention enables these tags to be shared across the users via an aggregation server where users are allowed to correct and augment the generated tags.  The augmentation of tags can be done using online service that provides
the users to extract more information for the tags.  The tags may not be mere descriptions but might be student notes, Q &amp; A, and general comments for an educational content.  Similarly, other genre of broadcast can be similarly augmented.
<br/><br/> The user inputs the bounding box on TV with a remote acting like a mouse or in air using gesture.  The object in that box is then tracked over frames.  The object to be tracked is accompanied with a dialog box where user tags are displayed. 
Moreover, the users are provided with share and ask buttons with the tags to allow social interaction with friends.  This metadata is uploaded using REST. <br/><br/>BRIEF DESCRIPTION OF DRAWINGS
<br/><br/> The foregoing summary, as well as the following detailed description of preferred embodiments, is better understood when read in conjunction with the appended drawings.  For the purpose of illustrating the invention, there is shown in the
drawings example constructions of the invention; however, the invention is not limited to the specific methods and architecture disclosed in the drawings:
<br/><br/> FIG. 1 schematically shows system diagram 100 containing different system elements for crowdsourced tagging and metadata generation on TV in accordance an embodiment of the invention.
<br/><br/> FIG. 2 shows a block-diagram 200 enabling crowdsourced tagging of objects on TV managed from paired handheld/Smartphone camera in accordance an embodiment of the invention.
<br/><br/> FIG. 3 is a flow chart 300 showing different steps implemented for tagging objects in a video on a TV using crowd sourcing technique in accordance to an embodiment of the invention.
<br/><br/> The description has been presented with reference to an exemplary embodiment of the invention.  Persons skilled in the art and technology to which this invention pertains will appreciate that alterations and changes in the described method and
system of operation can be practiced without meaningfully departing from the principle, spirit and scope of this invention.
<br/><br/> FIG. 1 shows system diagram 100 for tracking of objects in a video broadcasted on a TV in accordance to an embodiment of the present invention.  TV-IN 101 is shown that is used as broadcasting medium for broadcasting the various media content
that is viewed by various subscribers worldwide.  The TV-IN 101 as shown includes processing engine 103, TV screen 102 to display the content, Wi-Fi or Bluetooth device 104 embedded for enabling the communication with other pairing devices, camera 105
for capturing gesture inputs from a user watching the content on the TV-IN 101.
<br/><br/> In an embodiment as shown in FIG. 1, the system 100 includes a Tablet/Smartphone 107 that is paired with the TV-IN 101 using standard protocol compatible devices such as Wi-Fi or Bluetooth device 104.  In another embodiment, the
Tablet/Smartphone 107 may be connected to the TV-IN 101 by other communication protocol means including but not limited to WAN, LAN, MAN, Internet and combinations thereof.
<br/><br/> In an embodiment, the TV-IN 101 is connected to different online service providers including but not limited to information provider Wikipedia 112-1, social networking sites such as Twitter 112-2 and <b><i>Facebook</i></b> 112-3 by means of Internet
connection 106.  These service providers' enables people connected in a social community network to generate additional information about a video broadcasted on the TV-IN 101 by using crowd sourcing technique.
<br/><br/> As shown in FIG. 1, a metadata database 110 is shown that is used to collect the metadata generated for a video displayed on the TV-IN 101.  A cloud computing server 109 connected to the internet 106 enables processing of high computational
power required by the pairing devices 107 connected to the Internet 106 while transfer of data.  Further, an analysis engine 111 is shown that is used for analysis of images in the video displayed on TV-IN 101.  As shown, the camera 105 enables capture
of user Hand gestures 108 to identify the users' objects of interest in the video broadcasted on the TV-IN 101.
<br/><br/> The system 100 shown in FIG. 1 in accordance to an embodiment of the present invention helps in tracking of different objects of interest in the video broadcasted on the TV-IN 101.  This is explained as below.
<br/><br/> In an embodiment, as shown in FIG. 1, a connected TV-IN 101 is paired with a handheld Tablet or Smartphone 107 via Bluetooth or WI-FI device 104.  The screen area of the Tablet is matched corresponding to viewable area of the TV-IN 101.  The
video displayed on TV screen 102 of the TV-IN 101 is pushed on the paired Tablet or Smartphone using communication protocol such as Bluetooth or WI-FI depending on the type of devices used.
<br/><br/> In an embodiment of the present invention, the user can input the objects in the video broadcasted on the TV-IN 101 in which she is interested.  That is, at least one object in the video can be tracked to generate metadata.  Such an object can
be an actor, a merchandise item, a gadget or a vehicle etc.
<br/><br/> In an embodiment of the present invention, these objects are identified by the user by indicating a boundary-box and then tagged by the users.  In one embodiment of the present invention, the user can input the boundary-box indicating the area
of interest in the broadcasted video to be tagged by the users connected in social-community network using remote input device such as mouse.  In another embodiment, the input can also be given by the Tablet or Smartphone Camera 107 that points to the
portion of the TV screen 102 desired.  In still another embodiment, with camera support from the TV, Hand Gestures 108 can indicate the desired object boundaries on the screen as well.
<br/><br/> In an embodiment, as-soon-as the object to be tracked is indicated in the boundary-box by the user using these input devices, tags may be inputted about the identified object of interest in the video broadcasted on TV-IN 101.  The identified
object of interest is accompanied with a dialog box where user tags are displayed.  Further, in an embodiment of the present invention, share and ask buttons are provided with the tags to allow social interaction with friends.
<br/><br/> For example, if the video of a movie is being played on the TV-IN 101, the user may indicate the object of interest in the video as an actor of the played movie, which is indicated by the boundary-box input by the user.
<br/><br/> The next task is to track the identified object of interest in the upcoming frames.  This is done by using the bounding box specified by the user in upcoming frames in real-time and then transmitting the tags to the network.  In an embodiment,
the tracking of the identified object of interest in the upcoming video frames is done by image matching technique implemented by the analysis engine 111 as shown in FIG. 1.
<br/><br/> As the object of interest is identified by using the bounding box, the said object of interest is considered as the template image, and by implementing the image matching algorithm using the analysis engine 111, the said identified object is
searched in the upcoming frames to find a match in a window around the last position.  Hence, the tracking of the object of interest takes place in the further upcoming frames.
<br/><br/> For example, in an embodiment of the invention, if the object of interest is identified in the first video frame f1 at time t1 on the broadcasted video at location x1, then in the upcoming frames f2, f3 .  . . fn, by matching the identified
object of interest that was tracked at time t1 is further tracked at time t2, t3 .  . . . In at locations x2, x3 .  . . xn respectively.  The matching algorithm implemented by the analysis engine 111 enables the tracking of the identified object of
interest in the upcoming frames f2, f3 .  . . fn.
<br/><br/> The identified object of interest tracked is then tagged and augmented using crowd sourcing technique.  Crowd sourcing is used for augmenting knowledge among different communities across the globe by using the tags generated.
<br/><br/> Thus, the identified object of interest is further furnished with additional information to generate immediate metadata related to the tracked object.  For example, as indicated in the movie example, the actor will be tracked by multiple users
connected through social community network by means of inputting tags related to the identified actor in the upcoming video frames.
<br/><br/> One user connected in the network may input then name of the actor, the other may include the actor's upcoming movies, and the third may input actor's spouse information.  Similarly, other users may input additional information about the
identified actor in the upcoming video frames.  This information is a metadata that is used to track the actor in the future.  The generated metadata is uploaded to the metadata database 110 using REST.
<br/><br/> The paired devices 107 connected to the internet have the ability to offload some of their computation on other servers in case they do not possess sufficient computational power themselves via cloud services 109.
<br/><br/> These tags are shared across the users via an aggregation server where users are allowed to correct and augment such tags.  In an embodiment, websites like IMDB and Wikipedia are used for suggestions by the users to extract more information for
such tags.  The tags may not be mere descriptions but might be student notes, Q &amp; A, and general comments for an educational content.  Similarly, other genre of broadcast can be similarly augmented.
<br/><br/> A shown in FIG. 1, social networking sites such as Twitter 112-2 and <b><i>Facebook</i></b> 112-3 are used for social sharing of tags.  Also, the information database Wikipedia 112-1 is used to get more content related to the tags and for automated tagging
efforts.  Analysis engine 111 further helps in image analysis of scenes or for any context analysis of generated metadata as well.
<br/><br/> Thus, the invention presents uses crowd sourcing to help generate tagged metadata for user identified objects of interest on TV.  Users indicate bounded boxes around their objects, thereby simplifying the problem of object detection.  Social
networks come into play to make the tagging a fun social activity while solving the problem of recognition.  The method proposed will enable people connected in a social-community network to use the bounding box just for fun, but in the end it will help
in creating metadata in video frames and proper tagging of objects in video frame.
<br/><br/> FIG. 2 shows an example of tagging a particular object in a video broadcasted on a TV in accordance to an embodiment of the present invention.  As shown in FIG. 2, a connected TV 201 is paired with a handheld Tablet 202 via Bluetooth or WI-FI. 
The video broadcasted on the TV 201 may be divided into plurality of objects.  As shown in FIG. 2, 201-1, 201-2, 201-3 may be indicated as multiple objects of the video broadcasted on TV 201.
<br/><br/> With the increasing trend of TV viewing with a second screen, in an embodiment of the present invention the broadcasted video can be pushed to the user's connected Tablet 201 paired with the TV 201.  In other embodiments, the broadcasted video
can be pushed if connected with the TV 201 to user's laptop or smart phone acting as a paired device 202.
<br/><br/> As shown in the FIG. 2, the screen area of the Tablet 202 is adjusted corresponding to viewable area of the TV 201.  This is done to make the picture displayed on TV screen compatible to the display area of the Tablet 202 so that the objects of
interest can be tracked efficiently.
<br/><br/> As indicted in FIG. 2, the user "ME" is currently logged in to the Tablet 202.  As discussed, the user enables selection of object of interest in the video by indicating the boundary-box.  In an embodiment, the user "ME" selects using her
fingers the face of an actor 201-1 as an object of interest indicated in FIG. 2 in the scene on displayed on the Tablet 202.  In another embodiment, the camera in the TV 201 can be used to take input as users' gesture to capture the objects of interest
indicated by a boundary-box.  The boundary-box 203 is shown that indicates the boundaries of the object to be tracked in the upcoming video frames.  As shown, the face of an actor 201-1 is bounded by the boundary-box 203 referred as 204 in the FIG. 2.
<br/><br/> Now, the selection of the face of the actor 204 in the scene by the user `"ME" displays the existing tags related to the face of the actor from the other users connected in the social-community network.  For example, as shown in FIG. 2, the
existing tags from the user "ME""s friend "GB" is displayed as bubbles near the selection of the object.  The tags indicated by GB are shown as bubbles 205-1 and 205-3 referred as tags "Actor" and "Lead" respectively.
<br/><br/> Further, the user "ME" is able to add her own tags to the same object and is allowed to share it.  As shown in the FIG. 2, the tag "Brown" added by the user "ME" is displayed as bubble 205-2.  Further, these tags can be augmented by the users
using additional information from the online service providers such as IMDB and Wikipedia.  Also, the tags can be augmented by providing share and ask buttons to have interaction among different users using social networking sites such as <b><i>Facebook</i></b> and
Twitter.  This adds additional information to the generated tags from the multiple users connected in a social community network give rise to immediate metadata.  This metadata helps in tracking the object 204 in the upcoming video frames.  This
technique of utilizing the different users from the social community network for augmenting knowledge is called as "Crowd sourcing".
<br/><br/> Further, the generated metadata is uploaded on a metadata database for future tracking of the object in the upcoming video frames using REST.
<br/><br/> The paired device 202 connected to the internet have the ability to offload some of their computation on other servers in case they do not possess sufficient computational power themselves.  For example, cloud services as shown in FIG. 1 are
being implemented to possess the sufficient computational power.
<br/><br/> FIG. 3 shows a flow chart 300 depicting steps for tagging objects in a video on a TV using crowd sourcing technique in accordance to an embodiment of the invention.
<br/><br/> At step 301, Tablet PC/Smartphone camera is paired with the TV broadcasting the video.
<br/><br/> At step 302, the video broadcasted on TV is pushed to the paired Tablet PC/Smartphone camera.
<br/><br/> At step 303, a boundary-box bounding the object of interest in the video is selected by a user of the tablet/Smartphone.
<br/><br/> At step 304, the object of interest in the bounding-box is tagged by the users connected in a social community network in the upcoming video frames.
<br/><br/> At step 305, the tagged object is augmented by adding additional information from online web services.
<br/><br/> At step 306, a metadata related to augmented object is generated based on tagged information related to the object.
<br/><br/> At step 307, the metadata generated is uploaded on a server for future tracking of the object.
<br/><br/> The methodology and techniques described with respect to the embodiments can be performed using a machine or other computing device within which a set of instructions, when executed, may cause the machine to perform any one or more of the
methodologies discussed above.  In some embodiments, the machine operates as a standalone device.  In some embodiments, the machine may be connected (e.g., using a network) to other machines.  In a networked deployment, the machine may operate in the
capacity of a server or a client user machine in a server-client user network environment, or as a peer machine in a peer-to-peer (or distributed) network environment.  The machine may comprise a server computer, a client user computer, a personal
computer (PC), a tablet PC, a laptop computer, a desktop computer, a control system, a network router, switch or bridge, or any machine capable of executing a set of instructions (sequential or otherwise) that specify actions to be taken by that machine. Further, while a single machine is illustrated, the term "machine" shall also be taken to include any collection of machines that individually or jointly execute a set (or multiple sets) of instructions to perform any one or more of the methodologies
discussed herein.
<br/><br/> The machine may include a processor (e.g., a central processing unit (CPU), a graphics processing unit (GPU, or both), a main memory and a static memory, which communicate with each other via a bus.  The machine may further include a video
display unit (e.g., a liquid crystal display (LCD), a flat panel, a solid state display, or a cathode ray tube (CRT)).  The machine may include an input device (e.g., a keyboard) or touch-sensitive screen, a cursor control device (e.g., a mouse), a disk
drive unit, a signal generation device (e.g., a speaker or remote control) and a network interface device.
<br/><br/> Dedicated hardware implementations including, but not limited to, application specific integrated circuits, programmable logic arrays and other hardware devices can likewise be constructed to implement the methods described herein.  Applications
that may include the apparatus and systems of various embodiments broadly include a variety of electronic and computer systems.  Some embodiments implement functions in two or more specific interconnected hardware modules or devices with related control
and data signals communicated between and through the modules, or as portions of an application-specific integrated circuit.  Thus, the example system is applicable to software, firmware, and hardware implementations.
<br/><br/> The present disclosure contemplates a machine readable medium containing instructions, or that which receives and executes instructions from a propagated signal so that a device connected to a network environment can send or receive voice, video
or data, and to communicate over the network using the instructions.  The instructions may further be transmitted or received over a network via the network interface device.
<br/><br/> While the machine-readable medium can be a single medium, the term "machine readable medium" should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that
store the one or more sets of instructions.  The term "machine-readable medium" shall also be taken to include any medium that is capable of storing, encoding or carrying a set of instructions for execution by the machine and that cause the machine to
perform any one or more of the methodologies of the present disclosure.
<br/><br/> The term "machine-readable medium" shall accordingly be taken to include, but not be limited to: tangible media; solid-state memories such as a memory card or other package that houses one or more read-only (non-volatile) memories, random access
memories, or other rewritable (volatile) memories; magneto-optical or optical medium such as a disk or tape; no transitory mediums or other self-contained information archive or set of archives is considered a distribution medium equivalent to a tangible
storage medium.  Accordingly, the disclosure is considered to include any one or more of a machine-readable medium or a distribution medium, as listed herein and including art-recognized equivalents and successor media, in which the software
implementations herein are stored.
<br/><br/> The illustrations of arrangements described herein are intended to provide a general understanding of the structure of various embodiments, and they are not intended to serve as a complete description of all the elements and features of
apparatus and systems that might make use of the structures described herein.  Many other arrangements will be apparent to those of skill in the art upon reviewing the above description.  Other arrangements may be utilized and derived there from, such
that structural and logical substitutions and changes may be made without departing from the scope of this disclosure.  Figures are also merely representational and may not be drawn to scale.  Certain proportions thereof may be exaggerated, while others
may be minimized.  Accordingly, the specification and drawings are to be regarded in an illustrative rather than a restrictive sense.
<br/><br/> The preceding description has been presented with reference to various embodiments of the invention.  Persons skilled in the art and technology to which this
<br/><br/> Invention pertains will appreciate that alterations and changes in the described structures and methods of operation can be practiced without meaningfully departing from the principle, spirit and scope of this invention.
<br/><br/>Advantages of the Invention
<br/><br/> The present invention has following advantages: The present invention simplifies the problem of object identification and recognition on TV, which are long standing computationally hard problems of image processing.  The metadata generated for
each video via crowd sourcing can be uploaded to online services and stored for both future uses as well as re-used as part of rich interactive multimedia applications.  The present invention enables tagging of objects in a social TV using crowd sourcing
where people tag for fun and generate metadata as a result of tagging.  The present invention enables by way of tagged objects other users connected in a social community network to know the interesting objects in the current videos.
<br/><br/><center><b>* * * * *</b></center>
<hr/>
   <center>
   <a href="http://pdfpiw.uspto.gov/.piw?Docid=09357242&amp;homeurl=http%3A%2F%2Fpatft.uspto.gov%2Fnetacgi%2Fnph-Parser%3FSect1%3DPTO2%2526Sect2%3DHITOFF%2526u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%2526r%3D387%2526f%3DG%2526l%3D50%2526d%3DPTXT%2526s1%3Dfacebook%2526p%3D8%2526OS%3Dfacebook%2526RS%3Dfacebook&amp;PageNum=&amp;Rtype=&amp;SectionNum=&amp;idkey=NONE&amp;Input=View+first+page"><img alt="[Image]" border="0" src="/netaicon/PTO/image.gif" valign="middle"/></a>
   <table>
   <tbody><tr><td align="center"><a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/ShowShoppingCart?backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D387%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D8%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209357242"><img alt="[View Shopping Cart]" border="0" src="/netaicon/PTO/cart.gif" valign="middle"/></a>
   <a href="http://ebiz1.uspto.gov/vision-service/ShoppingCart_P/AddToShoppingCart?docNumber=9357242&amp;backUrl1=http%3A//patft.uspto.gov/netacgi/nph-Parser?Sect1%3DPTO2%26Sect2%3DHITOFF%26u%3D%25252Fnetahtml%25252FPTO%25252Fsearch-adv.htm%26r%3D387%26f%3DG%26l%3D50%26d%3DPTXT%26s1%3Dfacebook%26p%3D8%26OS%3Dfacebook&amp;backLabel1=Back%20to%20Document%3A%209357242">
   <img alt="[Add to Shopping Cart]" border="0" src="/netaicon/PTO/order.gif" valign="middle"/></a>
   </td></tr>
   <tr><td align="center">
     <a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=387&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=7&amp;Query=facebook"><img alt="[PREV_LIST]" border="0" src="/netaicon/PTO/prevlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=387&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;Query=facebook"><img alt="[HIT_LIST]" border="0" src="/netaicon/PTO/hitlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=387&amp;f=S&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=9&amp;Query=facebook"><img alt="[NEXT_LIST]" border="0" src="/netaicon/PTO/nextlist.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=386&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;OS=facebook"><img alt="[PREV_DOC]" border="0" src="/netaicon/PTO/prevdoc.gif" valign="MIDDLE"/></a>
<a href="/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=388&amp;f=G&amp;l=50&amp;d=PTXT&amp;s1=facebook&amp;p=8&amp;OS=facebook"><img alt="[NEXT_DOC]" border="0" src="/netaicon/PTO/nextdoc.gif" valign="MIDDLE"/></a>

   <a href="#top"><img alt="[Top]" border="0" src="/netaicon/PTO/top.gif" valign="middle"/></a>
   </td></tr>
   </tbody></table>
   <a name="bottom"></a>
   <a href="/netahtml/PTO/index.html"><img alt="[Home]" border="0" src="/netaicon/PTO/home.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-bool.html"><img alt="[Boolean Search]" border="0" src="/netaicon/PTO/boolean.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/search-adv.htm"><img alt="[Manual Search]" border="0" src="/netaicon/PTO/manual.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/srchnum.htm"><img alt="[Number Search]" border="0" src="/netaicon/PTO/number.gif" valign="middle"/></a>
   <a href="/netahtml/PTO/help/help.htm"><img alt="[Help]" border="0" src="/netaicon/PTO/help.gif" valign="middle"/></a>
   </center>

</coma></body></html>